\section{Related Work}
\label{sec:related}
The majority of recent studies on LLMs are concentrated on the evaluation of their capability and security.
Chang et al.~\cite{chang2023survey} conducted a comprehensive survey of evaluations of LLMs. Effective evaluations play a crucial role in facilitating substantial improvements in LLMs.
Yu et al.~\cite{yu2023gptfuzzer} proposed GPTFuzzer, a black-box fuzzing framework to evaluate the robustness of LLMs.
In the code generation task, Pearce et al.~\cite{pearce2022asleep} evaluated the security of code generation LLM, i.e., Copilot.
Liu et al.~\cite{liu2023your} proposed EvalPlus, a benchmark framework to evaluate the correctness of the code generated by LLMs.
\emph{Prior studies primarily focus on testing the robustness and security of LLMs. However, our work aims to investigate the vulnerabilities, especially remote code execution, in apps, which are caused by LLM involvement. This paves a new attack surface for penetrating into the victim system, so any app with LLM capabilities is susceptible of this threat.}

On the other hand, several studies have been conducted on adversarial prompting against LLMs and LLM-integrated apps. 
Greshake et al.~\cite{greshake2023not} proposed a new attack vector, indirect prompt injection, which can remotely manipulate the content of LLM's output to the user.
Li et al.~\cite{li2023multi} proposed a multi-step jailbreaking prompt to extract users' private information in ChatGPT and New Bing.
Liu et al.~\cite{liu2023prompt} proposed a black-box prompt injection attack to access unrestricted functionality and system prompts of ten commercial LLM-integrated apps.
Shen et al.~\cite{shen2023anything} performed a measurement on jailbreak prompts, which is aiming to circumvent the security restrictions of LLMs.
Pedro et al.~\cite{pedro2023prompt} proposed a security analysis on the known SQL injection vulnerability in LangChain caused by prompt injection.
Zou et al.~\cite{zou2023universal} performed a transferable adversarial attack against multiple LLMs using prompts trained from white-box LLMs. 
\emph{Different from these studies, \tool{} performs adversarial prompt attacks, e.g., prompt injection and escape techniques, on LLM-integrated apps especially in the real-world scenario, and discovers severe RCE vulnerabilities. 
To the best of our knowledge, \tool{} makes the first attempt to systematically detect, exploit and measure RCE vulnerabilities in varies LLM-integrated frameworks and apps in the real-world scenario.
}
\vspace{-5pt}