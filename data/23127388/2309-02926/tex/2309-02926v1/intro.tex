\section{Introduction}
\label{sec:intro}

Recently, Large Language Models (LLMs) have demonstrated remarkable potential in various downstream tasks. Evidence highlights how LLM's involvement has revitalized numerous tasks, such as code generation~\cite{wang2023codet5+}, data analysis~\cite{cheng2023gpt}, and program repair~\cite{xia2023automated}, achieving outstanding improvements in effectiveness. 
This explosion of technological innovation has drawn the attention of many app developers. To enhance the competitiveness of their products, they have enthusiastically embraced the integration of LLMs into their apps, resulting in a proliferation of LLM-integrated apps. 

To facilitate the ease of constructing LLM-integrated apps for the general public, some developers created a multitude of LLM-integrated frameworks, also called LLM-integration middleware, for example,  LangChain~\cite{langchain} and LlamaIndex~\cite{llamaindex}. These frameworks have garnered substantial attention, evidenced by numerous projects on platforms like GitHub amassing thousands of stars. They aim to complement and extend LLM's capabilities, maximizing their potential to address a wide range of practical challenges. By enabling users to interact with LLMs through natural language, these frameworks empower individuals to tackle more complex problems that would otherwise be beyond the scope of LLM alone. 
Hence, app developers can now build apps by simply invoking framework APIs as their backend rather than interacting with LLMs directly. 
However, these frameworks may also have potential vulnerabilities, influencing the security of apps built on these frameworks. 

Previous research has indicated the potential risks of SQL injection in certain LLM-integrated apps~\cite{pedro2023prompt}. Attackers can remotely exploit SQL injection in these apps through prompt injection. In reaction to SQL injection vulnerabilities, researchers proposed several mitigation measures, such as SQL query rewriting and database permission hardening. 
But our research demonstrates that, in addition to SQL injection, LLM-integrated apps are facing even more serious threats in the form of Remote Code Execution (RCE), which allows attackers to execute arbitrary code remotely and even obtain the entire control of the app via prompt injection. 
Apart from WannaCry ransomware~\cite{chen2017automated} and Log4J~\cite{log4j}, it is a new type of RCE achieved by leveraging the defects of both LLMs and apps. More severely, attacker can achieve RCE by just one line natural language without having solid background of computer security.  
Even surprisingly, it is LLMs that provide a covert ``channel'' for attackers to remotely access and endanger the victim, \ie, apps. 
This type of attack comes to fruition if the following requirements are satisfied.

(1) \emph{Uncontrollable responses of LLMs.} Due to the inherent unpredictability and randomness of LLMs' behaviors, developers cannot accurately predict how an LLM will respond to a wide range of diverse prompts. Thus, effectively constraining LLMs' behavior becomes challenging. Based on this, attackers may manipulate LLM's outputs by strategically crafted prompts, bypassing the restrictions, and enabling subsequent malicious actions (\eg jailbreaking~\cite{deng2023jailbreaker}). 

(2) \emph{Execution of untrusted code.} Most LLM-integrated frameworks with code execution capabilities receive the code generated by LLMs which is untrusted. 
However, developers often do not provide appropriate checks or filters for such code, allowing it to be executed in an unprotected environment. Thus, attackers may achieve RCE by manipulating the code generated by LLMs via a prompt. 

To date, there has been a dearth of comprehensive research, systematically analyzing the security, especially RCE vulnerabilities of LLM-integrated frameworks and apps available in markets. 
It is hence desired to explore how to detect and validate RCE vulnerabilities, and unveil the consequences caused for different stakeholders.

\vspace {3pt}\noindent\textbf{Challenges.} To this end, we have to solve the following challenges.

(1) It is non-trivial to detect vulnerabilities in a large codebase from the outset effectively, considering the extensive codebase of LLM-integrated frameworks and apps. Moreover, the involvement of LLMs makes the logic more intractable, where the detection has to chain up apps, frameworks and LLMs for a precise analysis.

(2) There are many unexpected obstacles during testing real-world LLM-integrated apps to validate and exploit the RCE vulnerabilities. More specifically, the randomness of LLM responses, security mechanisms against malicious prompts, process isolation and even network accessibility can all affect the exploitability.

\vspace {3pt}\noindent\textbf{Our Approach.}
To detect RCE vulnerabilities in LLM-integrated frameworks and evaluate their exploitability in real-world apps, we propose a multi-step approach named \tool{}. 
First, we enhance static analysis techniques to scan framework source code, extracting call chains from user-level API to hazardous functions, and subsequently validating their exploitability locally (Section~\ref{sec:approach:1}). 
To directly explore the hazards in real-world scenario, we develop heuristic methods to collect potentially affected LLM-integrated apps from both code hosting platforms and app markets, respectively (Section~\ref{sec:approach:2}). 
Last, we present a systematical prompt-based exploitation method for these RCE vulnerabilities.
By combining multiple strategies like hallucination tests and escaping techniques, we are able to systematically validate and exploit the vulnerabilities, thus streamlining the testing process for apps (Section \ref{sec:approach:4}). 

We evaluate \tool{} on 11 LLM-integrated frameworks, and \tool{} identifies 20 vulnerabilities, with 13 vulnerabilities being assigned CVE IDs, 6 of which have a 9.8 CVSS score.
Notably, \tool{}'s performance and accuracy on the call chain extraction task improved significantly compared to the Python static analysis framework, PyCG. 
As a result, \tool{} successfully extracts 51 call chains that potentially lead to RCE among 11 frameworks during 20.332s with a 13.7\% false positive rate.
Moreover, \tool{} tests 51 potentially vulnerable apps in real-world scenarios and successfully exploits 17 apps, revealing 16 RCE vulnerabilities and 1 SQL injection vulnerability. 

\vspace {3pt}\noindent\textbf{Contributions.} We make the following contributions.
\begin{itemize} [leftmargin=*]
    \item \textbf{An efficient and lightweight method for detecting RCE vulnerabilities in LLM-integrated frameworks.} 
    To efficiently detect RCE vulnerabilities within LLM-integrated frameworks, we propose a lightweight and efficient source code analysis approach. This enables the fast extraction of call chains from user-level APIs to hazardous functions within frameworks. We successfully find 20 vulnerabilities across 11 frameworks, 17 of which all are acknowledged by the framework developers and 13 unique CVEs are assigned. This approach enables us to find the most RCE vulnerabilities in LLM frameworks as of paper submission.
    \item \textbf{An prompt-based exploitation method for LLM-integrated apps.}
    We propose a novel combination of attacking strategies including hallucination test and LLM escaping that can circumvent both the difficulties and defenses from LLMs and apps. 
    It enables us to make a successful attack on 17 real-world apps (out of 51 collected ones), where 16 of them are susceptible of RCE and the left one is vulnerable to SQL injection. 
    
    \item \textbf{The first systematic analysis of these new RCE exploitation vectors, vulnerabilities and practical attacks.}  
    Based on \tool and results, we have the unique opportunity to characterize these vulnerabilities from the aspects of vulnerability types, triggering mechanisms, exploitation targets, and defensive methods. We further explore some post-exploitation scenarios after being subjected to RCE attacks from the perspectives of app hosts and users, raising practical real-world attacks. Notably, these practical real-world attacks are verified in real-world scenarios by deploying the white box victim apps in dataset locally.

\end{itemize}

\vspace {3pt}\noindent\textbf{Ethical Considerations.} We responsibly reported all the issues mentioned above to the corresponding developers in a timely manner, without disclosing any attack methods or results to the public. 
Observed that some vulnerable apps are popular (with 700+ stars on GitHub) and some are commercial applications, we use \texttt{[Anonymous App]} to represent a real-world app in some examples to protect sensitive information of these apps. In addition, to avoid disturbing the functionality of the public app, we deploy the victim app locally to complete the experiments in Section \ref{sec:Measure:attack}. 

\vspace {3pt}\noindent\textbf{LLMSmith Website.} More detailed information and attack demo videos (locally) are available in our website \url{https://sites.google.com/view/llmsmith}~\cite{llmsmith}. Note that we did not turn on the functionality of Google Analytics, so feel free to visit our website.