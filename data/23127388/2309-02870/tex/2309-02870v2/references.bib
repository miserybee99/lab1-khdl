@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}
@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={779--788},
  year={2016}
}
@inproceedings{paszke2019pytorch,
  author       = {Adam Paszke and
                  Sam Gross and
                  Francisco Massa and
                  Adam Lerer and
                  James Bradbury and
                  Gregory Chanan and
                  Trevor Killeen and
                  Zeming Lin and
                  Natalia Gimelshein and
                  Luca Antiga and
                  Alban Desmaison and
                  Andreas K{\"{o}}pf and
                  Edward Z. Yang and
                  Zachary DeVito and
                  Martin Raison and
                  Alykhan Tejani and
                  Sasank Chilamkurthy and
                  Benoit Steiner and
                  Lu Fang and
                  Junjie Bai and
                  Soumith Chintala},
  title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems},
  pages        = {8024--8035},
  year         = {2019},
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{wang2022foster,
  title={Foster: Feature boosting and compression for class-incremental learning},
  author={Wang, Fu-Yun and Zhou, Da-Wei and Ye, Han-Jia and Zhan, De-Chuan},
  booktitle={European Conference on Computer Vision},
  pages={398--414},
  year={2022},
}
@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9650--9660},
  year={2021}
}
@inproceedings{simon2021learning,
  title={On learning the geodesic path for incremental learning},
  author={Simon, Christian and Koniusz, Piotr and Harandi, Mehrtash},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1591--1600},
  year={2021}
}
@inproceedings{tian_contrastive_2019,
  author       = {Yonglong Tian and
                  Dilip Krishnan and
                  Phillip Isola},
  title        = {Contrastive Representation Distillation},
  booktitle    = {8th International Conference on Learning Representations},
  year         = {2020},
}
@inproceedings{douillard2020podnet,
  title={Podnet: Pooled outputs distillation for small-tasks incremental learning},
  author={Douillard, Arthur and Cord, Matthieu and Ollion, Charles and Robert, Thomas and Valle, Eduardo},
  booktitle={16th European Conference on Conputer Vision},
  pages={86--102},
  year={2020},
}
@inproceedings{aguilar2020knowledge,
  title={Knowledge distillation from internal representations},
  author={Aguilar, Gustavo and Ling, Yuan and Zhang, Yu and Yao, Benjamin and Fan, Xing and Guo, Chenlei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={7350--7357},
  year={2020}
}

@inproceedings{hou2018lifelong,
  title={Lifelong learning via progressive distillation and retrospection},
  author={Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  booktitle={Proceedings of the European Conference on Computer Vision},
  pages={437--452},
  year={2018}
}

@inproceedings{hou2019learning,
  title={Learning a unified classifier incrementally via rebalancing},
  author={Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={831--839},
  year={2019}
}

@article{chaudhry_efficient_2018,
	title = {Efficient {Lifelong} {Learning} with {A}-{GEM}},
	url = {http://arxiv.org/abs/1812.00420},
	abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz \& Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
	urldate = {2021-06-07},
	journal = {arXiv:1812.00420 [cs, stat]},
	author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.00420
version: 1},
	keywords = {a-GEM, AGEM, Computer Science - Machine Learning, GEM, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Z8EQZHKX/1812.html:text/html;Chaudhry et al. - 2018 - Efficient Lifelong Learning with A-GEM.pdf:/Users/nicolas/Documents/Zotero/arXiv1812.00420 [cs, stat]2018/Chaudhry et al. - 2018 - Efficient Lifelong Learning with A-GEM.pdf:application/pdf},
}

@article{yu_semantic_2020,
	title = {Semantic {Drift} {Compensation} for {Class}-{Incremental} {Learning}},
	url = {http://arxiv.org/abs/2004.00440},
	abstract = {Class-incremental learning of deep networks sequentially increases the number of classes to be classified. During training, the network has only access to data of one task at a time, where each task contains several classes. In this setting, networks suffer from catastrophic forgetting which refers to the drastic drop in performance on previous tasks. The vast majority of methods have studied this scenario for classification networks, where for each new task the classification layer of the network must be augmented with additional weights to make room for the newly added classes. Embedding networks have the advantage that new classes can be naturally included into the network without adding new weights. Therefore, we study incremental learning for embedding networks. In addition, we propose a new method to estimate the drift, called semantic drift, of features and compensate for it without the need of any exemplars. We approximate the drift of previous tasks based on the drift that is experienced by current task data. We perform experiments on fine-grained datasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks suffer significantly less from catastrophic forgetting. We outperform existing methods which do not require exemplars and obtain competitive results compared to methods which store exemplars. Furthermore, we show that our proposed SDC when combined with existing methods to prevent forgetting consistently improves results.},
	urldate = {2021-06-06},
	journal = {arXiv:2004.00440 [cs]},
	author = {Yu, Lu and Twardowski, Bartłomiej and Liu, Xialei and Herranz, Luis and Wang, Kai and Cheng, Yongmei and Jui, Shangling and van de Weijer, Joost},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.00440},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, drift compensation},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/GJBJJTF2/2004.html:text/html;Yu et al. - 2020 - Semantic Drift Compensation for Class-Incremental .pdf:/Users/nicolas/Documents/Zotero/arXiv2004.00440 [cs]2020/Yu et al. - 2020 - Semantic Drift Compensation for Class-Incremental .pdf:application/pdf},
}

@article{lee_overcoming_2019,
	title = {Overcoming {Catastrophic} {Forgetting} with {Unlabeled} {Data} in the {Wild}},
	url = {http://arxiv.org/abs/1903.12648},
	abstract = {Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8\% higher accuracy and 46.5\% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.},
	urldate = {2021-06-06},
	journal = {arXiv:1903.12648 [cs, stat]},
	author = {Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
	month = oct,
	year = {2019},
	note = {arXiv: 1903.12648},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, unlabeled},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/ETHF9TGH/1903.html:text/html;Lee et al. - 2019 - Overcoming Catastrophic Forgetting with Unlabeled .pdf:/Users/nicolas/Documents/Zotero/arXiv1903.12648 [cs, stat]2019/Lee et al. - 2019 - Overcoming Catastrophic Forgetting with Unlabeled .pdf:application/pdf},
}

@article{mai_online_2021,
  title={Online continual learning in image classification: An empirical survey},
  author={Mai, Zheda and Li, Ruiwen and Jeong, Jihwan and Quispe, David and Kim, Hyunwoo and Sanner, Scott},
  journal={Neurocomputing},
  volume={469},
  pages={28--51},
  year={2022},
  publisher={Elsevier}
}

@article{masana_class-incremental_2021,
	title = {Class-incremental learning: survey and performance evaluation on image classification},
	shorttitle = {Class-incremental learning},
	url = {http://arxiv.org/abs/2010.15277},
	abstract = {For future learning systems incremental learning is desirable, because it allows for: efficient resource usage by eliminating the need to retrain from scratch at the arrival of new data; reduced memory usage by preventing or limiting the amount of data required to be stored -- also important when privacy limitations are imposed; and learning that more closely resembles human learning. The main challenge for incremental learning is catastrophic forgetting, which refers to the precipitous drop in performance on previously learned tasks after learning a new one. Incremental learning of deep neural networks has seen explosive growth in recent years. Initial work focused on task-incremental learning, where a task-ID is provided at inference time. Recently, we have seen a shift towards class-incremental learning where the learner must discriminate at inference time between all classes seen in previous tasks without recourse to a task-ID. In this paper, we provide a complete survey of existing class-incremental learning methods for image classification, and in particular we perform an extensive experimental evaluation on thirteen class-incremental methods. We consider several new experimental scenarios, including a comparison of class-incremental methods on multiple large-scale image classification datasets, investigation into small and large domain shifts, and comparison of various network architectures.},
	urldate = {2021-06-04},
	journal = {arXiv:2010.15277 [cs]},
	author = {Masana, Marc and Liu, Xialei and Twardowski, Bartlomiej and Menta, Mikel and Bagdanov, Andrew D. and van de Weijer, Joost},
	month = may,
	year = {2021},
	note = {arXiv: 2010.15277},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, survey},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/JZT3YMJN/2010.html:text/html;Masana et al. - 2021 - Class-incremental learning survey and performance.pdf:/Users/nicolas/Documents/Zotero/arXiv2010.15277 [cs]2021/Masana et al. - 2021 - Class-incremental learning survey and performance.pdf:application/pdf},
}

@inproceedings{mai_supervised_2021,
  title={Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning},
  author={Mai, Zheda and Li, Ruiwen and Kim, Hyunwoo and Sanner, Scott},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3589--3599},
  year={2021}
}

@article{serra_overcoming_2018,
	title = {Overcoming catastrophic forgetting with hard attention to the task},
	url = {http://arxiv.org/abs/1801.01423},
	abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
	urldate = {2021-06-03},
	journal = {arXiv:1801.01423 [cs, stat]},
	author = {Serrà, Joan and Surís, Dídac and Miron, Marius and Karatzoglou, Alexandros},
	month = may,
	year = {2018},
	note = {arXiv: 1801.01423},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, HAT},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/QG8778JN/1801.html:text/html;Serrà et al. - 2018 - Overcoming catastrophic forgetting with hard atten.pdf:/Users/nicolas/Documents/Zotero/arXiv1801.01423 [cs, stat]2018/Serrà et al. - 2018 - Overcoming catastrophic forgetting with hard atten.pdf:application/pdf},
}

@article{aljundi_expert_2017,
	title = {Expert {Gate}: {Lifelong} {Learning} with a {Network} of {Experts}},
	shorttitle = {Expert {Gate}},
	url = {http://arxiv.org/abs/1611.06194},
	abstract = {In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process,data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with finetuning or learning without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.},
	urldate = {2021-06-02},
	journal = {arXiv:1611.06194 [cs, stat]},
	author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.06194},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Aljundi et al. - 2017 - Expert Gate Lifelong Learning with a Network of E.pdf:/Users/nicolas/Documents/Zotero/arXiv1611.06194 [cs, stat]2017/Aljundi et al. - 2017 - Expert Gate Lifelong Learning with a Network of E.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/E7PQAHKJ/1611.html:text/html},
}

@article{zenke_continual_2017,
	title = {Continual {Learning} {Through} {Synaptic} {Intelligence}},
	url = {http://arxiv.org/abs/1703.04200},
	abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.},
	urldate = {2021-06-02},
	journal = {arXiv:1703.04200 [cs, q-bio, stat]},
	author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.04200},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition, SI},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/S7PUAZ4S/1703.html:text/html;Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf:/Users/nicolas/Documents/Zotero/arXiv1703.04200 [cs, q-bio, stat]2017/Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf:application/pdf},
}

@article{li_learning_2017,
	title = {Learning without {Forgetting}},
	url = {http://arxiv.org/abs/1606.09282},
	abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
	urldate = {2021-05-31},
	journal = {arXiv:1606.09282 [cs, stat]},
	author = {Li, Zhizhong and Hoiem, Derek},
	month = feb,
	year = {2017},
	note = {arXiv: 1606.09282},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, CL data-focused},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/BUBNICC8/Li and Hoiem - 2017 - Learning without Forgetting.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/5HBFLTKJ/1606.html:text/html},
}

@article{van_de_ven_generative_2019,
	title = {Generative replay with feedback connections as a general strategy for continual learning},
	url = {http://arxiv.org/abs/1809.10635},
	abstract = {A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as "soft targets") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.},
	urldate = {2021-05-20},
	journal = {arXiv:1809.10635 [cs, stat]},
	author = {van de Ven, Gido M. and Tolias, Andreas S.},
	month = apr,
	year = {2019},
	note = {arXiv: 1809.10635},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, RtF},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/6HJPCRYF/van de Ven and Tolias - 2019 - Generative replay with feedback connections as a g.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/HJPKQXPN/1809.html:text/html},
}

@article{shin_continual_2017,
	title = {Continual {Learning} with {Deep} {Generative} {Replay}},
	url = {http://arxiv.org/abs/1705.08690},
	urldate = {2021-05-19},
	journal = {arXiv preprint arXiv:1705.08690},
	author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
	month = dec,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/I9J76NS4/1705.html:text/html;Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:/Users/nicolas/Documents/Zotero/arXiv1705.08690 [cs]2017/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:application/pdf},
}

@article{farquhar_unifying_2019,
	title = {A {Unifying} {Bayesian} {View} of {Continual} {Learning}},
	url = {http://arxiv.org/abs/1902.06494},
	abstract = {Some machine learning applications require continual learning - where data comes in a sequence of datasets, each is used for training and then permanently discarded. From a Bayesian perspective, continual learning seems straightforward: Given the model posterior one would simply use this as the prior for the next task. However, exact posterior evaluation is intractable with many models, especially with Bayesian neural networks (BNNs). Instead, posterior approximations are often sought. Unfortunately, when posterior approximations are used, prior-focused approaches do not succeed in evaluations designed to capture properties of realistic continual learning use cases. As an alternative to prior-focused methods, we introduce a new approximate Bayesian derivation of the continual learning loss. Our loss does not rely on the posterior from earlier tasks, and instead adapts the model itself by changing the likelihood term. We call these approaches likelihood-focused. We then combine prior- and likelihood-focused methods into one objective, tying the two views together under a single unifying framework of approximate Bayesian continual learning.},
	urldate = {2021-05-19},
	journal = {arXiv:1902.06494 [cs, stat]},
	author = {Farquhar, Sebastian and Gal, Yarin},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06494
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/YSSG36Y5/Farquhar and Gal - 2019 - A Unifying Bayesian View of Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SLJGK4V6/1902.html:text/html},
}

@article{farquhar_towards_2019,
	title = {Towards {Robust} {Evaluations} of {Continual} {Learning}},
	url = {http://arxiv.org/abs/1805.09733},
	abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
	urldate = {2021-05-19},
	journal = {arXiv:1805.09733 [cs, stat]},
	author = {Farquhar, Sebastian and Gal, Yarin},
	month = jun,
	year = {2019},
	note = {arXiv: 1805.09733},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/NI84R2GY/Farquhar and Gal - 2019 - Towards Robust Evaluations of Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/HXZPQYLT/1805.html:text/html},
}

@inproceedings{rebuffi2017icarl,
  title={icarl: Incremental classifier and representation learning},
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2001--2010},
  year={2017}
}

@misc{saboo_shivamsaboo17overcoming-catastrophic-forgetting--neural-networks_2021,
	title = {shivamsaboo17/{Overcoming}-{Catastrophic}-forgetting-in-{Neural}-{Networks}},
	url = {https://github.com/shivamsaboo17/Overcoming-Catastrophic-forgetting-in-Neural-Networks},
	abstract = {Elastic weight consolidation technique for incremental learning.},
	urldate = {2021-04-27},
	author = {Saboo, Shivam},
	month = apr,
	year = {2021},
	note = {original-date: 2019-01-17T04:38:56Z},
	keywords = {deepmind, elastic-weight-consolidation, incremental-learning, neural-network},
}

@article{rao_continual_2019,
	title = {Continual {Unsupervised} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1910.14481},
	abstract = {Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.},
	urldate = {2021-04-22},
	journal = {arXiv:1910.14481 [cs, stat]},
	author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.14481},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/LGDR6JN5/1910.html:text/html;Rao et al. - 2019 - Continual Unsupervised Representation Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv1910.14481 [cs, stat]2019/Rao et al. - 2019 - Continual Unsupervised Representation Learning.pdf:application/pdf},
}

@article{de_lange_continual_2021,
	title = {A continual learning survey: {Defying} forgetting in classification tasks},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {A continual learning survey},
	url = {http://arxiv.org/abs/1909.08383},
	doi = {10.1109/TPAMI.2021.3057446},
	abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
	urldate = {2021-04-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne},
	year = {2021},
	note = {arXiv: 1909.08383
version: 3},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--1},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/U9TC9FW2/1909.html:text/html;De Lange et al. - 2021 - A continual learning survey Defying forgetting in.pdf:/Users/nicolas/Documents/Zotero/IEEE Transactions on Pattern Analysis and Machine Intelligence2021/De Lange et al. - 2021 - A continual learning survey Defying forgetting in.pdf:application/pdf},
}

@inproceedings{kirkpatrick_overcoming_2017,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
}


@article{lomonaco_cvpr_2020,
	title = {{CVPR} 2020 {Continual} {Learning} in {Computer} {Vision} {Competition}: {Approaches}, {Results}, {Current} {Challenges} and {Future} {Directions}},
	shorttitle = {{CVPR} 2020 {Continual} {Learning} in {Computer} {Vision} {Competition}},
	url = {http://arxiv.org/abs/2009.09929},
	abstract = {In the last few years, we have witnessed a renewed and fast-growing interest in continual learning with deep neural networks with the shared objective of making current AI systems more adaptive, efficient and autonomous. However, despite the significant and undoubted progress of the field in addressing the issue of catastrophic forgetting, benchmarking different continual learning approaches is a difficult task by itself. In fact, given the proliferation of different settings, training and evaluation protocols, metrics and nomenclature, it is often tricky to properly characterize a continual learning algorithm, relate it to other solutions and gauge its real-world applicability. The first Continual Learning in Computer Vision challenge held at CVPR in 2020 has been one of the first opportunities to evaluate different continual learning algorithms on a common hardware with a large set of shared evaluation metrics and 3 different settings based on the realistic CORe50 video benchmark. In this paper, we report the main results of the competition, which counted more than 79 teams registered, 11 finalists and 2300\$ in prizes. We also summarize the winning approaches, current challenges and future research directions.},
	urldate = {2021-04-15},
	journal = {arXiv:2009.09929 [cs, stat]},
	author = {Lomonaco, Vincenzo and Pellegrini, Lorenzo and Rodriguez, Pau and Caccia, Massimo and She, Qi and Chen, Yu and Jodelet, Quentin and Wang, Ruiping and Mai, Zheda and Vazquez, David and Parisi, German I. and Churamani, Nikhil and Pickett, Marc and Laradji, Issam and Maltoni, Davide},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.09929},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Y4U5I3CK/2009.html:text/html;Lomonaco et al. - 2020 - CVPR 2020 Continual Learning in Computer Vision Co.pdf:/Users/nicolas/Documents/Zotero/arXiv2009.09929 [cs, stat]2020/Lomonaco et al. - 2020 - CVPR 2020 Continual Learning in Computer Vision Co.pdf:application/pdf},
}

@misc{noauthor_facebookresearchswav_2021,
	title = {facebookresearch/swav},
	copyright = {View license         ,                 View license},
	url = {https://github.com/facebookresearch/swav},
	abstract = {PyTorch implementation of SwAV https//arxiv.org/abs/2006.09882},
	urldate = {2021-04-15},
	publisher = {Facebook Research},
	month = apr,
	year = {2021},
	note = {original-date: 2020-07-16T21:17:58Z},
}

@article{caron_unsupervised_2021,
	title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
	url = {http://arxiv.org/abs/2006.09882},
	abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
	urldate = {2021-04-15},
	journal = {arXiv:2006.09882 [cs]},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	month = jan,
	year = {2021},
	note = {arXiv: 2006.09882},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/9ZUI2QTA/2006.html:text/html;Caron et al. - 2021 - Unsupervised Learning of Visual Features by Contra.pdf:/Users/nicolas/Documents/Zotero/arXiv2006.09882 [cs]2021/Caron et al. - 2021 - Unsupervised Learning of Visual Features by Contra2.pdf:application/pdf},
}

@misc{noauthor_catastrophic_2021,
	title = {Catastrophic interference},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Catastrophic_interference&oldid=1017898147},
	abstract = {Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. With these networks, human capabilities such as memory and learning can be modeled using computer simulations. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ratcliff (1990). It is a radical manifestation of the 'sensitivity-stability' dilemma or the 'stability-plasticity' dilemma. Specifically, these problems refer to the challenge of making an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network can generalize to unseen inputs, but they are very sensitive to new information. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is an issue when modelling human memory, because unlike these networks, humans typically do not show catastrophic forgetting.},
	language = {en},
	urldate = {2021-04-15},
	journal = {Wikipedia},
	month = apr,
	year = {2021},
	note = {Page Version ID: 1017898147},
	file = {Snapshot:/Users/nicolas/Zotero/storage/7E34IVWL/index.html:text/html},
}

@article{kirkpatrick_overcoming_2017-1,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	copyright = {©  . Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/114/13/3521},
	doi = {10.1073/pnas.1611835114},
	abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
	language = {en},
	number = {13},
	urldate = {2021-04-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	month = mar,
	year = {2017},
	pmid = {28292907},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {artificial intelligence, continual learning, deep learning, stability plasticity, synaptic consolidation},
	pages = {3521--3526},
	file = {Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf:/Users/nicolas/Documents/Zotero/Proceedings of the National Academy of Sciences2017/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf:application/pdf;Snapshot:/Users/nicolas/Zotero/storage/9TIEC42K/3521.html:text/html},
}

@article{lopez-paz_gradient_2017,
	title = {Gradient {Episodic} {Memory} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/1706.08840},
	abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
	urldate = {2021-04-15},
	journal = {arXiv:1706.08840 [cs]},
	author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
	month = nov,
	year = {2017},
	note = {arXiv: 1706.08840},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/RPPHMHD4/Lopez-Paz and Ranzato - 2017 - Gradient Episodic Memory for Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/CS6KSE9M/1706.html:text/html},
}

@article{seff_continual_2017,
	title = {Continual {Learning} in {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1705.08395},
	abstract = {Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling.},
	urldate = {2021-04-15},
	journal = {arXiv:1705.08395 [cs, stat]},
	author = {Seff, Ari and Beatson, Alex and Suo, Daniel and Liu, Han},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08395},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/H59KVJYH/1705.html:text/html;Seff et al. - 2017 - Continual Learning in Generative Adversarial Nets.pdf:/Users/nicolas/Documents/Zotero/arXiv1705.08395 [cs, stat]2017/Seff et al. - 2017 - Continual Learning in Generative Adversarial Nets.pdf:application/pdf},
}

@inproceedings{zenke_continual_2017-1,
	title = {Continual {Learning} {Through} {Synaptic} {Intelligence}},
	url = {http://proceedings.mlr.press/v70/zenke17a.html},
	abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biologica...},
	language = {en},
	urldate = {2021-04-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {3987--3995},
	file = {Snapshot:/Users/nicolas/Zotero/storage/N2C6PYKM/zenke17a.html:text/html;Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf:/Users/nicolas/Documents/Zotero/PMLR2017/Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf:application/pdf},
}

@article{lomonaco_continual_nodate,
	title = {Continual {Learning} with {Deep} {Architectures}},
	language = {en},
	author = {Lomonaco, Vincenzo},
	pages = {150},
	file = {Lomonaco - Continual Learning with Deep Architectures.pdf:/Users/nicolas/Zotero/storage/RMRAX28S/Lomonaco - Continual Learning with Deep Architectures.pdf:application/pdf},
}

@incollection{schwenker_comparing_2016,
	address = {Cham},
	title = {Comparing {Incremental} {Learning} {Strategies} for {Convolutional} {Neural} {Networks}},
	volume = {9896},
	isbn = {978-3-319-46181-6 978-3-319-46182-3},
	url = {http://link.springer.com/10.1007/978-3-319-46182-3_15},
	abstract = {In the last decade, Convolutional Neural Networks (CNNs) have shown to perform incredibly well in many computer vision tasks such as object recognition and object detection, being able to extract meaningful high-level invariant features. However, partly because of their complex training and tricky hyper-parameters tuning, CNNs have been scarcely studied in the context of incremental learning where data are available in consecutive batches and retraining the model from scratch is unfeasible. In this work we compare different incremental learning strategies for CNN based architectures, targeting real-word applications.},
	language = {en},
	urldate = {2021-04-15},
	booktitle = {Artificial {Neural} {Networks} in {Pattern} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Lomonaco, Vincenzo and Maltoni, Davide},
	editor = {Schwenker, Friedhelm and Abbas, Hazem M. and El Gayar, Neamat and Trentin, Edmondo},
	year = {2016},
	doi = {10.1007/978-3-319-46182-3_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {175--184},
	file = {Lomonaco and Maltoni - 2016 - Comparing Incremental Learning Strategies for Conv.pdf:/Users/nicolas/Zotero/storage/6RP63QPU/Lomonaco and Maltoni - 2016 - Comparing Incremental Learning Strategies for Conv.pdf:application/pdf},
}

@article{pomponi_efficient_2020,
	title = {Efficient {Continual} {Learning} in {Neural} {Networks} with {Embedding} {Regularization}},
	volume = {397},
	issn = {09252312},
	url = {http://arxiv.org/abs/1909.03742},
	doi = {10.1016/j.neucom.2020.01.093},
	abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
	urldate = {2021-04-15},
	journal = {Neurocomputing},
	author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
	month = jul,
	year = {2020},
	note = {arXiv: 1909.03742},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {139--148},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/AZ6TMM7Q/1909.html:text/html;Pomponi et al. - 2020 - Efficient Continual Learning in Neural Networks wi.pdf:/Users/nicolas/Documents/Zotero/Neurocomputing2020/Pomponi et al. - 2020 - Efficient Continual Learning in Neural Networks wi.pdf:application/pdf},
}

@article{caron_deep_2019,
  title={Deep clustering for unsupervised learning of visual features},
  author={Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={132--149},
  year={2018}
}

@article{chen_exploring_2020,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2011.10566},
	abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
	urldate = {2021-04-13},
	journal = {arXiv:2011.10566 [cs]},
	author = {Chen, Xinlei and He, Kaiming},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.10566},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/NFHSWS3J/2011.html:text/html;Chen_He_2020_Exploring Simple Siamese Representation Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv2011.10566 [cs]2020/Chen_He_2020_Exploring Simple Siamese Representation Learning.pdf:application/pdf},
}

@article{chen_improved_2020,
	title = {Improved {Baselines} with {Momentum} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2003.04297},
	abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
	urldate = {2021-04-13},
	journal = {arXiv:2003.04297 [cs]},
	author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.04297},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IW9S2MJC/2003.html:text/html;Chen et al_2020_Improved Baselines with Momentum Contrastive Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv2003.04297 [cs]2020/Chen et al_2020_Improved Baselines with Momentum Contrastive Learning.pdf:application/pdf},
}

@article{yan_clusterfit_2019,
	title = {{ClusterFit}: {Improving} {Generalization} of {Visual} {Representations}},
	shorttitle = {{ClusterFit}},
	url = {http://arxiv.org/abs/1912.03330},
	abstract = {Pre-training convolutional neural networks with weakly-supervised and self-supervised strategies is becoming increasingly popular for several computer vision tasks. However, due to the lack of strong discriminative signals, these learned representations may overfit to the pre-training objective (e.g., hashtag prediction) and not generalize well to downstream tasks. In this work, we present a simple strategy - ClusterFit (CF) to improve the robustness of the visual representations learned during pre-training. Given a dataset, we (a) cluster its features extracted from a pre-trained network using k-means and (b) re-train a new network from scratch on this dataset using cluster assignments as pseudo-labels. We empirically show that clustering helps reduce the pre-training task-specific information from the extracted features thereby minimizing overfitting to the same. Our approach is extensible to different pre-training frameworks -- weak- and self-supervised, modalities -- images and videos, and pre-training tasks -- object and action classification. Through extensive transfer learning experiments on 11 different target datasets of varied vocabularies and granularities, we show that ClusterFit significantly improves the representation quality compared to the state-of-the-art large-scale (millions / billions) weakly-supervised image and video models and self-supervised image models.},
	urldate = {2021-04-13},
	journal = {arXiv:1912.03330 [cs]},
	author = {Yan, Xueting and Misra, Ishan and Gupta, Abhinav and Ghadiyaram, Deepti and Mahajan, Dhruv},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.03330},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/ZLUNWWTI/1912.html:text/html;Yan et al_2019_ClusterFit.pdf:/Users/nicolas/Documents/Zotero/arXiv1912.03330 [cs]2019/Yan et al_2019_ClusterFit.pdf:application/pdf},
}

@inproceedings{caron_unsupervised_2019,
	title = {Unsupervised {Pre}-{Training} of {Image} {Features} on {Non}-{Curated} {Data}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.html},
	urldate = {2021-04-13},
	author = {Caron, Mathilde and Bojanowski, Piotr and Mairal, Julien and Joulin, Armand},
	year = {2019},
	pages = {2959--2968},
	file = {Caron et al_2019_Unsupervised Pre-Training of Image Features on Non-Curated Data.pdf:/Users/nicolas/Documents/Zotero/2019/Caron et al_2019_Unsupervised Pre-Training of Image Features on Non-Curated Data.pdf:application/pdf;Snapshot:/Users/nicolas/Zotero/storage/E6LLFK9A/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.html:text/html},
}

@article{saito_scanimate_2021,
	title = {{SCANimate}: {Weakly} {Supervised} {Learning} of {Skinned} {Clothed} {Avatar} {Networks}},
	shorttitle = {{SCANimate}},
	url = {http://arxiv.org/abs/2104.03313},
	abstract = {We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.},
	urldate = {2021-04-09},
	journal = {arXiv:2104.03313 [cs]},
	author = {Saito, Shunsuke and Yang, Jinlong and Ma, Qianli and Black, Michael J.},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.03313
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/EDCZKQY3/2104.html:text/html;Saito et al. - 2021 - SCANimate Weakly Supervised Learning of Skinned C.pdf:/Users/nicolas/Documents/Zotero/arXiv2104.03313 [cs]2021/Saito et al. - 2021 - SCANimate Weakly Supervised Learning of Skinned C.pdf:application/pdf},
}

@article{lin_streaming_2021,
	title = {Streaming {Self}-{Training} via {Domain}-{Agnostic} {Unlabeled} {Images}},
	url = {http://arxiv.org/abs/2104.03309},
	abstract = {We present streaming self-training (SST) that aims to democratize the process of learning visual recognition models such that a non-expert user can define a new task depending on their needs via a few labeled examples and minimal domain knowledge. Key to SST are two crucial observations: (1) domain-agnostic unlabeled images enable us to learn better models with a few labeled examples without any additional knowledge or supervision; and (2) learning is a continuous process and can be done by constructing a schedule of learning updates that iterates between pre-training on novel segments of the streams of unlabeled data, and fine-tuning on the small and fixed labeled dataset. This allows SST to overcome the need for a large number of domain-specific labeled and unlabeled examples, exorbitant computational resources, and domain/task-specific knowledge. In this setting, classical semi-supervised approaches require a large amount of domain-specific labeled and unlabeled examples, immense resources to process data, and expert knowledge of a particular task. Due to these reasons, semi-supervised learning has been restricted to a few places that can house required computational and human resources. In this work, we overcome these challenges and demonstrate our findings for a wide range of visual recognition tasks including fine-grained image classification, surface normal estimation, and semantic segmentation. We also demonstrate our findings for diverse domains including medical, satellite, and agricultural imagery, where there does not exist a large amount of labeled or unlabeled data.},
	urldate = {2021-04-09},
	journal = {arXiv:2104.03309 [cs]},
	author = {Lin, Zhiqiu and Ramanan, Deva and Bansal, Aayush},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.03309
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/DYM7TQWF/2104.html:text/html;Lin et al. - 2021 - Streaming Self-Training via Domain-Agnostic Unlabe.pdf:/Users/nicolas/Documents/Zotero/arXiv2104.03309 [cs]2021/Lin et al. - 2021 - Streaming Self-Training via Domain-Agnostic Unlabe.pdf:application/pdf},
}

@article{grill_bootstrap_2020,
	title = {Bootstrap your own latent: {A} new approach to self-supervised {Learning}},
	shorttitle = {Bootstrap your own latent},
	url = {http://arxiv.org/abs/2006.07733},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6{\textbackslash}\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
	urldate = {2021-04-06},
	journal = {arXiv:2006.07733 [cs, stat]},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	month = sep,
	year = {2020},
	note = {arXiv: 2006.07733},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/6J56N237/2006.html:text/html;Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf:/Users/nicolas/Documents/Zotero/arXiv2006.07733 [cs, stat]2020/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf:application/pdf},
}

@article{chen_improved_2020-1,
	title = {Improved {Baselines} with {Momentum} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2003.04297},
	abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
	urldate = {2021-04-06},
	journal = {arXiv:2003.04297 [cs]},
	author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.04297},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/9B85MNZ5/2003.html:text/html;Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf:/Users/nicolas/Documents/Zotero/arXiv2003.04297 [cs]2020/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf:application/pdf},
}

@misc{noauthor_unsupervised_nodate,
	title = {Unsupervised {Machine} {Learning} {For} {Data} {Clustering}},
	url = {https://ehikioya.com/forums/topic/unsupervised-machine-learning-for-data-clustering/},
	abstract = {Unsupervised Machine Learning helps us understand the relationships that exist within a dataset. It is a branch of Machine Learning that learns from test data that has not been labeled, classified or categorized. In a Multiple Linear Regression problem (which is a Supervised Machine Learning Algorithm), you would typically have independent variables and a response […]},
	language = {en-US},
	urldate = {2021-03-17},
	journal = {Ehi Kioya},
	file = {Snapshot:/Users/nicolas/Zotero/storage/6HRA5F3F/unsupervised-machine-learning-for-data-clustering.html:text/html},
}

@misc{noauthor_vgg16_2018,
	title = {{VGG16} - {Convolutional} {Network} for {Classification} and {Detection}},
	url = {https://neurohive.io/en/popular-networks/vgg16/},
	abstract = {How does VGG16 neural network achieves 92.7\% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes.},
	language = {en-US},
	urldate = {2021-03-16},
	month = nov,
	year = {2018},
	file = {Snapshot:/Users/nicolas/Zotero/storage/XI3B2HC6/vgg16.html:text/html},
}

@misc{says_neural_2017,
	title = {Neural {Network} {Tutorial} - {Artificial} {Intelligence} {\textbar} {Deep} {Learning}},
	url = {https://www.edureka.co/blog/neural-network-tutorial/},
	abstract = {This blog on Neural Network tutorial, talks about what is Multi Layer Perceptron and how it works. It also includes a use-case in the end.},
	language = {en-US},
	urldate = {2021-03-16},
	journal = {Edureka},
	author = {says, Yves},
	month = dec,
	year = {2017},
	note = {Section: Uncategorized},
	file = {Snapshot:/Users/nicolas/Zotero/storage/K9QJ7V8V/neural-network-tutorial.html:text/html},
}

@misc{noauthor_what_2020,
	title = {What {Is} {Artificial} {Neural} {Network}? {All} {You} {Need} {To} {Know}},
	shorttitle = {What {Is} {Artificial} {Neural} {Network}?},
	url = {https://k21academy.com/datascience/deep-learning/artificial-neural-network/},
	abstract = {Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks.},
	language = {en-US},
	urldate = {2021-03-16},
	journal = {Cloud Training Program},
	month = dec,
	year = {2020},
	file = {Snapshot:/Users/nicolas/Zotero/storage/EYUCTQDI/artificial-neural-network.html:text/html},
}

@article{zemouri_intelligence_2019,
	title = {Intelligence artificielle : quel avenir en anatomie pathologique ?},
	volume = {39},
	shorttitle = {Intelligence artificielle},
	doi = {10.1016/j.annpat.2019.01.004},
	abstract = {Résumé
Les techniques d’intelligence artificielle et en particulier les réseaux de neurones profonds (Deep Learning) sont en pleine émergence dans le domaine biomédical. Les réseaux de neurones s’inspirent du modèle biologique, ils sont interconnectés entre eux et suivent des modèles mathématiques. Lors de l’utilisation des réseaux de neurones artificiels, deux phases sont nécessaires : une phase d’apprentissage et une phase d’exploitation. Les deux principales applications sont la classification et la régression. Des outils informatiques comme les processeurs graphiques accélérateurs de calcul ou des bibliothèques de développement spécifiques ont donné un nouveau souffle à ces techniques. Leur champ d’application est vaste et permet la gestion de données de masse (Big data) en génomique et biologie moléculaire ainsi que l’analyse automatisée de lames histologiques grâce aux techniques de numérisation réalisées à l’aide de scanners de lames de type Whole Slide Image. Le Whole Slide Image scanner peut acquérir et stocker des lames de microscopie sous forme d’image numériques. Cette numérisation associée aux algorithmes de deep learning permet une reconnaissance automatique des lésions grâce à l’identification de régions d’intérêt, validées au préalable par le pathologiste. Ces techniques d’aide assistée par ordinateur sont testées en particulier en pathologie mammaire et dermatologique. Elles permettront, associées aux données cliniques, radiologiques et de biologie moléculaire, une vision plus globale et performante, et réaliseront une aide au diagnostic en pathologie.},
	journal = {Annales de Pathologie},
	author = {Zemouri, Ryad and devalland, Christine and Valmary-Degano, Séverine and Zerhouni, Noureddine},
	month = feb,
	year = {2019},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/SYUEKPXE/Zemouri et al. - 2019 - Intelligence artificielle  quel avenir en anatomi.pdf:application/pdf},
}

@article{dave_tclr_2021,
	title = {{TCLR}: {Temporal} {Contrastive} {Learning} for {Video} {Representation}},
	shorttitle = {{TCLR}},
	url = {http://arxiv.org/abs/2101.07974},
	abstract = {Contrastive learning has nearly closed the gap between supervised and self-supervised learning of image representations. Existing extensions of contrastive learning to the domain of video data however do not explicitly attempt to represent the internal distinctiveness across the temporal dimension of video clips. We develop a new temporal contrastive learning framework consisting of two novel losses to improve upon existing contrastive self-supervised video representation learning methods. The first loss adds the task of discriminating between non-overlapping clips from the same video, whereas the second loss aims to discriminate between timesteps of the feature map of an input clip in order to increase the temporal diversity of the features. Temporal contrastive learning achieves significant improvement over the state-of-the-art results in downstream video understanding tasks such as action recognition, limited-label action classification, and nearest-neighbor video retrieval on video datasets across multiple 3D CNN architectures. With the commonly used 3D-ResNet-18 architecture, we achieve 82.4\% (+5.1\% increase over the previous best) top-1 accuracy on UCF101 and 52.9\% (+5.4\% increase) on HMDB51 action classification, and 56.2\% (+11.7\% increase) Top-1 Recall on UCF101 nearest neighbor video retrieval.},
	urldate = {2021-03-02},
	journal = {arXiv:2101.07974 [cs]},
	author = {Dave, Ishan and Gupta, Rohit and Rizve, Mamshad Nayeem and Shah, Mubarak},
	month = feb,
	year = {2021},
	note = {arXiv: 2101.07974},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/VV3WSARN/2101.html:text/html;Dave et al_2021_TCLR.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.07974 [cs]2021/Dave et al_2021_TCLR.pdf:application/pdf},
}

@article{wu_conditional_2020,
	title = {Conditional {Mutual} information-based {Contrastive} {Loss} for {Financial} {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2002.07638},
	abstract = {We present a representation learning framework for financial time series forecasting. One challenge of using deep learning models for finance forecasting is the shortage of available training data when using small datasets. Direct trend classification using deep neural networks trained on small datasets is susceptible to the overfitting problem. In this paper, we propose to first learn compact representations from time series data, then use the learned representations to train a simpler model for predicting time series movements. We consider a class-conditioned latent variable model. We train an encoder network to maximize the mutual information between the latent variables and the trend information conditioned on the encoded observed variables. We show that conditional mutual information maximization can be approximated by a contrastive loss. Then, the problem is transformed into a classification task of determining whether two encoded representations are sampled from the same class or not. This is equivalent to performing pairwise comparisons of the training datapoints, and thus, improves the generalization ability of the encoder network. We use deep autoregressive models as our encoder to capture long-term dependencies of the sequence data. Empirical experiments indicate that our proposed method has the potential to advance state-of-the-art performance.},
	urldate = {2021-03-02},
	journal = {arXiv:2002.07638 [cs, stat]},
	author = {Wu, Hanwei and Gattami, Ather and Flierl, Markus},
	month = may,
	year = {2020},
	note = {arXiv: 2002.07638},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/294MC5WG/2002.html:text/html;Wu et al_2020_Conditional Mutual information-based Contrastive Loss for Financial Time Series.pdf:/Users/nicolas/Documents/Zotero/arXiv2002.07638 [cs, stat]2020/Wu et al_2020_Conditional Mutual information-based Contrastive Loss for Financial Time Series.pdf:application/pdf},
}

@article{kamnitsas_semi-supervised_2018,
	title = {Semi-{Supervised} {Learning} via {Compact} {Latent} {Space} {Clustering}},
	url = {http://arxiv.org/abs/1806.02679},
	abstract = {We present a novel cost function for semi-supervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data.},
	urldate = {2021-03-02},
	journal = {arXiv:1806.02679 [cs, stat]},
	author = {Kamnitsas, Konstantinos and Castro, Daniel C. and Folgoc, Loic Le and Walker, Ian and Tanno, Ryutaro and Rueckert, Daniel and Glocker, Ben and Criminisi, Antonio and Nori, Aditya},
	month = jul,
	year = {2018},
	note = {arXiv: 1806.02679},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/7BFUBHPQ/1806.html:text/html;Kamnitsas et al_2018_Semi-Supervised Learning via Compact Latent Space Clustering.pdf:/Users/nicolas/Documents/Zotero/arXiv1806.02679 [cs, stat]2018/Kamnitsas et al_2018_Semi-Supervised Learning via Compact Latent Space Clustering.pdf:application/pdf},
}

@article{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2021-02-24},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.05709
version: 3},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/C4WS6256/2002.html:text/html;Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf:/Users/nicolas/Documents/Zotero/arXiv2002.05709 [cs, stat]2020/Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations2.pdf:application/pdf},
}

@article{sermanet_time-contrastive_2018,
	title = {Time-{Contrastive} {Networks}: {Self}-{Supervised} {Learning} from {Video}},
	shorttitle = {Time-{Contrastive} {Networks}},
	url = {http://arxiv.org/abs/1704.06888},
	abstract = {We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate},
	language = {en},
	urldate = {2021-02-24},
	journal = {arXiv:1704.06888 [cs]},
	author = {Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey},
	month = mar,
	year = {2018},
	note = {arXiv: 1704.06888},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Sermanet et al_2018_Time-Contrastive Networks.pdf:/Users/nicolas/Documents/Zotero/arXiv1704.06888 [cs]2018/Sermanet et al_2018_Time-Contrastive Networks.pdf:application/pdf},
}

@article{zimmermann_contrastive_2021,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	url = {http://arxiv.org/abs/2102.08850},
	abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
	urldate = {2021-02-22},
	journal = {arXiv:2102.08850 [cs]},
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.08850
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/4FZCP9XC/2102.html:text/html;Zimmermann et al_2021_Contrastive Learning Inverts the Data Generating Process.pdf:/Users/nicolas/Documents/Zotero/arXiv2102.08850 [cs]2021/Zimmermann et al_2021_Contrastive Learning Inverts the Data Generating Process.pdf:application/pdf},
}

@article{hinton_distilling_2015,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}


@article{wang_normface_2017,
	title = {{NormFace}: {L2} {Hypersphere} {Embedding} for {Face} {Verification}},
	shorttitle = {{NormFace}},
	url = {http://arxiv.org/abs/1704.06369},
	doi = {10.1145/3123266.3123359},
	abstract = {Thanks to the recent developments of Convolutional Neural Networks, the performance of face verification methods has increased rapidly. In a typical face verification method, feature normalization is a critical step for boosting performance. This motivates us to introduce and study the effect of normalization during training. But we find this is non-trivial, despite normalization being differentiable. We identify and study four issues related to normalization through mathematical analysis, which yields understanding and helps with parameter settings. Based on this analysis we propose two strategies for training using normalized features. The first is a modification of softmax loss, which optimizes cosine similarity instead of inner-product. The second is a reformulation of metric learning by introducing an agent vector for each class. We show that both strategies, and small variants, consistently improve performance by between 0.2\% to 0.4\% on the LFW dataset based on two models. This is significant because the performance of the two models on LFW dataset is close to saturation at over 98\%. Codes and models are released on https://github.com/happynear/NormFace},
	urldate = {2021-02-19},
	journal = {Proceedings of the 25th ACM international conference on Multimedia},
	author = {Wang, Feng and Xiang, Xiang and Cheng, Jian and Yuille, Alan L.},
	month = oct,
	year = {2017},
	note = {arXiv: 1704.06369},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1041--1049},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/MA7LFS46/1704.html:text/html;Wang et al_2017_NormFace.pdf:/Users/nicolas/Documents/Zotero/Proceedings of the 25th ACM international conference on Multimedia2017/Wang et al_2017_NormFace.pdf:application/pdf},
}

@article{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artiﬁcial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv: 1807.03748},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mutual Information},
	file = {Oord et al_2019_Representation Learning with Contrastive Predictive Coding.pdf:/Users/nicolas/Documents/Zotero/arXiv1807.03748 [cs, stat]2019/Oord et al_2019_Representation Learning with Contrastive Predictive Coding.pdf:application/pdf},
}

@article{hjelm_learning_2019,
	title = {Learning deep representations by mutual information estimation and maximization},
	url = {http://arxiv.org/abs/1808.06670},
	abstract = {This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can signiﬁcantly improve a representation’s suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classiﬁcation tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards ﬂexible formulations of representation learning objectives for speciﬁc end-goals.},
	language = {en},
	urldate = {2021-02-05},
	journal = {arXiv:1808.06670 [cs, stat]},
	author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
	month = feb,
	year = {2019},
	note = {arXiv: 1808.06670},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mutual Information},
}

@article{gordon_watching_2020,
	title = {Watching the {World} {Go} {By}: {Representation} {Learning} from {Unlabeled} {Videos}},
	shorttitle = {Watching the {World} {Go} {By}},
	url = {http://arxiv.org/abs/2003.07990},
	abstract = {Recent single image unsupervised representation learning techniques show remarkable success on a variety of tasks. The basic principle in these works is instance discrimination: learning to differentiate between two augmented versions of the same image and a large batch of unrelated images. Networks learn to ignore the augmentation noise and extract semantically meaningful representations. Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial ways and are not aligned with how objects actually change e.g. occlusion, deformation, viewpoint change. In this paper, we argue that videos offer this natural augmentation for free. Videos can provide entirely new views of objects, show deformation, and even connect semantically similar but visually distinct concepts. We propose Video Noise Contrastive Estimation, a method for using unlabeled video to learn strong, transferable single image representations. We demonstrate improvements over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks. Code and the Random Related Video Views dataset are available at https://www.github.com/danielgordon10/vince},
	language = {en},
	urldate = {2021-02-19},
	journal = {arXiv:2003.07990 [cs]},
	author = {Gordon, Daniel and Ehsani, Kiana and Fox, Dieter and Farhadi, Ali},
	month = may,
	year = {2020},
	note = {arXiv: 2003.07990},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Gordon et al. - 2020 - Watching the World Go By Representation Learning .pdf:/Users/nicolas/Zotero/storage/TUYINXAB/Gordon et al. - 2020 - Watching the World Go By Representation Learning .pdf:application/pdf},
}

@article{bourrier_echantillonnage_nodate,
	title = {Échantillonnage compressé et réduction de dimension pour l'apprentissage non supervisé},
	language = {fr},
	author = {Bourrier, Anthony},
	pages = {129},
	file = {Bourrier - Échantillonnage compressé et réduction de dimensio.pdf:/Users/nicolas/Zotero/storage/HHKEYQQH/Bourrier - Échantillonnage compressé et réduction de dimensio.pdf:application/pdf},
}

@inproceedings{chopra_learning_2005,
	address = {San Diego, CA, USA},
	title = {Learning a {Similarity} {Metric} {Discriminatively}, with {Application} to {Face} {Verification}},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467314/},
	doi = {10.1109/CVPR.2005.202},
	abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or veriﬁcation applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the ¢¤£ norm in the target space approximates the “semantic” distance in the input space. The method is applied to a face veriﬁcation task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artiﬁcial occlusions such as dark glasses and obscuring scarves.},
	language = {en},
	urldate = {2021-02-18},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	year = {2005},
	pages = {539--546},
	file = {Chopra et al. - 2005 - Learning a Similarity Metric Discriminatively, wit.pdf:/Users/nicolas/Zotero/storage/BGRMK3N5/Chopra et al. - 2005 - Learning a Similarity Metric Discriminatively, wit.pdf:application/pdf},
}

@article{ma_noise_2018,
	title = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}: {Consistency} and {Statistical} {Efficiency}},
	shorttitle = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}},
	url = {http://arxiv.org/abs/1809.01812},
	abstract = {Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and trade-offs of both methods.},
	urldate = {2021-02-16},
	journal = {arXiv:1809.01812 [cs, stat]},
	author = {Ma, Zhuang and Collins, Michael},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.01812},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/P24M95IV/1809.html:text/html;Ma_Collins_2018_Noise Contrastive Estimation and Negative Sampling for Conditional Models.pdf:/Users/nicolas/Documents/Zotero/arXiv1809.01812 [cs, stat]2018/Ma_Collins_2018_Noise Contrastive Estimation and Negative Sampling for Conditional Models.pdf:application/pdf},
}

@article{sermanet_time-contrastive_2018-1,
	title = {Time-{Contrastive} {Networks}: {Self}-{Supervised} {Learning} from {Video}},
	shorttitle = {Time-{Contrastive} {Networks}},
	url = {http://arxiv.org/abs/1704.06888},
	abstract = {We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate},
	language = {en},
	urldate = {2021-02-15},
	journal = {arXiv:1704.06888 [cs]},
	author = {Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey},
	month = mar,
	year = {2018},
	note = {arXiv: 1704.06888},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Sermanet et al. - 2018 - Time-Contrastive Networks Self-Supervised Learnin.pdf:/Users/nicolas/Zotero/storage/WXHTMU5C/Sermanet et al. - 2018 - Time-Contrastive Networks Self-Supervised Learnin.pdf:application/pdf},
}

@article{tian_weakly-supervised_2021,
	title = {Weakly-supervised {Video} {Anomaly} {Detection} with {Contrastive} {Learning} of {Long} and {Short}-range {Temporal} {Features}},
	url = {http://arxiv.org/abs/2101.10030},
	abstract = {In this paper, we address the problem of weaklysupervised video anomaly detection, in which given videolevel labels for training, we aim to identify in test videos, the snippets containing abnormal events. Although current methods based on multiple instance learning (MIL) show effective detection performance, they ignore important video temporal dependencies. Also, the number of abnormal snippets can vary per anomaly video, which complicates the training process of MIL-based methods because they tend to focus on the most abnormal snippet – this can cause it to mistakenly select a normal snippet instead of an abnormal snippet, and also to fail to select all abnormal snippets available. We propose a novel method, named Multiscale Temporal Network trained with top-K Contrastive Multiple Instance Learning (MTN-KMIL), to address the issues above. The main contributions of MTN-KMIL are: 1) a novel synthesis of a pyramid of dilated convolutions and a self-attention mechanism, with the former capturing the multi-scale short-range temporal dependencies between snippets and the latter capturing long-range temporal dependencies; and 2) a novel contrastive MIL learning method that enforces large margins between the top-K normal and abnormal video snippets at the feature representation level and anomaly score level, resulting in accurate anomaly discrimination. Extensive experiments show that our method outperforms several state-of-the-art methods by a large margin on three benchmark data sets (ShanghaiTech, UCF-Crime and XD-Violence). Code is available at https://github.com/tianyu0207/MTNKMIL.},
	language = {en},
	urldate = {2021-02-15},
	journal = {arXiv:2101.10030 [cs]},
	author = {Tian, Yu and Pang, Guansong and Chen, Yuanhong and Singh, Rajvinder and Verjans, Johan W. and Carneiro, Gustavo},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.10030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Tian et al_2021_Weakly-supervised Video Anomaly Detection with Contrastive Learning of Long and.pdf:/Users/nicolas/Documents/Zotero/test_group_sync/Contrastive Learning/Tian et al_2021_Weakly-supervised Video Anomaly Detection with Contrastive Learning of Long and.pdf:application/pdf},
}

@article{mnih_fast_nodate,
	title = {A fast and simple algorithm for training neural probabilistic language models},
	abstract = {In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.},
	language = {en},
	author = {Mnih, Andriy and Teh, Yee Whye},
	pages = {8},
	file = {Mnih_Teh_A fast and simple algorithm for training neural probabilistic language models.pdf:/Users/nicolas/Documents/Zotero/_/Mnih_Teh_A fast and simple algorithm for training neural probabilistic language models.pdf:application/pdf},
}

@article{dosovitskiy_image_2020,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2021-02-10},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/4KXW4ZUT/2010.html:text/html;Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf:/Users/nicolas/Documents/Zotero/arXiv2010.11929 [cs]2020/Dosovitskiy et al_2020_An Image is Worth 16x16 Words.pdf:application/pdf},
}

@article{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2021-02-10},
	journal = {arXiv:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.11946
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/GB83UQPI/1905.html:text/html;Tan_Le_2019_EfficientNet.pdf:/Users/nicolas/Documents/Zotero/arXiv1905.11946 [cs, stat]2019/Tan_Le_2019_EfficientNet.pdf:application/pdf},
}

@inproceedings{hadsell_dimensionality_2006,
	address = {New York, NY, USA},
	title = {Dimensionality {Reduction} by {Learning} an {Invariant} {Mapping}},
	volume = {2},
	isbn = {978-0-7695-2597-6},
	url = {http://ieeexplore.ieee.org/document/1640964/},
	doi = {10.1109/CVPR.2006.100},
	abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that “similar” points in input space are mapped to nearby points on the manifold. Most existing techniques for solving the problem suffer from two drawbacks. First, most of them depend on a meaningful and computable distance metric in input space. Second, they do not compute a “function” that can accurately map new input samples whose relationship to the training data is unknown. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent non-linear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distance measure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
	language = {en},
	urldate = {2021-02-08},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Volume} 2 ({CVPR}'06)},
	publisher = {IEEE},
	author = {Hadsell, R. and Chopra, S. and LeCun, Y.},
	year = {2006},
	pages = {1735--1742},
	file = {Hadsell et al_2006_Dimensionality Reduction by Learning an Invariant Mapping.pdf:/Users/nicolas/Documents/Zotero/IEEE2006/Hadsell et al_2006_Dimensionality Reduction by Learning an Invariant Mapping.pdf:application/pdf},
}

@inproceedings{khosla_supervised_2020,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18661--18673},
  year={2020}
}


@misc{tian_hobbitlongsupcontrast_2021,
	title = {{HobbitLong}/{SupContrast}},
	copyright = {BSD-2-Clause License         ,                 BSD-2-Clause License},
	url = {https://github.com/HobbitLong/SupContrast},
	abstract = {PyTorch implementation of "Supervised Contrastive Learning"  (and SimCLR incidentally)},
	urldate = {2021-02-05},
	author = {Tian, Yonglong},
	month = feb,
	year = {2021},
	note = {original-date: 2020-05-08T09:58:30Z},
}

@misc{zhirongw_zhirongwlemniscatepytorch_2021,
	title = {zhirongw/lemniscate.pytorch},
	url = {https://github.com/zhirongw/lemniscate.pytorch},
	abstract = {Unsupervised Feature Learning via Non-parametric Instance Discrimination},
	urldate = {2021-02-05},
	author = {zhirongw},
	month = feb,
	year = {2021},
	note = {original-date: 2018-05-04T08:59:24Z},
	keywords = {computer-vision, cvpr2018, deep-learning, imagenet, nce, pytorch, representation-learning, self-supervised-learning, unsupervised-learning},
}

@article{henaff_data-efficient_2020,
	title = {Data-{Efficient} {Image} {Recognition} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1905.09272},
	abstract = {Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artiﬁcial ones remains an open challenge. We hypothesize that data-efﬁcient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-theart linear classiﬁcation accuracy on the ImageNet dataset. When used as input for non-linear classiﬁcation with deep neural networks, this representation allows us to use 2–5× less labels than classiﬁers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classiﬁers.},
	language = {en},
	urldate = {2021-02-05},
	journal = {arXiv:1905.09272 [cs]},
	author = {Hénaff, Olivier J. and Srinivas, Aravind and De Fauw, Jeffrey and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and Oord, Aaron van den},
	month = jul,
	year = {2020},
	note = {arXiv: 1905.09272},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Hénaff et al_2020_Data-Efficient Image Recognition with Contrastive Predictive Coding.pdf:/Users/nicolas/Documents/Zotero/arXiv1905.09272 [cs]2020/Hénaff et al_2020_Data-Efficient Image Recognition with Contrastive Predictive Coding.pdf:application/pdf},
}

@article{mnih_learning_nodate,
	title = {Learning word embeddings efficiently with noise-contrastive estimation},
	abstract = {Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor.},
	language = {en},
	author = {Mnih, Andriy and Kavukcuoglu, Koray},
	pages = {9},
	file = {Mnih_Kavukcuoglu_Learning word embeddings efficiently with noise-contrastive estimation.pdf:/Users/nicolas/Documents/Zotero/_/Mnih_Kavukcuoglu_Learning word embeddings efficiently with noise-contrastive estimation.pdf:application/pdf},
}

@article{frosst_analyzing_2019,
	title = {Analyzing and {Improving} {Representations} with the {Soft} {Nearest} {Neighbor} {Loss}},
	url = {http://arxiv.org/abs/1902.01889},
	abstract = {We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we ﬁnd that maximizing the entanglement of representations of different classes in the hidden layers is beneﬁcial for discrimination in the ﬁnal layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.},
	language = {en},
	urldate = {2021-02-05},
	journal = {arXiv:1902.01889 [cs, stat]},
	author = {Frosst, Nicholas and Papernot, Nicolas and Hinton, Geoffrey},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.01889},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Frosst et al_2019_Analyzing and Improving Representations with the Soft Nearest Neighbor Loss.pdf:/Users/nicolas/Documents/Zotero/arXiv1902.01889 [cs, stat]2019/Frosst et al_2019_Analyzing and Improving Representations with the Soft Nearest Neighbor Loss.pdf:application/pdf},
}

@article{monnier_deep_2020,
	title = {Deep {Transformation}-{Invariant} {Clustering}},
	url = {http://arxiv.org/abs/2006.11132},
	abstract = {Recent advances in image clustering typically focus on learning better deep representations. In contrast, we present an orthogonal approach that does not rely on abstract features but instead learns to predict transformations and performs clustering directly in pixel space. This learning process naturally ﬁts in the gradient-based training of K-means and Gaussian mixture model, without requiring any additional loss or hyper-parameters. It leads us to two new deep transformation-invariant clustering frameworks, which jointly learn prototypes and transformations. More speciﬁcally, we use deep learning modules that enable us to resolve invariance to spatial, color and morphological transformations. Our approach is conceptually simple and comes with several advantages, including the possibility to easily adapt the desired invariance to the task and a strong interpretability of both cluster centers and assignments to clusters. We demonstrate that our novel approach yields competitive and highly promising results on standard image clustering benchmarks. Finally, we showcase its robustness and the advantages of its improved interpretability by visualizing clustering results over real photograph collections.},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:2006.11132 [cs, stat]},
	author = {Monnier, Tom and Groueix, Thibault and Aubry, Mathieu},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.11132},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Monnier et al_2020_Deep Transformation-Invariant Clustering.pdf:/Users/nicolas/Documents/Zotero/arXiv2006.11132 [cs, stat]2020/Monnier et al_2020_Deep Transformation-Invariant Clustering.pdf:application/pdf},
}

@article{zhong_deep_2020,
	title = {Deep {Robust} {Clustering} by {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2008.03030},
	abstract = {Recently, many unsupervised deep learning methods have been proposed to learn clustering with unlabelled data. By introducing data augmentation, most of the latest methods look into deep clustering from the perspective that the original image and its transformation should share similar semantic clustering assignment. However, the representation features could be quite different even they are assigned to the same cluster since softmax function is only sensitive to the maximum value. This may result in high intra-class diversities in the representation feature space, which will lead to unstable local optimal and thus harm the clustering performance. To address this drawback, we proposed Deep Robust Clustering (DRC). Different from existing methods, DRC looks into deep clustering from two perspectives of both semantic clustering assignment and representation feature, which can increase inter-class diversities and decrease intra-class diversities simultaneously. Furthermore, we summarized a general framework that can turn any maximizing mutual information into minimizing contrastive loss by investigating the internal relationship between mutual information and contrastive learning. And we successfully applied it in DRC to learn invariant features and robust clusters. Extensive experiments on six widely-adopted deep clustering benchmarks demonstrate the superiority of DRC in both stability and accuracy. e.g., attaining 71.6\% mean accuracy on CIFAR-10, which is 7.1\% higher than state-of-the-art results.},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:2008.03030 [cs]},
	author = {Zhong, Huasong and Chen, Chong and Jin, Zhongming and Hua, Xian-Sheng},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.03030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhong et al_2020_Deep Robust Clustering by Contrastive Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv2008.03030 [cs]2020/Zhong et al_2020_Deep Robust Clustering by Contrastive Learning.pdf:application/pdf},
}

@article{bucher_hard_2016,
	title = {Hard {Negative} {Mining} for {Metric} {Learning} {Based} {Zero}-{Shot} {Classification}},
	url = {http://arxiv.org/abs/1608.07441},
	abstract = {Zero-Shot learning has been shown to be an eﬃcient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve ZeroShot classiﬁcation problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. This paper extends their approach by proposing several schemes to control the generation of the negative pairs, resulting in a signiﬁcant improvement of the performance and giving above state-of-the-art results on three challenging ZSC datasets.},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:1608.07441 [cs, stat]},
	author = {Bucher, Maxime and Herbin, Stéphane and Jurie, Frédéric},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.07441},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Bucher et al_2016_Hard Negative Mining for Metric Learning Based Zero-Shot Classification.pdf:/Users/nicolas/Documents/Zotero/arXiv1608.07441 [cs, stat]2016/Bucher et al_2016_Hard Negative Mining for Metric Learning Based Zero-Shot Classification.pdf:application/pdf},
}

@inproceedings{he_momentum_2020,
  author       = {Kaiming He and
                  Haoqi Fan and
                  Yuxin Wu and
                  Saining Xie and
                  Ross B. Girshick},
  title        = {Momentum Contrast for Unsupervised Visual Representation Learning},
  booktitle    = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR}},
  pages        = {9726--9735},
  year         = {2020},
}



@article{vaswani_attention_nodate,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	pages = {11},
	file = {Vaswani et al_Attention is All you Need.pdf:/Users/nicolas/Documents/Zotero/_/Vaswani et al_Attention is All you Need.pdf:application/pdf},
}

@article{chen_imclr_2020,
	title = {{ImCLR}: {Implicit} {Contrastive} {Learning} for {Image} {Classification}},
	shorttitle = {{ImCLR}},
	url = {http://arxiv.org/abs/2011.12618},
	abstract = {Contrastive learning is an effective method for learning visual representations. In most cases, this involves adding an explicit loss function to encourage similar images to have similar representations, and different images to have different representations. Inspired by contrastive learning, we introduce a clever input construction for Implicit Contrastive Learning (ImCLR), primarily in the supervised setting: there, the network can implicitly learn to differentiate between similar and dissimilar images. Each input is presented as a concatenation of two images, and the label is the mean of the two one-hot labels. Furthermore, this requires almost no change to existing pipelines, which allows for easy integration and for fair demonstration of effectiveness on a wide range of well-accepted benchmarks. Namely, there is no change to loss, no change to hyperparameters, and no change to general network architecture. We show that ImCLR improves the test error in the supervised setting across a variety of settings, including 3.24\% on Tiny ImageNet, 1.30\% on CIFAR-100, 0.14\% on CIFAR-10, and 2.28\% on STL-10. We show that this holds across different number of labeled samples, maintaining approximately a 2\% gap in test accuracy down to using only 5\% of the whole dataset. We further show that gains hold for robustness to common input corruptions and perturbations at varying severities with a 0.72\% improvement on CIFAR-100-C, and in the semi-supervised setting with a 2.16\% improvement with the standard benchmark Π-model. We demonstrate that ImCLR is complementary to existing data augmentation techniques, achieving over 1\% improvement on CIFAR-100 and 2\% improvement on Tiny ImageNet by combining ImCLR with CutMix over either baseline, and 2\% by combining ImCLR with AutoAugment over either baseline.},
	language = {en},
	urldate = {2021-02-03},
	journal = {arXiv:2011.12618 [cs]},
	author = {Chen, John and Sinha, Samarth and Kyrillidis, Anastasios},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.12618},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al_2020_ImCLR.pdf:/Users/nicolas/Documents/Zotero/arXiv2011.12618 [cs]2020/Chen et al_2020_ImCLR.pdf:application/pdf},
}

@article{gutmann_noise-contrastive_2010,
	title = {Noise-contrastive estimation: {A} new estimation principle for unnormalized statistical models},
	abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation oﬀers the best trade-oﬀ between computational and statistical eﬃciency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random ﬁeld.},
	language = {en},
	author = {Gutmann, Michael and Hyvarinen, Aapo},
	year = {2010},
	pages = {8},
	file = {Gutmann_Hyvarinen_2010_Noise-contrastive estimation.pdf:/Users/nicolas/Documents/Zotero/2010/Gutmann_Hyvarinen_2010_Noise-contrastive estimation.pdf:application/pdf},
}

@article{sohn_improved_2016,
	title = {Improved {Deep} {Metric} {Learning} with {Multi}-class {N}-pair {Loss} {Objective}},
	abstract = {Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N -pair loss. The proposed objective function ﬁrstly generalizes triplet loss by allowing joint comparison among more than one negative examples – more speciﬁcally, N -1 negative examples – and secondly reduces the computational burden of evaluating deep embedding vectors via an efﬁcient batch construction strategy using only N pairs of examples, instead of (N +1)×N . We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including ﬁne-grained object recognition and veriﬁcation, image clustering and retrieval, and face veriﬁcation and identiﬁcation.},
	language = {en},
	author = {Sohn, Kihyuk},
	year = {2016},
	pages = {9},
	file = {Sohn_2016_Improved Deep Metric Learning with Multi-class N-pair Loss Objective.pdf:/Users/nicolas/Documents/Zotero/2016/Sohn_2016_Improved Deep Metric Learning with Multi-class N-pair Loss Objective.pdf:application/pdf},
}

@article{bertasius_cobe_2020,
	title = {{COBE}: {Contextualized} {Object} {Embeddings} from {Narrated} {Instructional} {Video}},
	shorttitle = {{COBE}},
	url = {http://arxiv.org/abs/2007.07306},
	abstract = {Many objects in the real world undergo dramatic variations in visual appearance. For example, a tomato may be red or green, sliced or chopped, fresh or fried, liquid or solid. Training a single detector to accurately recognize tomatoes in all these different states is challenging. On the other hand, contextual cues (e.g., the presence of a knife, a cutting board, a strainer or a pan) are often strongly indicative of how the object appears in the scene. Recognizing such contextual cues is useful not only to improve the accuracy of object detection or to determine the state of the object, but also to understand its functional properties and to infer ongoing or upcoming human-object interactions. A fully-supervised approach to recognizing object states and their contexts in the real-world is unfortunately marred by the long-tailed, open-ended distribution of the data, which would effectively require massive amounts of annotations to capture the appearance of objects in all their different forms. Instead of relying on manually-labeled data for this task, we propose a new framework for learning Contextualized OBject Embeddings (COBE) from automatically-transcribed narrations of instructional videos. We leverage the semantic and compositional structure of language by training a visual detector to predict a contextualized word embedding of the object and its associated narration. This enables the learning of an object representation where concepts relate according to a semantic language metric. Our experiments show that our detector learns to predict a rich variety of contextual object information, and that it is highly effective in the settings of few-shot and zero-shot learning.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2007.07306 [cs]},
	author = {Bertasius, Gedas and Torresani, Lorenzo},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.07306},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Bertasius_Torresani_2020_COBE.pdf:/Users/nicolas/Documents/Zotero/arXiv2007.07306 [cs]2020/Bertasius_Torresani_2020_COBE.pdf:application/pdf},
}

@article{anand_contrastive_2020,
	title = {Contrastive {Self}-{Supervised} {Learning}},
	url = {http://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html},
	abstract = {Contrastive self-supervised learning techniques are a promising class of methods that build representations by learning to encode what makes two things similar or different.},
	language = {en},
	urldate = {2021-01-27},
	journal = {ankeshanand.com},
	author = {Anand, Ankesh},
	month = jan,
	year = {2020},
	file = {Snapshot:/Users/nicolas/Zotero/storage/TXNMLA82/contrative-self-supervised-learning.html:text/html},
}

@article{chen_simple_2020-1,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.05709},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf:/Users/nicolas/Documents/Zotero/arXiv2002.05709 [cs, stat]2020/Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf:application/pdf},
}

@article{chen_sampling_2017,
	title = {On {Sampling} {Strategies} for {Neural} {Network}-based {Collaborative} {Filtering}},
	url = {http://arxiv.org/abs/1706.07881},
	abstract = {Recent advances in neural networks have inspired people to design hybrid recommendation algorithms that can incorporate both (1) user-item interaction information and (2) content information including image, audio, and text. Despite their promising results, neural network-based recommendation algorithms pose extensive computational costs, making it challenging to scale and improve upon. In this paper, we propose a general neural network-based recommendation framework, which subsumes several existing stateof-the-art recommendation algorithms, and address the efficiency issue by investigating sampling strategies in the stochastic gradient descent training for the framework. We tackle this issue by first establishing a connection between the loss functions and the useritem interaction bipartite graph, where the loss function terms are defined on links while major computation burdens are located at nodes. We call this type of loss functions “graph-based” loss functions, for which varied mini-batch sampling strategies can have different computational costs. Based on the insight, three novel sampling strategies are proposed, which can significantly improve the training efficiency of the proposed framework (up to ×30 times speedup in our experiments), as well as improving the recommendation performance. Theoretical analysis is also provided for both the computational cost and the convergence. We believe the study of sampling strategies have further implications on general graphbased loss functions, and would also enable more research under the neural network-based recommendation framework. ACM Reference format: Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong. 2017. On Sampling Strategies for Neural Network-based Collaborative Filtering. In Proceedings of KDD ’17, Halifax, NS, Canada, August 13-17, 2017, 14 pages.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:1706.07881 [cs, stat]},
	author = {Chen, Ting and Sun, Yizhou and Shi, Yue and Hong, Liangjie},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.07881},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	file = {Chen et al_2017_On Sampling Strategies for Neural Network-based Collaborative Filtering.pdf:/Users/nicolas/Documents/Zotero/arXiv1706.07881 [cs, stat]2017/Chen et al_2017_On Sampling Strategies for Neural Network-based Collaborative Filtering.pdf:application/pdf},
}

@inproceedings{covington_deep_2016,
	address = {Boston Massachusetts USA},
	title = {Deep {Neural} {Networks} for {YouTube} {Recommendations}},
	isbn = {978-1-4503-4035-9},
	url = {https://dl.acm.org/doi/10.1145/2959100.2959190},
	doi = {10.1145/2959100.2959190},
	abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: ﬁrst, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.},
	language = {en},
	urldate = {2021-01-26},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Covington, Paul and Adams, Jay and Sargin, Emre},
	month = sep,
	year = {2016},
	pages = {191--198},
	file = {Covington et al_2016_Deep Neural Networks for YouTube Recommendations.pdf:/Users/nicolas/Documents/Zotero/ACM2016/Covington et al_2016_Deep Neural Networks for YouTube Recommendations.pdf:application/pdf},
}

@article{wu_unsupervised_2018,
	title = {Unsupervised {Feature} {Learning} via {Non}-{Parametric} {Instance}-level {Discrimination}},
	url = {http://arxiv.org/abs/1805.01978},
	abstract = {Neural net classiﬁers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classiﬁcation problem at the instance-level, and use noisecontrastive estimation to tackle the computational challenges imposed by the large number of instance classes.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:1805.01978 [cs]},
	author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella and Lin, Dahua},
	month = may,
	year = {2018},
	note = {arXiv: 1805.01978},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Wu et al_2018_Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination.pdf:/Users/nicolas/Documents/Zotero/arXiv1805.01978 [cs]2018/Wu et al_2018_Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination.pdf:application/pdf},
}

@article{zhou_generating_2020,
	title = {Generating {Adjacency} {Matrix} for {Video}-{Query} based {Video} {Moment} {Retrieval}},
	url = {http://arxiv.org/abs/2008.08977},
	abstract = {In this paper, we continue our work on Video-Query based Video Moment retrieval task. Based on using graph convolution to extract intra-video and inter-video frame features, we improve the method by using similarity-metric based graph convolution, whose weighted adjacency matrix is achieved by calculating similarity metric between features of any two different timesteps in the graph. Experiments on ActivityNet v1.2 and Thumos14 dataset shows the effectiveness of this improvement, and it outperforms the state-of-the-art methods.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:2008.08977 [cs, eess]},
	author = {Zhou, Yuan and Wang, Mingfei and Wang, Ruolin and Huo, Shuwei},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.08977},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Zhou et al_2020_Generating Adjacency Matrix for Video-Query based Video Moment Retrieval.pdf:/Users/nicolas/Documents/Zotero/arXiv2008.08977 [cs, eess]2020/Zhou et al_2020_Generating Adjacency Matrix for Video-Query based Video Moment Retrieval.pdf:application/pdf},
}

@article{zhou_graph_2020,
	title = {Graph {Neural} {Network} for {Video}-{Query} based {Video} {Moment} {Retrieval}},
	url = {http://arxiv.org/abs/2007.09877},
	abstract = {In this paper, we focus on Video Query based Video Moment Retrieval (VQ-VMR) task, which uses a query video clip as input to retrieve a semantic relative video clip in another untrimmed long video. we find that in VQ-VMR datasets, there exists a phenomenon showing that there does not exist consistent relationship between feature similarity by frame and feature similarity by video, which affects the feature fusion among frames. However, existing VQ-VMR methods do not fully consider it. Taking this phenomenon into account, in this article, we treat video features as a graph by concatenating the query video feature and proposal video feature along time dimension, where each timestep is treated as a node, each row of the feature matrix is treated as feature of each node. Then, with the power of graph neural networks, we propose a Multi-Graph Feature Fusion Module to fuse the relation feature of this graph. After evaluating our method on ActivityNet v1.2 dataset and Thumos14 dataset, we find that our proposed method outperforms the state of art methods.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:2007.09877 [cs, eess]},
	author = {Zhou, Yuan and Wang, Mingfei and Wang, Ruolin and Huo, Shuwei},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.09877},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Zhou et al_2020_Graph Neural Network for Video-Query based Video Moment Retrieval.pdf:/Users/nicolas/Documents/Zotero/arXiv2007.09877 [cs, eess]2020/Zhou et al_2020_Graph Neural Network for Video-Query based Video Moment Retrieval.pdf:application/pdf},
}

@article{zhang_noise_nodate,
	title = {Noise {Learning} for {Weakly} {Supervised} {Segment} {Classiﬁcation} in {Video}},
	abstract = {This paper describes our solution for the 3rd YouTube8M video understanding challenge. The challenge of this year is different from the previous challenge. Given a large scale video dataset with video-level labels and a small scale video dataset with segment-level labels, we are asked to recognize segments in videos this year. It can be regarded as a weakly supervised learning problem. To answer the challenge, we propose a solution consists of three different models, i.e., segment-level classiﬁer, self-attention mechanism, noise learning classiﬁer. Among them, the noise learning classiﬁer performs the best. By noise learning, it can reduce the noise of label and sample for training, and improve the performance. Moreover, we achieve the MAP of 0.78878 in the private leaderboard by model ensemble based on introduced models, ranking the 8th place on the challenge.},
	language = {en},
	author = {Zhang, Zhaoyu and Wu, Xiang and Dong, Jianfeng and He, Yuan and Xue, Hui and Mao, Feng},
	pages = {5},
	file = {Zhang et al_Noise Learning for Weakly Supervised Segment Classiﬁcation in Video.pdf:/Users/nicolas/Documents/Zotero/_/Zhang et al_Noise Learning for Weakly Supervised Segment Classiﬁcation in Video.pdf:application/pdf},
}

@article{le_hierarchical_2021,
	title = {Hierarchical {Conditional} {Relation} {Networks} for {Multimodal} {Video} {Question} {Answering}},
	url = {http://arxiv.org/abs/2010.10019},
	abstract = {Video QA challenges modelers in multiple fronts. Modeling video necessitates building not only spatio-temporal models for the dynamic visual channel but also multimodal structures for associated information channels such as subtitles or audio. Video QA adds at least two more layers of complexity - selecting relevant content for each channel in the context of the linguistic query, and composing spatio-temporal concepts and relations in response to the query. To address these requirements, we start with two insights: (a) content selection and relation construction can be jointly encapsulated into a conditional computational structure, and (b) video-length structures can be composed hierarchically. For (a) this paper introduces a general-reusable neural unit dubbed Conditional Relation Network (CRN) taking as input a set of tensorial objects and translating into a new set of objects that encode relations of the inputs. The generic design of CRN helps ease the common complex model building process of Video QA by simple block stacking with flexibility in accommodating input modalities and conditioning features across both different domains. As a result, we realize insight (b) by introducing Hierarchical Conditional Relation Networks (HCRN) for Video QA. The HCRN primarily aims at exploiting intrinsic properties of the visual content of a video and its accompanying channels in terms of compositionality, hierarchy, and near and far-term relation. HCRN is then applied for Video QA in two forms, short-form where answers are reasoned solely from the visual content, and long-form where associated information, such as subtitles, presented. Our rigorous evaluations show consistent improvements over SOTAs on well-studied benchmarks including large-scale real-world datasets such as TGIF-QA and TVQA, demonstrating the strong capabilities of our CRN unit and the HCRN for complex domains such as Video QA.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:2010.10019 [cs]},
	author = {Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
	month = jan,
	year = {2021},
	note = {arXiv: 2010.10019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Le et al_2021_Hierarchical Conditional Relation Networks for Multimodal Video Question.pdf:/Users/nicolas/Documents/Zotero/arXiv2010.10019 [cs]2021/Le et al_2021_Hierarchical Conditional Relation Networks for Multimodal Video Question.pdf:application/pdf},
}

@article{mao_hierarchical_2019,
	title = {Hierarchical {Video} {Frame} {Sequence} {Representation} with {Deep} {Convolutional} {Graph} {Network}},
	volume = {11132},
	url = {http://arxiv.org/abs/1906.00377},
	doi = {10.1007/978-3-030-11018-5_24},
	abstract = {High accuracy video label prediction (classiﬁcation) models are attributed to large scale data. These data could be frame feature sequences extracted by a pre-trained convolutional-neural-network, which promote the eﬃciency for creating models. Unsupervised solutions such as feature average pooling, as a simple label-independent parameter-free based method, has limited ability to represent the video. While the supervised methods, like RNN, can greatly improve the recognition accuracy. However, the video length is usually long, and there are hierarchical relationships between frames across events in the video, the performance of RNN based models are decreased. In this paper, we proposes a novel video classiﬁcation method based on a deep convolutional graph neural network(DCGN). The proposed method utilize the characteristics of the hierarchical structure of the video, and performed multi-level feature extraction on the video frame sequence through the graph network, obtained a video representation reﬂecting the event semantics hierarchically. We test our model on YouTube-8M Large-Scale Video Understanding dataset, and the result outperforms RNN based benchmarks.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:1906.00377 [cs]},
	author = {Mao, Feng and Wu, Xiang and Xue, Hui and Zhang, Rong},
	year = {2019},
	note = {arXiv: 1906.00377},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {262--270},
	file = {Mao et al_2019_Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph.pdf:/Users/nicolas/Documents/Zotero/arXiv1906.00377 [cs]2019/Mao et al_2019_Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph.pdf:application/pdf},
}

@article{chen_negative_2020,
	title = {Negative sampling in semi-supervised learning},
	url = {http://arxiv.org/abs/1911.05166},
	abstract = {We introduce Negative Sampling in SemiSupervised Learning (NS3L), a simple, fast, easy to tune algorithm for semi-supervised learning (SSL). NS3L is motivated by the success of negative sampling/contrastive estimation. We demonstrate that adding the NS3L loss to state-of-theart SSL algorithms, such as the Virtual Adversarial Training (VAT), signiﬁcantly improves upon vanilla VAT and its variant, VAT with Entropy Minimization. By adding the NS3L loss to MixMatch, the current state-of-the-art approach on semi-supervised tasks, we observe signiﬁcant improvements over vanilla MixMatch. We conduct extensive experiments on the CIFAR10, CIFAR100, SVHN and STL10 benchmark datasets. Finally, we perform an ablation study for NS3L regarding its hyperparameter tuning.},
	language = {en},
	urldate = {2021-01-26},
	journal = {arXiv:1911.05166 [cs, stat]},
	author = {Chen, John and Shah, Vatsal and Kyrillidis, Anastasios},
	month = jun,
	year = {2020},
	note = {arXiv: 1911.05166},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chen et al_2020_Negative sampling in semi-supervised learning.pdf:/Users/nicolas/Documents/Zotero/arXiv1911.05166 [cs, stat]2020/Chen et al_2020_Negative sampling in semi-supervised learning.pdf:application/pdf},
}

@misc{liu_automix_2022,
	title = {{AutoMix}: {Unveiling} the {Power} of {Mixup} for {Stronger} {Classifiers}},
	shorttitle = {{AutoMix}},
	url = {http://arxiv.org/abs/2103.13027},
	abstract = {Data mixing augmentation have proved to be effective in improving the generalization ability of deep neural networks. While early methods mix samples by hand-crafted policies (e.g., linear interpolation), recent methods utilize saliency information to match the mixed samples and labels via complex offline optimization. However, there arises a trade-off between precise mixing policies and optimization complexity. To address this challenge, we propose a novel automatic mixup (AutoMix) framework, where the mixup policy is parameterized and serves the ultimate classification goal directly. Specifically, AutoMix reformulates the mixup classification into two sub-tasks (i.e., mixed sample generation and mixup classification) with corresponding sub-networks and solves them in a bi-level optimization framework. For the generation, a learnable lightweight mixup generator, Mix Block, is designed to generate mixed samples by modeling patch-wise relationships under the direct supervision of the corresponding mixed labels. To prevent the degradation and instability of bi-level optimization, we further introduce a momentum pipeline to train AutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks prove the superiority of AutoMix compared with state-of-the-art in various classification scenarios and downstream tasks.},
	urldate = {2022-10-28},
	publisher = {arXiv},
	author = {Liu, Zicheng and Li, Siyuan and Wu, Di and Liu, Zihan and Chen, Zhiyuan and Wu, Lirong and Li, Stan Z.},
	month = sep,
	year = {2022},
	note = {arXiv:2103.13027 [cs]
version: 6},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/JKAFVXIZ/2103.html:text/html;Liu et al. - 2022 - AutoMix Unveiling the Power of Mixup for Stronger.pdf:/Users/nicolas/Documents/Zotero/arXiv2022/Liu et al. - 2022 - AutoMix Unveiling the Power of Mixup for Stronger.pdf:application/pdf},
}

@inproceedings{variani_gaussian_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {A {Gaussian} {Mixture} {Model} layer jointly optimized with discriminative features within a {Deep} {Neural} {Network} architecture},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178776/},
	doi = {10.1109/ICASSP.2015.7178776},
	abstract = {This article proposes and evaluates a Gaussian Mixture Model (GMM) represented as the last layer of a Deep Neural Network (DNN) architecture and jointly optimized with all previous layers using Asynchronous Stochastic Gradient Descent (ASGD). The resulting “Deep GMM” architecture was investigated with special attention to the following issues: (1) The extent to which joint optimization improves over separate optimization of the DNN-based feature extraction layers and the GMM layer; (2) The extent to which depth (measured in number of layers, for a matched total number of parameters) helps a deep generative model based on the GMM layer, compared to a vanilla DNN model; (3) Head-to-head performance of Deep GMM architectures vs. equivalent DNN architectures of comparable depth, using the same optimization criterion (frame-level Cross Entropy (CE)) and optimization method (ASGD); (4) Expanded possibilities for modeling offered by the Deep GMM generative model. The proposed Deep GMMs were found to yield Word Error Rates (WERs) competitive with state-of-the-art DNN systems, at the cost of pre-training using standard DNNs to initialize the Deep GMM feature extraction layers. An extension to Deep Subspace GMMs is described, resulting in additional gains.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Variani, Ehsan and McDermott, Erik and Heigold, Georg},
	month = apr,
	year = {2015},
	pages = {4270--4274},
	file = {Variani et al. - 2015 - A Gaussian Mixture Model layer jointly optimized w.pdf:/Users/nicolas/Zotero/storage/6JK9KBES/Variani et al. - 2015 - A Gaussian Mixture Model layer jointly optimized w.pdf:application/pdf},
}

@article{michel_banner_2018,
	title = {Banner {Click} {Through} {Rate} {Classification} {Using} {Deep} {Convolutional} {Neural} {Network}},
	abstract = {In banner advertising, Click Through Rate (CTR) is one of the most important indicators to evaluate one advertisement’s quality. Advertisers create massive number of banner candidates in empirical ways, then proceed to actual tests by delivering advertisement to measure each banner’s effectiveness. This process is expensive and therefore our CTR prediction helps reducing online advertising costs. In this work, we propose a method to classify ‘effective’ and ‘ineffective’ advertising banners based on image processing using state-of-the-art CNN. We first focus only on images then conduct experiments including metadata (product, advertiser, etc) to increase the CTR prediction accuracy and demonstrate which metadata is the most influential. Subsequently, each approach is compared to human performance. In the second part of our work, we detect which parts of the image contribute predominantly to increase the CTR by generating heat maps for each classes. This work leads to a deeper understanding of a banner advertising success and helps making decisions on how to improve it.},
	language = {en},
	author = {Michel, Nicolas and Sakata, Hayato and Kurita, Keita and Yamasaki, Toshihiko},
	year = {2018},
	pages = {4},
	file = {Michel et al. - 2018 - Banner Click Through Rate Classification Using Dee.pdf:/Users/nicolas/Zotero/storage/LFMK5BDH/Michel et al. - 2018 - Banner Click Through Rate Classification Using Dee.pdf:application/pdf},
}

@inproceedings{michel_banner_2018-1,
	title = {Banner {Click} {Through} {Rate} {Classification} {Using} {Deep} {Convolutional} {Neural} {Network}},
	url = {https://confit.atlas.jp/guide/event/jsai2018/subject/1O1-01/detail},
	abstract = {JSAI2018,Banner Click Through Rate Classification Using Deep Convolutional Neural Network},
	urldate = {2022-10-13},
	author = {Michel, Nicolas},
	month = apr,
	year = {2018},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/9I2VXZPB/Michel - 2018 - Banner Click Through Rate Classification Using Dee.pdf:application/pdf;Snapshot:/Users/nicolas/Zotero/storage/QI88AMG4/detail.html:text/html},
}

@misc{michel_contrastive_2022,
	title = {Contrastive {Learning} for {Online} {Semi}-{Supervised} {General} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2207.05615},
	doi = {10.48550/arXiv.2207.05615},
	abstract = {We study Online Continual Learning with missing labels and propose SemiCon, a new contrastive loss designed for partly labeled data. We demonstrate its efficiency by devising a memory-based method trained on an unlabeled data stream, where every data added to memory is labeled using an oracle. Our approach outperforms existing semi-supervised methods when few labels are available, and obtain similar results to state-of-the-art supervised methods while using only 2.6\% of labels on Split-CIFAR10 and 10\% of labels on Split-CIFAR100.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Michel, Nicolas and Negrel, Romain and Chierchia, Giovanni and Bercher, Jean-François},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05615 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/QJQCNBHI/2207.html:text/html;Michel et al. - 2022 - Contrastive Learning for Online Semi-Supervised Ge.pdf:/Users/nicolas/Documents/Zotero/arXiv2022/Michel et al. - 2022 - Contrastive Learning for Online Semi-Supervised Ge.pdf:application/pdf},
}

@misc{inoue_cross-domain_2018,
	title = {Cross-{Domain} {Weakly}-{Supervised} {Object} {Detection} through {Progressive} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1803.11365},
	doi = {10.48550/arXiv.1803.11365},
	abstract = {Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Inoue, Naoto and Furuta, Ryosuke and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
	month = mar,
	year = {2018},
	note = {arXiv:1803.11365 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SUBPDNAA/1803.html:text/html;Inoue et al. - 2018 - Cross-Domain Weakly-Supervised Object Detection th.pdf:/Users/nicolas/Documents/Zotero/arXiv2018/Inoue et al. - 2018 - Cross-Domain Weakly-Supervised Object Detection th.pdf:application/pdf},
}

@article{tao_improved_2022,
	title = {An {Improved} {Inter}-{Intra} {Contrastive} {Learning} {Framework} on {Self}-{Supervised} {Video} {Representation}},
	volume = {32},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2022.3141051},
	abstract = {In this paper, we propose a self-supervised contrastive learning method to learn video feature representations. In traditional self-supervised contrastive learning methods, constraints from anchor, positive, and negative data pairs are used to train the model. In such a case, different samplings of the same video are treated as positives, and video clips from different videos are treated as negatives. Because the spatio-temporal information is important for video representation, we set the temporal constraints more strictly by introducing intra-negative samples. In addition to samples from different videos, negative samples are extended by breaking temporal relations in video clips from the same anchor video. With the proposed Inter-Intra Contrastive (IIC) framework, we can train spatio-temporal convolutional networks to learn feature representations from videos. Strong data augmentations, residual clips, as well as head projector are utilized to construct an improved version. Three kinds of intra-negative generation functions are proposed and extensive experiments using different network backbones are conducted on benchmark datasets. Without using pre-computed optical flow data, our improved version can outperform previous IIC by a large margin, such as 19.4\% (from 36.8\% to 56.2\%) and 5.2\% (from 15.5\% to 20.7\%) points improvements in top-1 accuracy on UCF101 and HMDB51 datasets for video retrieval, respectively. For video recognition, over 3\% points improvements can also be obtained on these two benchmark datasets. Discussions and visualizations validate that our IICv2 can capture better temporal clues and indicate the potential mechanism.},
	number = {8},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Tao, Li and Wang, Xueting and Yamasaki, Toshihiko},
	month = aug,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Data models, Feature extraction, Learning systems, Optical imaging, Optical sensors, Representation learning, Self-supervised learning, spatio-temporal convolution, Task analysis, video recognition, video representation, video retrieval},
	pages = {5266--5280},
	file = {IEEE Xplore Abstract Record:/Users/nicolas/Zotero/storage/CDKTBBLM/9674754.html:text/html;Tao et al. - 2022 - An Improved Inter-Intra Contrastive Learning Frame.pdf:/Users/nicolas/Documents/Zotero/IEEE Transactions on Circuits and Systems for Video Technology2022/Tao et al. - 2022 - An Improved Inter-Intra Contrastive Learning Frame.pdf:application/pdf},
}

@misc{yu_scale_2022,
	title = {{SCALE}: {Online} {Self}-{Supervised} {Lifelong} {Learning} without {Prior} {Knowledge}},
	shorttitle = {{SCALE}},
	url = {http://arxiv.org/abs/2208.11266},
	doi = {10.48550/arXiv.2208.11266},
	abstract = {Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Previous works assumed strong prior knowledge about the incoming data (e.g., knowing the class boundaries) which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we formally define the online unsupervised lifelong learning problem with class-incremental streaming data, which is non-iid and single-pass. The problem is more challenging than existing lifelong learning problems due to the absence of labels and prior knowledge. To address the issue, we propose Self-Supervised ContrAstive Lifelong LEarning (SCALE) which extracts and memorizes knowledge on-the-fly. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgetting loss, and an online memory update for uniform subset selection. All three components are designed to work collaboratively to maximize learning performance. Our loss functions leverage pairwise similarity thus remove the dependency on supervision or prior knowledge. We perform comprehensive experiments of SCALE under iid and four non-iid data streams. SCALE outperforms the best state-of-the-art algorithm on all settings with improvements of up to 6.43\%, 5.23\% and 5.86\% kNN accuracy on CIFAR-10, CIFAR-100 and SubImageNet datasets.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Yu, Xiaofan and Guo, Yunhui and Gao, Sicun and Rosing, Tajana},
	month = aug,
	year = {2022},
	note = {Number: arXiv:2208.11266
arXiv:2208.11266 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/GWI7GHQG/2208.html:text/html;Yu et al_2022_SCALE.pdf:/Users/nicolas/Documents/Zotero/arXiv2022/Yu et al_2022_SCALE.pdf:application/pdf},
}

@article{wang_continual_2022,
	title = {Continual {Learning} through {Retrieval} and {Imagination}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20837},
	doi = {10.1609/aaai.v36i8.20837},
	abstract = {Continual learning is an intellectual ability of artificial agents to learn new concepts from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting.},
	language = {en},
	number = {8},
	urldate = {2022-07-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Zhen and Liu, Liu and Duan, Yiqun and Tao, Dacheng},
	month = jun,
	year = {2022},
	pages = {8594--8602},
	file = {Wang et al. - 2022 - Continual Learning through Retrieval and Imaginati.pdf:/Users/nicolas/Zotero/storage/PWPTATFL/Wang et al. - 2022 - Continual Learning through Retrieval and Imaginati.pdf:application/pdf},
}

@inproceedings{zhao2022decoupled,
  title={Decoupled knowledge distillation},
  author={Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11953--11962},
  year={2022}
}


@inproceedings{negrel_boosted_2015,
	address = {Swansea},
	title = {Boosted {Metric} {Learning} for {Efficient} {Identity}-{Based} {Face} {Retrieval}},
	isbn = {978-1-901725-53-7},
	url = {http://www.bmva.org/bmvc/2015/papers/paper139/index.html},
	doi = {10.5244/C.29.139},
	abstract = {This paper presents MLBoost, an efﬁcient method for learning to compare face signatures, and shows its application to the hierarchical organization of large face databases. More precisely, the proposed metric learning (ML) algorithm is based on boosting so that the metric is learned iteratively by combining several weak metrics. Boosting allows our method to be free of any hyper-parameters (no cross-validation required) and to be robust with respect to overﬁtting. This MLBoost algorithm can be trained from constraints involving two pairs of vectors (quadruplets) with a quadratic complexity. The paper also shows how it can be included in a semi-supervised hierarchical clustering framework adapted to identity based face search. Our approach is validated on a benchmark relying on the Labelled Faces in the Wild (LFW) dataset supplemented with 1M face distractors.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2015},
	publisher = {British Machine Vision Association},
	author = {Negrel, Romain and Lechervy, Alexis and Jurie, Frederic},
	year = {2015},
	keywords = {boosting},
	pages = {139.1--139.12},
	file = {Negrel et al. - 2015 - Boosted Metric Learning for Efficient Identity-Bas.pdf:/Users/nicolas/Zotero/storage/EHHP3J8H/Negrel et al. - 2015 - Boosted Metric Learning for Efficient Identity-Bas.pdf:application/pdf},
}

@techreport{zhang_feature_2022,
	title = {Feature {Forgetting} in {Continual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2205.13359},
	abstract = {In continual and lifelong learning, good representation learning can help increase performance and reduce sample complexity when learning new tasks. There is evidence that representations do not suffer from "catastrophic forgetting" even in plain continual learning, but little further fact is known about its characteristics. In this paper, we aim to gain more understanding about representation learning in continual learning, especially on the feature forgetting problem. We devise a protocol for evaluating representation in continual learning, and then use it to present an overview of the basic trends of continual representation learning, showing its consistent deficiency and potential issues. To study the feature forgetting problem, we create a synthetic dataset to identify and visualize the prevalence of feature forgetting in neural networks. Finally, we propose a simple technique using gating adapters to mitigate feature forgetting. We conclude by discussing that improving representation learning benefits both old and new tasks in continual learning.},
	number = {arXiv:2205.13359},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Zhang, Xiao and Dou, Dejing and Wu, Ji},
	month = may,
	year = {2022},
	note = {arXiv:2205.13359 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SZXHPICT/2205.html:text/html;Zhang et al. - 2022 - Feature Forgetting in Continual Representation Lea.pdf:/Users/nicolas/Documents/Zotero/arXiv2022/Zhang et al. - 2022 - Feature Forgetting in Continual Representation Lea.pdf:application/pdf},
}

@techreport{ho_prototypes-guided_2021,
	title = {Prototypes-{Guided} {Memory} {Replay} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/2108.12641},
	abstract = {Continual learning (CL) refers to a machine learning paradigm that using only a small account of training samples and previously learned knowledge to enhance learning performance. CL models learn tasks from various domains in a sequential manner. The major difficulty in CL is catastrophic forgetting of previously learned tasks, caused by shifts in data distributions. The existing CL models often employ a replay-based approach to diminish catastrophic forgetting. Most CL models stochastically select previously seen samples to retain learned knowledge. However, occupied memory size keeps enlarging along with accumulating learned tasks. Hereby, we propose a memory-efficient CL method. We devise a dynamic prototypes-guided memory replay module, incorporating it into an online meta-learning model. We conduct extensive experiments on text classification and additionally investigate the effect of training set orders on CL model performance. The experimental results testify the superiority of our method in alleviating catastrophic forgetting and enabling efficient knowledge transfer.},
	number = {arXiv:2108.12641},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Ho, Stella and Liu, Ming and Du, Lan and Gao, Longxiang and Xiang, Yong},
	month = aug,
	year = {2021},
	note = {arXiv:2108.12641 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Prototype},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/INSTFJ58/2108.html:text/html;Ho et al. - 2021 - Prototypes-Guided Memory Replay for Continual Lear.pdf:/Users/nicolas/Documents/Zotero/arXiv2021/Ho et al. - 2021 - Prototypes-Guided Memory Replay for Continual Lear.pdf:application/pdf},
}

@article{davari_probing_2022,
	title = {Probing {Representation} {Forgetting} in {Supervised} and {Unsupervised} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2203.13381},
	abstract = {Continual Learning research typically focuses on tackling the phenomenon of catastrophic forgetting in neural networks. Catastrophic forgetting is associated with an abrupt loss of knowledge previously learned by a model when the task, or more broadly the data distribution, being trained on changes. In supervised learning problems this forgetting, resulting from a change in the model's representation, is typically measured or observed by evaluating the decrease in old task performance. However, a model's representation can change without losing knowledge about prior tasks. In this work we consider the concept of representation forgetting, observed by using the difference in performance of an optimal linear classifier before and after a new task is introduced. Using this tool we revisit a number of standard continual learning benchmarks and observe that, through this lens, model representations trained without any explicit control for forgetting often experience small representation forgetting and can sometimes be comparable to methods which explicitly control for forgetting, especially in longer task sequences. We also show that representation forgetting can lead to new insights on the effect of model capacity and loss function used in continual learning. Based on our results, we show that a simple yet competitive approach is to learn representations continually with standard supervised contrastive learning while constructing prototypes of class samples when queried on old samples.},
	urldate = {2022-04-04},
	journal = {arXiv:2203.13381 [cs]},
	author = {Davari, MohammadReza and Asadi, Nader and Mudur, Sudhir and Aljundi, Rahaf and Belilovsky, Eugene},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.13381},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SD8IBCVG/2203.html:text/html;Davari et al_2022_Probing Representation Forgetting in Supervised and Unsupervised Continual.pdf:/Users/nicolas/Documents/Zotero/arXiv2203.13381 [cs]2022/Davari et al_2022_Probing Representation Forgetting in Supervised and Unsupervised Continual.pdf:application/pdf},
}

@article{pham_continual_2022,
	title = {Continual {Normalization}: {Rethinking} {Batch} {Normalization} for {Online} {Continual} {Learning}},
	shorttitle = {Continual {Normalization}},
	url = {http://arxiv.org/abs/2203.16102},
	abstract = {Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and non-stationary nature of continual learning data, especially in the online setting, amplify the discrepancy between training and testing in BN and hinder the performance of older tasks. In this work, we study the cross-task normalization effect of BN in online continual learning where BN normalizes the testing data using moments biased towards the current task, resulting in higher catastrophic forgetting. This limitation motivates us to propose a simple yet effective method that we call Continual Normalization (CN) to facilitate training similar to BN while mitigating its negative effect. Extensive experiments on different continual learning algorithms and online scenarios show that CN is a direct replacement for BN and can provide substantial performance improvements. Our implementation is available at {\textbackslash}url\{https://github.com/phquang/Continual-Normalization\}.},
	urldate = {2022-04-04},
	journal = {arXiv:2203.16102 [cs]},
	author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.16102},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/93PJYMNY/2203.html:text/html;Pham et al_2022_Continual Normalization.pdf:/Users/nicolas/Documents/Zotero/arXiv2203.16102 [cs]2022/Pham et al_2022_Continual Normalization.pdf:application/pdf},
}

@inproceedings{gopalakrishnan_knowledge_2022,
	address = {Waikoloa, HI, USA},
	title = {Knowledge {Capture} and {Replay} for {Continual} {Learning}},
	isbn = {978-1-66540-915-5},
	url = {https://ieeexplore.ieee.org/document/9706856/},
	doi = {10.1109/WACV51458.2022.00041},
	abstract = {Deep neural networks model data for a task or a sequence of tasks, where the knowledge extracted from the data is encoded in the parameters and representations of the network. Extraction and utilization of these representations is vital when data is no longer available in the future, especially in a continual learning scenario. We introduce flashcards, which are visual representations that capture the encoded knowledge of a network as a recursive function of some predefined random image patterns. In a continual learning scenario, flashcards help to prevent catastrophic forgetting by consolidating the knowledge of all the previous tasks. Flashcards are required to be constructed only before learning the subsequent task, hence, they are independent of the number of tasks trained before, making them task agnostic. We demonstrate the efficacy of flashcards in capturing learned knowledge representation (as an alternative to the original data), and empirically validate on a variety of continual learning tasks: reconstruction, denoising, and task-incremental classification, using several heterogeneous (varying background and complexity) benchmark datasets. Experimental evidence indicates that: (i) flashcards as a replay strategy is task agnostic, (ii) performs better than generative replay, and (iii) is on par with episodic replay without additional memory overhead.},
	language = {en},
	urldate = {2022-03-15},
	booktitle = {2022 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Gopalakrishnan, Saisubramaniam and Singh, Pranshu Ranjan and Fayek, Haytham and Ramasamy, Savitha and Ambikapathi, ArulMurugan},
	month = jan,
	year = {2022},
	pages = {337--345},
	file = {Gopalakrishnan et al. - 2022 - Knowledge Capture and Replay for Continual Learnin.pdf:/Users/nicolas/Zotero/storage/9QDU9UK7/Gopalakrishnan et al. - 2022 - Knowledge Capture and Replay for Continual Learnin.pdf:application/pdf},
}

@article{thai_does_2021,
	title = {Does {Continual} {Learning} = {Catastrophic} {Forgetting}?},
	url = {http://arxiv.org/abs/2101.07295},
	abstract = {Continual learning is known for suffering from catastrophic forgetting, a phenomenon where earlier learned concepts are forgotten at the expense of more recent samples. In this work, we challenge the assumption that continual learning is inevitably associated with catastrophic forgetting by presenting a set of tasks that surprisingly do not suffer from catastrophic forgetting when learned continually. We provide evidence that these reconstruction-type tasks exhibit positive forward transfer and that single-view 3D shape reconstruction improves the performance on learned and novel categories over time. We provide the novel analysis of knowledge transfer ability by looking at the output distribution shift across sequential learning tasks. Finally, we show that the robustness of these tasks leads to the potential of having a proxy representation learning task for continual classification. The codebase, dataset, and pre-trained models released with this article can be found at https://github.com/rehg-lab/CLRec.},
	urldate = {2022-03-15},
	journal = {arXiv:2101.07295 [cs]},
	author = {Thai, Anh and Stojanov, Stefan and Huang, Zixuan and Rehg, Isaac and Rehg, James M.},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.07295
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/VL7MRNGQ/2101.html:text/html;Thai et al. - 2021 - Does Continual Learning = Catastrophic Forgetting.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.07295 [cs]2021/Thai et al. - 2021 - Does Continual Learning = Catastrophic Forgetting.pdf:application/pdf},
}

@article{wang_ordisco_2021,
	title = {{ORDisCo}: {Effective} and {Efficient} {Usage} of {Incremental} {Unlabeled} {Data} for {Semi}-supervised {Continual} {Learning}},
	shorttitle = {{ORDisCo}},
	url = {http://arxiv.org/abs/2101.00407},
	abstract = {Continual learning usually assumes the incoming data are fully labeled, which might not be applicable in real applications. In this work, we consider semi-supervised continual learning (SSCL) that incrementally learns from partially labeled data. Observing that existing continual learning methods lack the ability to continually exploit the unlabeled data, we propose deep Online Replay with Discriminator Consistency (ORDisCo) to interdependently learn a classifier with a conditional generative adversarial network (GAN), which continually passes the learned data distribution to the classifier. In particular, ORDisCo replays data sampled from the conditional generator to the classifier in an online manner, exploiting unlabeled data in a time- and storage-efficient way. Further, to explicitly overcome the catastrophic forgetting of unlabeled data, we selectively stabilize parameters of the discriminator that are important for discriminating the pairs of old unlabeled data and their pseudo-labels predicted by the classifier. We extensively evaluate ORDisCo on various semi-supervised learning benchmark datasets for SSCL, and show that ORDisCo achieves significant performance improvement on SVHN, CIFAR10 and Tiny-ImageNet, compared to strong baselines.},
	urldate = {2022-03-15},
	journal = {arXiv:2101.00407 [cs, stat]},
	author = {Wang, Liyuan and Yang, Kuo and Li, Chongxuan and Hong, Lanqing and Li, Zhenguo and Zhu, Jun},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.00407
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/4HVJE9JT/2101.html:text/html;Wang et al. - 2021 - ORDisCo Effective and Efficient Usage of Incremen.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.00407 [cs, stat]2021/Wang et al. - 2021 - ORDisCo Effective and Efficient Usage of Incremen.pdf:application/pdf},
}

@misc{noauthor_representation_nodate,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {https://deepmind.com/research/publications/2019/representation-learning-contrastive-predictive-coding},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding.},
	language = {ALL},
	urldate = {2022-02-16},
	journal = {Deepmind},
	file = {Snapshot:/Users/nicolas/Zotero/storage/2DRUC47X/representation-learning-contrastive-predictive-coding.html:text/html},
}

@inproceedings{singh_task-agnostic_2021,
	title = {Task-{Agnostic} {Continual} {Learning} {Using} {Base}-{Child} {Classifiers}},
	doi = {10.1109/ICIP42928.2021.9506504},
	abstract = {Continual learning (CL) aims to learn new tasks by forward transfer of information learnt from previous tasks and without forgetting them. In task incremental CL, task information is vital during both strategy development and inference. Providing such partial knowledge about the test sample demands additional complexity and may become intractable, especially when the sample source is ambiguous. In this work, we design a task-agnostic approach that uses base-child hybrid setup to incrementally learn tasks while mitigating forgetting. Multiple base classifiers guided by reference points learn new tasks and this information is distilled via feature space induced sampling strategy. A central child classifier consolidates information across tasks and infers the task identifier automatically. Experimental results on standard datasets show that the proposed approach outperforms the various state-of-the-art regularization and replay CL algorithms in terms of accuracy, by 50\% and 7\% with homogeneous and heterogeneous tasks, respectively, in task-agnostic scenarios.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Singh, Pranshu Ranjan and Gopalakrishnan, Saisubramaniam and ZhongZheng, Qiao and Suganthan, Ponnuthurai N. and Ramasamy, Savitha and Ambikapathi, ArulMurugan},
	month = sep,
	year = {2021},
	note = {ISSN: 2381-8549},
	keywords = {Task analysis, Classification algorithms, Complexity theory, Conferences, Continual Learning, Hybrid Networks, Image processing, Knowledge Distillation, Standards, Task-Incremental Classification},
	pages = {794--798},
	file = {IEEE Xplore Abstract Record:/Users/nicolas/Zotero/storage/TB7A2GZB/9506504.html:text/html;Singh et al. - 2021 - Task-Agnostic Continual Learning Using Base-Child .pdf:/Users/nicolas/Documents/Zotero/2021/Singh et al. - 2021 - Task-Agnostic Continual Learning Using Base-Child .pdf:application/pdf},
}

@article{brahma_hypernetworks_2021,
	title = {Hypernetworks for {Continual} {Semi}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2110.01856},
	abstract = {Learning from data sequentially arriving, possibly in a non i.i.d. way, with changing task distribution over time is called continual learning. Much of the work thus far in continual learning focuses on supervised learning and some recent works on unsupervised learning. In many domains, each task contains a mix of labelled (typically very few) and unlabelled (typically plenty) training examples, which necessitates a semi-supervised learning approach. To address this in a continual learning setting, we propose a framework for semi-supervised continual learning called Meta-Consolidation for Continual Semi-Supervised Learning (MCSSL). Our framework has a hypernetwork that learns the metadistribution that generates the weights of a semisupervised auxiliary classiﬁer generative adversarial network (Semi-ACGAN) as the base network. We consolidate the knowledge of sequential tasks in the hypernetwork, and the base network learns the semi-supervised learning task. Further, we present Semi-Split CIFAR-10, a new benchmark for continual semi-supervised learning, obtained by modifying the Split CIFAR-10 dataset, in which the tasks with labelled and unlabelled data arrive sequentially. Our proposed model yields signiﬁcant improvements in the continual semi-supervised learning setting. We compare the performance of several existing continual learning approaches on the proposed continual semi-supervised learning benchmark of the Semi-Split CIFAR-10 dataset.},
	language = {en},
	urldate = {2022-02-08},
	journal = {arXiv:2110.01856 [cs, stat]},
	author = {Brahma, Dhanajit and Verma, Vinay Kumar and Rai, Piyush},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.01856},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Brahma et al. - 2021 - Hypernetworks for Continual Semi-Supervised Learni.pdf:/Users/nicolas/Zotero/storage/GJ4QM8SN/Brahma et al. - 2021 - Hypernetworks for Continual Semi-Supervised Learni.pdf:application/pdf},
}

@incollection{mccloskey_catastrophic_1989,
	title = {Catastrophic {Interference} in {Connectionist} {Networks}: {The} {Sequential} {Learning} {Problem}},
	volume = {24},
	shorttitle = {Catastrophic {Interference} in {Connectionist} {Networks}},
	url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
	abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
	language = {en},
	urldate = {2022-02-08},
	booktitle = {Psychology of {Learning} and {Motivation}},
	publisher = {Academic Press},
	author = {McCloskey, Michael and Cohen, Neal J.},
	editor = {Bower, Gordon H.},
	month = jan,
	year = {1989},
	doi = {10.1016/S0079-7421(08)60536-8},
	pages = {109--165},
	file = {ScienceDirect Snapshot:/Users/nicolas/Zotero/storage/GZFTS7NC/S0079742108605368.html:text/html},
}

@article{he_deep_2015,
 title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{boschini_continual_2021,
	title = {Continual {Semi}-{Supervised} {Learning} through {Contrastive} {Interpolation} {Consistency}},
	url = {http://arxiv.org/abs/2108.06552},
	abstract = {Continual Learning (CL) investigates how to train Deep Networks on a stream of tasks without incurring forgetting. CL settings proposed in literature assume that every incoming example is paired with ground-truth annotations. However, this clashes with many real-world applications: gathering labeled data, which is in itself tedious and expensive, becomes infeasible when data flow as a stream. This work explores Continual Semi-Supervised Learning (CSSL): here, only a small fraction of labeled input examples are shown to the learner. We assess how current CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this novel and challenging scenario, where overfitting entangles forgetting. Subsequently, we design a novel CSSL method that exploits metric learning and consistency regularization to leverage unlabeled examples while learning. We show that our proposal exhibits higher resilience to diminishing supervision and, even more surprisingly, relying only on 25\% supervision suffices to outperform SOTA methods trained under full supervision.},
	language = {en},
	urldate = {2022-01-31},
	journal = {arXiv:2108.06552 [cs, stat]},
	author = {Boschini, Matteo and Buzzega, Pietro and Bonicelli, Lorenzo and Porrello, Angelo and Calderara, Simone},
	month = dec,
	year = {2021},
	note = {arXiv: 2108.06552},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Boschini et al. - 2021 - Continual Semi-Supervised Learning through Contras.pdf:/Users/nicolas/Zotero/storage/VWZJ6XSU/Boschini et al. - 2021 - Continual Semi-Supervised Learning through Contras.pdf:application/pdf},
}

@article{chaudhry_using_2021,
	title = {Using {Hindsight} to {Anchor} {Past} {Knowledge} in {Continual} {Learning}},
	url = {http://arxiv.org/abs/2002.08165},
	abstract = {In continual learning, the learner faces a stream of data whose distribution changes over time. Modern neural networks are known to suffer under this setting, as they quickly forget previously acquired knowledge. To address such catastrophic forgetting, many continual learning methods implement different types of experience replay, re-learning on past data stored in a small buffer known as episodic memory. In this work, we complement experience replay with a new objective that we call “anchoring”, where the learner uses bilevel optimization to update its knowledge on the current task, while keeping intact predictions on some anchor points of past tasks. These anchor points are learned using gradientbased optimization to maximize forgetting, which is approximated by ﬁne-tuning the currently trained model on the episodic memory of past tasks. Experiments on several supervised learning benchmarks for continual learning demonstrate that our approach improves the standard experience replay in terms of both accuracy and forgetting metrics and for various sizes of episodic memory.},
	language = {en},
	urldate = {2022-01-31},
	journal = {arXiv:2002.08165 [cs, stat]},
	author = {Chaudhry, Arslan and Gordo, Albert and Dokania, Puneet K. and Torr, Philip and Lopez-Paz, David},
	month = mar,
	year = {2021},
	note = {arXiv: 2002.08165},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chaudhry et al. - 2021 - Using Hindsight to Anchor Past Knowledge in Contin.pdf:/Users/nicolas/Zotero/storage/37MSUPCN/Chaudhry et al. - 2021 - Using Hindsight to Anchor Past Knowledge in Contin.pdf:application/pdf},
}

@article{shahbaz_international_2021,
	title = {International {Workshop} on {Continual} {Semi}-{Supervised} {Learning}: {Introduction}, {Benchmarks} and {Baselines}},
	shorttitle = {International {Workshop} on {Continual} {Semi}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2110.14613},
	abstract = {The aim of this paper is to formalise a new continual semi-supervised learning (CSSL) paradigm, proposed to the attention of the machine learning community via the IJCAI 2021 International Workshop on Continual Semi-Supervised Learning (CSSL@IJCAI)1, with the aim of raising the ﬁeld’s awareness about this problem and mobilising its effort in this direction. After a formal deﬁnition of continual semi-supervised learning and the appropriate training and testing protocols, the paper introduces two new benchmarks speciﬁcally designed to assess CSSL on two important computer vision tasks: activity recognition and crowd counting. We describe the Continual Activity Recognition (CAR) and Continual Crowd Counting (CCC) challenges built upon those benchmarks, the baseline models proposed for the challenges, and describe a simple CSSL baseline which consists in applying batch self-training in temporal sessions, for a limited number of rounds. The results show that learning from unlabelled data streams is extremely challenging, and stimulate the search for methods that can encode the dynamics of the data stream.},
	language = {en},
	urldate = {2022-01-31},
	journal = {arXiv:2110.14613 [cs]},
	author = {Shahbaz, Ajmal and Khan, Salman and Hossain, Mohammad Asiful and Lomonaco, Vincenzo and Cannons, Kevin and Xu, Zhan and Cuzzolin, Fabio},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.14613},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Shahbaz et al. - 2021 - International Workshop on Continual Semi-Supervise.pdf:/Users/nicolas/Zotero/storage/NZFSVLBM/Shahbaz et al. - 2021 - International Workshop on Continual Semi-Supervise.pdf:application/pdf},
}

@article{lee_neural_2020,
	title = {A {Neural} {Dirichlet} {Process} {Mixture} {Model} for {Task}-{Free} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2001.00689},
	abstract = {Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.},
	urldate = {2022-01-18},
	journal = {arXiv:2001.00689 [cs, stat]},
	author = {Lee, Soochan and Ha, Junsoo and Zhang, Dongsu and Kim, Gunhee},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.00689},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Lee et al. - 2020 - A Neural Dirichlet Process Mixture Model for Task-.pdf:/Users/nicolas/Documents/Zotero/arXiv2001.00689 [cs, stat]2020/Lee et al. - 2020 - A Neural Dirichlet Process Mixture Model for Task-.pdf:application/pdf},
}

@article{vitter_random_1985,
	title = {Random sampling with a reservoir},
	volume = {11},
	issn = {0098-3500, 1557-7295},
	abstract = {We introduce fast algorithms for selecting a random sample of
              n
              records without replacement from a pool of
              N
              records, where the value of
              N
              is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in
              O
              (
              n
              (1 + log(
              N/n
              ))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.},
	language = {en},
	number = {1},
	urldate = {2022-01-11},
	journal = {ACM Transactions on Mathematical Software},
	author = {Vitter, Jeffrey S.},
	month = mar,
	year = {1985},
	pages = {37--57},
	file = {Vitter - 1985 - Random sampling with a reservoir.pdf:/Users/nicolas/Zotero/storage/KKFT6H4U/Vitter - 1985 - Random sampling with a reservoir.pdf:application/pdf},
}

@article{qu_recent_2021,
	title = {Recent {Advances} of {Continual} {Learning} in {Computer} {Vision}: {An} {Overview}},
	shorttitle = {Recent {Advances} of {Continual} {Learning} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/2109.11369},
	abstract = {In contrast to batch learning where all training data is available at once, continual learning represents a family of methods that accumulate knowledge and learn continuously with data available in sequential order. Similar to the human learning process with the ability of learning, fusing, and accumulating new knowledge coming at different time steps, continual learning is considered to have high practical significance. Hence, continual learning has been studied in various artificial intelligence tasks. In this paper, we present a comprehensive review of the recent progress of continual learning in computer vision. In particular, the works are grouped by their representative techniques, including regularization, knowledge distillation, memory, generative replay, parameter isolation, and a combination of the above techniques. For each category of these techniques, both its characteristics and applications in computer vision are presented. At the end of this overview, several subareas, where continuous knowledge accumulation is potentially helpful while continual learning has not been well studied, are discussed.},
	urldate = {2022-01-10},
	journal = {arXiv:2109.11369 [cs]},
	author = {Qu, Haoxuan and Rahmani, Hossein and Xu, Li and Williams, Bryan and Liu, Jun},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.11369},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/JQI7G559/2109.html:text/html;Qu et al. - 2021 - Recent Advances of Continual Learning in Computer .pdf:/Users/nicolas/Documents/Zotero/arXiv2109.11369 [cs]2021/Qu et al. - 2021 - Recent Advances of Continual Learning in Computer 2.pdf:application/pdf},
}

@inproceedings{zhao_maintaining_2020,
	address = {Seattle, WA, USA},
	title = {Maintaining {Discrimination} and {Fairness} in {Class} {Incremental} {Learning}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156766/},
	doi = {10.1109/CVPR42600.2020.01322},
	abstract = {Deep neural networks (DNNs) have been applied in class incremental learning, which aims to solve common realworld problems of learning new classes continually. One drawback of standard DNNs is that they are prone to catastrophic forgetting. Knowledge distillation (KD) is a commonly used technique to alleviate this problem. In this paper, we demonstrate it can indeed help the model to output more discriminative results within old classes. However, it cannot alleviate the problem that the model tends to classify objects into new classes, causing the positive effect of KD to be hidden and limited. We observed that an important factor causing catastrophic forgetting is that the weights in the last fully connected (FC) layer are highly biased in class incremental learning. In this paper, we propose a simple and effective solution motivated by the aforementioned observations to address catastrophic forgetting. Firstly, we utilize KD to maintain the discrimination within old classes. Then, to further maintain the fairness between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal training process. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings. Experimental results show that the proposed method can effectively alleviate catastrophic forgetting and signiﬁcantly outperform state-of-the-art methods.},
	language = {en},
	urldate = {2022-01-04},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhao, Bowen and Xiao, Xi and Gan, Guojun and Zhang, Bin and Xia, Shu-Tao},
	month = jun,
	year = {2020},
	pages = {13205--13214},
	file = {Zhao et al. - 2020 - Maintaining Discrimination and Fairness in Class I.pdf:/Users/nicolas/Zotero/storage/XPF3XAQ8/Zhao et al. - 2020 - Maintaining Discrimination and Fairness in Class I.pdf:application/pdf},
}

@article{jaiswal_survey_2021,
	title = {A {Survey} on {Contrastive} {Self}-supervised {Learning}},
	url = {http://arxiv.org/abs/2011.00362},
	abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-deﬁned pseudo labels as supervision and use the learned representations for several downstream tasks. Speciﬁcally, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classiﬁcation, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress.},
	language = {en},
	urldate = {2021-12-16},
	journal = {arXiv:2011.00362 [cs]},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	month = feb,
	year = {2021},
	note = {arXiv: 2011.00362},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Jaiswal et al. - 2021 - A Survey on Contrastive Self-supervised Learning.pdf:/Users/nicolas/Zotero/storage/63E42QX9/Jaiswal et al. - 2021 - A Survey on Contrastive Self-supervised Learning.pdf:application/pdf},
}

@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in cognitive sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}


@article{hinton_distilling_2015-1,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2021-12-13},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/FKQMPSLU/1503.html:text/html;Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:/Users/nicolas/Documents/Zotero/arXiv1503.02531 [cs, stat]2015/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf},
}

@inproceedings{rolnick_experience_2019,
  author       = {David Rolnick and
                  Arun Ahuja and
                  Jonathan Schwarz and
                  Timothy P. Lillicrap and
                  Gregory Wayne},
  title        = {Experience Replay for Continual Learning},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems},
  pages        = {348--358},
  year         = {2019},
}



@article{chaudhry_tiny_2019,
	title = {On {Tiny} {Episodic} {Memories} in {Continual} {Learning}},
	url = {http://arxiv.org/abs/1902.10486},
	abstract = {In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, significantly outperforms specifically designed CL approaches with and without episodic memory. Interestingly, we find that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7{\textbackslash}\% and 17{\textbackslash}\% when the memory is populated with a single example per class.},
	urldate = {2021-12-06},
	journal = {arXiv:1902.10486 [cs, stat]},
	author = {Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K. and Torr, Philip H. S. and Ranzato, Marc'Aurelio},
	month = jun,
	year = {2019},
	note = {arXiv: 1902.10486},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/6DB7WINY/1902.html:text/html;Chaudhry et al. - 2019 - On Tiny Episodic Memories in Continual Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv1902.10486 [cs, stat]2019/Chaudhry et al. - 2019 - On Tiny Episodic Memories in Continual Learning.pdf:application/pdf},
}

@inproceedings{vedaldi_gdumb_2020,
  title={Gdumb: A simple approach that questions our progress in continual learning},
  author={Prabhu, Ameya and Torr, Philip HS and Dokania, Puneet K},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Proceedings, Part II 16},
  pages={524--540},
  year={2020},
}


@incollection{ferrari_riemannian_2018,
	address = {Cham},
	title = {Riemannian {Walk} for {Incremental} {Learning}: {Understanding} {Forgetting} and {Intransigence}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	shorttitle = {Riemannian {Walk} for {Incremental} {Learning}},
	url = {http://link.springer.com/10.1007/978-3-030-01252-6_33},
	abstract = {Incremental learning (IL) has received a lot of attention recently, however, the literature lacks a precise problem deﬁnition, proper evaluation settings, and metrics tailored speciﬁcally for the IL problem. One of the main objectives of this work is to ﬁll these gaps so as to provide a common ground for better understanding of IL. The main challenge for an IL algorithm is to update the classiﬁer whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, IL also suffers from a problem we call intransigence, its inability to update knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of IL algorithms. Furthermore, we present RWalk, a generalization of EWC++ (our efﬁcient version of EWC [6]) and Path Integral [25] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various IL algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off for forgetting and intransigence.},
	language = {en},
	urldate = {2021-12-06},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Chaudhry, Arslan and Dokania, Puneet K. and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01252-6_33},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {556--572},
	file = {Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf:/Users/nicolas/Zotero/storage/S4SUDVGZ/Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf:application/pdf},
}

@inproceedings{aljundi_online_2019,
title = {Online Continual Learning with Maximal Interfered Retrieval},
author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and Page-Caccia, Lucas},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {11849--11860},
year = {2019},
}

@article{french_catastrophic_nodate-1,
	title = {Catastrophic {Forgetting} in {Connectionist} {Networks}: {Causes}, {Consequences} and {Solutions}},
	abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Consequently, plausible models of human cognition should exhibit similar patterns of gradual forgetting old information as new information is acquired. Only rarely (see Box 3) does new learning in natural cognitive systems completely disrupt or erase previously learned information. In other words, natural cognitive systems do not, in general, forget catastrophically. Unfortunately, however, this is precisely what occurs under certain circumstances in distributed connectionist networks. It turns out that the very features that give these networks their much-touted abilities to generalize, to function in the presence of degraded input, etc., are the root cause of catastrophic forgetting. The challenge is how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article, we examine the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks. We consider how the brain might have overcome this problem and explore the consequences of this solution.},
	language = {en},
	author = {French, Robert M},
	pages = {18},
	file = {French - Catastrophic Forgetting in Connectionist Networks.pdf:/Users/nicolas/Zotero/storage/NA95YRD9/French - Catastrophic Forgetting in Connectionist Networks.pdf:application/pdf},
}

@article{abu-el-haija_youtube-8m_2016,
	title = {{YouTube}-{8M}: {A} {Large}-{Scale} {Video} {Classification} {Benchmark}},
	shorttitle = {{YouTube}-{8M}},
	url = {http://arxiv.org/abs/1609.08675},
	abstract = {Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of {\textasciitilde}8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.},
	urldate = {2021-12-02},
	journal = {arXiv:1609.08675 [cs]},
	author = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.08675},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 8M},
	file = {Abu-El-Haija et al. - 2016 - YouTube-8M A Large-Scale Video Classification Ben.pdf:/Users/nicolas/Documents/Zotero/arXiv1609.08675 [cs]2016/Abu-El-Haija et al. - 2016 - YouTube-8M A Large-Scale Video Classification Ben.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Z2SUWNGM/1609.html:text/html},
}

@article{liu_rotate_2018,
	title = {Rotate your {Networks}: {Better} {Weight} {Consolidation} and {Less} {Catastrophic} {Forgetting}},
	shorttitle = {Rotate your {Networks}},
	url = {http://arxiv.org/abs/1802.02950},
	abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to other state-of-the-art in lifelong learning without forgetting.},
	urldate = {2021-11-16},
	journal = {arXiv:1802.02950 [cs]},
	author = {Liu, Xialei and Masana, Marc and Herranz, Luis and Van de Weijer, Joost and Lopez, Antonio M. and Bagdanov, Andrew D.},
	month = dec,
	year = {2018},
	note = {arXiv: 1802.02950},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/N2CCFBCY/1802.html:text/html;Liu et al. - 2018 - Rotate your Networks Better Weight Consolidation .pdf:/Users/nicolas/Documents/Zotero/arXiv1802.02950 [cs]2018/Liu et al. - 2018 - Rotate your Networks Better Weight Consolidation .pdf:application/pdf},
}

@article{smith_memory-efficient_2021,
	title = {Memory-{Efficient} {Semi}-{Supervised} {Continual} {Learning}: {The} {World} is its {Own} {Replay} {Buffer}},
	shorttitle = {Memory-{Efficient} {Semi}-{Supervised} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2101.09536},
	abstract = {Rehearsal is a critical component for class-incremental continual learning, yet it requires a substantial memory budget. Our work investigates whether we can significantly reduce this memory budget by leveraging unlabeled data from an agent's environment in a realistic and challenging continual learning paradigm. Specifically, we explore and formalize a novel semi-supervised continual learning (SSCL) setting, where labeled data is scarce yet non-i.i.d. unlabeled data from the agent's environment is plentiful. Importantly, data distributions in the SSCL setting are realistic and therefore reflect object class correlations between, and among, the labeled and unlabeled data distributions. We show that a strategy built on pseudo-labeling, consistency regularization, Out-of-Distribution (OoD) detection, and knowledge distillation reduces forgetting in this setting. Our approach, DistillMatch, increases performance over the state-of-the-art by no less than 8.7\% average task accuracy and up to 54.5\% average task accuracy in SSCL CIFAR-100 experiments. Moreover, we demonstrate that DistillMatch can save up to 0.23 stored images per processed unlabeled image compared to the next best method which only saves 0.08. Our results suggest that focusing on realistic correlated distributions is a significantly new perspective, which accentuates the importance of leveraging the world's structure as a continual learning strategy.},
	urldate = {2021-11-15},
	journal = {arXiv:2101.09536 [cs]},
	author = {Smith, James and Balloch, Jonathan and Hsu, Yen-Chang and Kira, Zsolt},
	month = may,
	year = {2021},
	note = {arXiv: 2101.09536},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/YUDU6QGY/2101.html:text/html;Smith et al. - 2021 - Memory-Efficient Semi-Supervised Continual Learnin.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.09536 [cs]2021/Smith et al. - 2021 - Memory-Efficient Semi-Supervised Continual Learnin.pdf:application/pdf},
}

@article{lee_overcoming_2019-1,
	title = {Overcoming {Catastrophic} {Forgetting} with {Unlabeled} {Data} in the {Wild}},
	url = {http://arxiv.org/abs/1903.12648},
	abstract = {Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8\% higher accuracy and 46.5\% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.},
	urldate = {2021-11-09},
	journal = {arXiv:1903.12648 [cs, stat]},
	author = {Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
	month = oct,
	year = {2019},
	note = {arXiv: 1903.12648},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/3T29S9N5/1903.html:text/html;Lee et al. - 2019 - Overcoming Catastrophic Forgetting with Unlabeled .pdf:/Users/nicolas/Documents/Zotero/arXiv1903.12648 [cs, stat]2019/Lee et al. - 2019 - Overcoming Catastrophic Forgetting with Unlabeled 2.pdf:application/pdf},
}

@inproceedings{he_online_2021,
  title={Online continual learning via candidates voting},
  author={He, Jiangpeng and Zhu, Fengqing},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3154--3163},
  year={2022}
}


@inproceedings{dhar_learning_2019,
	address = {Long Beach, CA, USA},
	title = {Learning {Without} {Memorizing}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953962/},
	doi = {10.1109/CVPR.2019.00528},
	abstract = {Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classiﬁer to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called ‘Learning without Memorizing (LwM)’, to preserve the information about existing (base) classes, without storing any of their data, while making the classiﬁer progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss (LAD), and demonstrate that penalizing the changes in classiﬁers’ attention maps helps to retain information of the base classes, as new classes are added. We show that adding LAD to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Dhar, Prithviraj and Singh, Rajat Vikram and Peng, Kuan-Chuan and Wu, Ziyan and Chellappa, Rama},
	month = jun,
	year = {2019},
	pages = {5133--5141},
	file = {Dhar et al. - 2019 - Learning Without Memorizing.pdf:/Users/nicolas/Zotero/storage/2AK499D8/Dhar et al. - 2019 - Learning Without Memorizing.pdf:application/pdf},
}

@article{chaudhry_efficient_2019,
	title = {Efficient {Lifelong} {Learning} with {A}-{GEM}},
	url = {http://arxiv.org/abs/1812.00420},
	abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz \& Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
	urldate = {2021-09-30},
	journal = {arXiv:1812.00420 [cs, stat]},
	author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
	month = jan,
	year = {2019},
	note = {arXiv: 1812.00420},
	keywords = {AGEM, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/77RIDPG7/1812.html:text/html;Chaudhry et al. - 2019 - Efficient Lifelong Learning with A-GEM.pdf:/Users/nicolas/Documents/Zotero/arXiv1812.00420 [cs, stat]2019/Chaudhry et al. - 2019 - Efficient Lifelong Learning with A-GEM.pdf:application/pdf},
}

@article{cha_co2l_2021,
  title={Co2l: Contrastive continual learning},
  author={Cha, Hyuntak and Lee, Jaeho and Shin, Jinwoo},
  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9516--9525},
  year={2021}
}

@article{he_incremental_2021,
	title = {Incremental {Learning} {In} {Online} {Scenario}},
	url = {http://arxiv.org/abs/2003.13191},
	abstract = {Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-specific data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modified cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art offline incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classification based on our complete framework using the Food-101 dataset.},
	urldate = {2021-09-17},
	journal = {arXiv:2003.13191 [cs]},
	author = {He, Jiangpeng and Mao, Runyu and Shao, Zeman and Zhu, Fengqing},
	month = apr,
	year = {2021},
	note = {arXiv: 2003.13191},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/LJ85ZUDW/2003.html:text/html;He et al. - 2021 - Incremental Learning In Online Scenario.pdf:/Users/nicolas/Documents/Zotero/arXiv2003.13191 [cs]2021/He et al. - 2021 - Incremental Learning In Online Scenario.pdf:application/pdf},
}

@article{ji_complementary_2021,
	title = {Complementary {Calibration}: {Boosting} {General} {Continual} {Learning} with {Collaborative} {Distillation} and {Self}-{Supervision}},
	shorttitle = {Complementary {Calibration}},
	url = {http://arxiv.org/abs/2109.02426},
	abstract = {General Continual Learning (GCL) aims at learning from non independent and identically distributed stream data without catastrophic forgetting of the old tasks that don't rely on task boundaries during both training and testing stages. We reveal that the relation and feature deviations are crucial problems for catastrophic forgetting, in which relation deviation refers to the deficiency of the relationship among all classes in knowledge distillation, and feature deviation refers to indiscriminative feature representations. To this end, we propose a Complementary Calibration (CoCa) framework by mining the complementary model's outputs and features to alleviate the two deviations in the process of GCL. Specifically, we propose a new collaborative distillation approach for addressing the relation deviation. It distills model's outputs by utilizing ensemble dark knowledge of new model's outputs and reserved outputs, which maintains the performance of old tasks as well as balancing the relationship among all classes. Furthermore, we explore a collaborative self-supervision idea to leverage pretext tasks and supervised contrastive learning for addressing the feature deviation problem by learning complete and discriminative features for all classes. Extensive experiments on four popular datasets show that our CoCa framework achieves superior performance against state-of-the-art methods.},
	urldate = {2021-09-13},
	journal = {arXiv:2109.02426 [cs]},
	author = {Ji, Zhong and Li, Jin and Wang, Qiang and Zhang, Zhongfei},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.02426},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Coca},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/VKRK7I6D/2109.html:text/html;Ji et al. - 2021 - Complementary Calibration Boosting General Contin.pdf:/Users/nicolas/Documents/Zotero/arXiv2109.02426 [cs]2021/Ji et al. - 2021 - Complementary Calibration Boosting General Contin.pdf:application/pdf},
}

@article{rosasco_distilled_2021,
	title = {Distilled {Replay}: {Overcoming} {Forgetting} through {Synthetic} {Samples}},
	shorttitle = {Distilled {Replay}},
	url = {http://arxiv.org/abs/2103.15851},
	abstract = {Replay strategies are Continual Learning techniques which mitigate catastrophic forgetting by keeping a buffer of patterns from previous experiences, which are interleaved with new data during training. The amount of patterns stored in the buffer is a critical parameter which largely influences the final performance and the memory footprint of the approach. This work introduces Distilled Replay, a novel replay strategy for Continual Learning which is able to mitigate forgetting by keeping a very small buffer (1 pattern per class) of highly informative samples. Distilled Replay builds the buffer through a distillation process which compresses a large dataset into a tiny set of informative examples. We show the effectiveness of our Distilled Replay against popular replay-based strategies on four Continual Learning benchmarks.},
	urldate = {2021-09-10},
	journal = {arXiv:2103.15851 [cs]},
	author = {Rosasco, Andrea and Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.15851
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/NIQ5JTJC/2103.html:text/html;Rosasco et al. - 2021 - Distilled Replay Overcoming Forgetting through Sy.pdf:/Users/nicolas/Documents/Zotero/arXiv2103.15851 [cs]2021/Rosasco et al. - 2021 - Distilled Replay Overcoming Forgetting through Sy.pdf:application/pdf},
}

@article{de_lange_continual_2021-1,
	title = {Continual {Prototype} {Evolution}: {Learning} {Online} from {Non}-{Stationary} {Data} {Streams}},
	shorttitle = {Continual {Prototype} {Evolution}},
	url = {http://arxiv.org/abs/2009.00919},
	abstract = {Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streaming data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space during the learning process. Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. As an additional contribution, we generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework. We obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.},
	urldate = {2021-08-09},
	journal = {arXiv:2009.00919 [cs]},
	author = {De Lange, Matthias and Tuytelaars, Tinne},
	month = apr,
	year = {2021},
	note = {arXiv: 2009.00919},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, CoPE},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/ZRRI9GUH/De Lange and Tuytelaars - 2021 - Continual Prototype Evolution Learning Online fro.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/7I76TA5I/2009.html:text/html},
}

@article{orabona_training_2017,
	title = {Training {Deep} {Networks} without {Learning} {Rates} {Through} {Coin} {Betting}},
	url = {http://arxiv.org/abs/1705.07795},
	abstract = {Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.},
	urldate = {2021-09-01},
	journal = {arXiv:1705.07795 [cs, math, stat]},
	author = {Orabona, Francesco and Tommasi, Tatiana},
	month = nov,
	year = {2017},
	note = {arXiv: 1705.07795},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, COCOB, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/98JWCVVY/Orabona and Tommasi - 2017 - Training Deep Networks without Learning Rates Thro.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/LKSU3V2Y/1705.html:text/html},
}

@article{masuyama_multi-label_2021,
	title = {Multi-label {Classification} via {Adaptive} {Resonance} {Theory}-based {Clustering}},
	url = {http://arxiv.org/abs/2103.01511},
	abstract = {This paper proposes a multi-label classification algorithm capable of continual learning by applying an Adaptive Resonance Theory (ART)-based clustering algorithm and the Bayesian approach for label probability computation. The ART-based clustering algorithm adaptively and continually generates prototype nodes corresponding to given data, and the generated nodes are used as classifiers. The label probability computation independently counts the number of label appearances for each class and calculates the Bayesian probabilities. Thus, the label probability computation can cope with an increase in the number of labels. Experimental results with synthetic and real-world multi-label datasets show that the proposed algorithm has competitive classification performance to other well-known algorithms while realizing continual learning.},
	urldate = {2021-08-25},
	journal = {arXiv:2103.01511 [cs]},
	author = {Masuyama, Naoki and Nojima, Yusuke and Loo, Chu Kiong and Ishibuchi, Hisao},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.01511},
	keywords = {Computer Science - Machine Learning, ART},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/VZEZD7LH/2103.html:text/html;Masuyama et al. - 2021 - Multi-label Classification via Adaptive Resonance .pdf:/Users/nicolas/Documents/Zotero/arXiv2103.01511 [cs]2021/Masuyama et al. - 2021 - Multi-label Classification via Adaptive Resonance 2.pdf:application/pdf},
}

@article{he_unsupervised_2021,
	title = {Unsupervised {Continual} {Learning} {Via} {Pseudo} {Labels}},
	url = {http://arxiv.org/abs/2104.07164},
	abstract = {Continual learning aims to learn new tasks incrementally using less computation and memory resources instead of retraining the model from scratch whenever new task arrives. However, existing approaches are designed in supervised fashion assuming all data from new tasks have been manually annotated, which are not practical for many real-life applications. In this work, we propose to use pseudo label instead of the ground truth to make continual learning feasible in unsupervised mode. The pseudo labels of new data are obtained by applying global clustering algorithm and we propose to use the model updated from last incremental step as the feature extractor. Due to the scarcity of existing work, we introduce a new benchmark experimental protocol for unsupervised continual learning of image classification task under class-incremental setting where no class label is provided for each incremental learning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC) datasets by incorporating the pseudo label with various existing supervised approaches and show promising results in unsupervised scenario.},
	urldate = {2021-08-25},
	journal = {arXiv:2104.07164 [cs]},
	author = {He, Jiangpeng and Zhu, Fengqing},
	month = jul,
	year = {2021},
	note = {arXiv: 2104.07164
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, UCL*},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/D9ZGSMEV/He and Zhu - 2021 - Unsupervised Continual Learning Via Pseudo Labels.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/FMWRCP39/2104.html:text/html},
}

@article{khare_unsupervised_2021,
	title = {Unsupervised {Class}-{Incremental} {Learning} {Through} {Confusion}},
	url = {http://arxiv.org/abs/2104.04450},
	abstract = {While many works on Continual Learning have shown promising results for mitigating catastrophic forgetting, they have relied on supervised training. To successfully learn in a label-agnostic incremental setting, a model must distinguish between learned and novel classes to properly include samples for training. We introduce a novelty detection method that leverages network confusion caused by training incoming data as a new class. We found that incorporating a class-imbalance during this detection method substantially enhances performance. The effectiveness of our approach is demonstrated across a set of image classification benchmarks: MNIST, SVHN, CIFAR-10, CIFAR-100, and CRIB.},
	urldate = {2021-08-25},
	journal = {arXiv:2104.04450 [cs]},
	author = {Khare, Shivam and Cao, Kun and Rehg, James},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.04450
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, UCIL},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/RSUCNL8T/2104.html:text/html;Khare et al. - 2021 - Unsupervised Class-Incremental Learning Through Co.pdf:/Users/nicolas/Documents/Zotero/arXiv2104.04450 [cs]2021/Khare et al. - 2021 - Unsupervised Class-Incremental Learning Through Co.pdf:application/pdf},
}

@inproceedings{stojanov_incremental_2019,
	title = {Incremental {Object} {Learning} {From} {Contiguous} {Views}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Stojanov_Incremental_Object_Learning_From_Contiguous_Views_CVPR_2019_paper.html},
	urldate = {2021-08-25},
	author = {Stojanov, Stefan and Mishra, Samarth and Thai, Ngoc Anh and Dhanda, Nikhil and Humayun, Ahmad and Yu, Chen and Smith, Linda B. and Rehg, James M.},
	year = {2019},
	keywords = {CRIB},
	pages = {8777--8786},
	file = {Snapshot:/Users/nicolas/Zotero/storage/XZ2ILDBY/Stojanov_Incremental_Object_Learning_From_Contiguous_Views_CVPR_2019_paper.html:text/html;Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf:/Users/nicolas/Documents/Zotero/2019/Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf:application/pdf},
}

@article{aljundi_gradient_2019,  
title={Gradient based sample selection for online continual learning},
  author={Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
    pages={11817–11826},
  volume={32},
  year={2019}
}


@article{pratama_unsupervised_2021,
	title = {Unsupervised {Continual} {Learning} via {Self}-{Adaptive} {Deep} {Clustering} {Approach}},
	url = {http://arxiv.org/abs/2106.14563},
	abstract = {Unsupervised continual learning remains a relatively uncharted territory in the existing literature because the vast majority of existing works call for unlimited access of ground truth incurring expensive labelling cost. Another issue lies in the problem of task boundaries and task IDs which must be known for model's updates or model's predictions hindering feasibility for real-time deployment. Knowledge Retention in Self-Adaptive Deep Continual Learner, (KIERA), is proposed in this paper. KIERA is developed from the notion of flexible deep clustering approach possessing an elastic network structure to cope with changing environments in the timely manner. The centroid-based experience replay is put forward to overcome the catastrophic forgetting problem. KIERA does not exploit any labelled samples for model updates while featuring a task-agnostic merit. The advantage of KIERA has been numerically validated in popular continual learning problems where it shows highly competitive performance compared to state-of-the art approaches. Our implementation is available in {\textbackslash}textit\{{\textbackslash}url\{https://github.com/ContinualAL/KIERA\}\}.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.14563 [cs]},
	author = {Pratama, Mahardhika and Ashfahani, Andri and Lughofer, Edwin},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.14563},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, KIERA},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/KVNK6AEU/2106.html:text/html;Pratama et al. - 2021 - Unsupervised Continual Learning via Self-Adaptive .pdf:/Users/nicolas/Documents/Zotero/arXiv2106.14563 [cs]2021/Pratama et al. - 2021 - Unsupervised Continual Learning via Self-Adaptive 2.pdf:application/pdf},
}

@article{caccia_online_2020,
	title = {Online {Learned} {Continual} {Compression} with {Adaptive} {Quantization} {Modules}},
	url = {http://arxiv.org/abs/1911.08019},
	abstract = {We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments.},
	urldate = {2021-08-25},
	journal = {arXiv:1911.08019 [cs, stat]},
	author = {Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Pineau, Joelle},
	month = aug,
	year = {2020},
	note = {arXiv: 1911.08019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, AQM, VAE},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IJBC33K8/1911.html:text/html;Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive.pdf:/Users/nicolas/Documents/Zotero/arXiv1911.08019 [cs, stat]2020/Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive2.pdf:application/pdf},
}

@article{achille_life-long_2018,
	title = {Life-{Long} {Disentangled} {Representation} {Learning} with {Cross}-{Domain} {Latent} {Homologies}},
	url = {http://arxiv.org/abs/1808.06508},
	abstract = {Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.},
	urldate = {2021-08-25},
	journal = {arXiv:1808.06508 [cs, stat]},
	author = {Achille, Alessandro and Eccles, Tom and Matthey, Loic and Burgess, Christopher P. and Watters, Nick and Lerchner, Alexander and Higgins, Irina},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.06508},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, representation learning, unsupervised},
	file = {Achille et al. - 2018 - Life-Long Disentangled Representation Learning wit.pdf:/Users/nicolas/Documents/Zotero/arXiv1808.06508 [cs, stat]2018/Achille et al. - 2018 - Life-Long Disentangled Representation Learning wit.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/YEXKFU4L/1808.html:text/html},
}

@article{smith_always_2021,
	title = {Always {Be} {Dreaming}: {A} {New} {Approach} for {Data}-{Free} {Class}-{Incremental} {Learning}},
	shorttitle = {Always {Be} {Dreaming}},
	url = {http://arxiv.org/abs/2106.09701},
	abstract = {Modern computer vision applications suffer from catastrophic forgetting when incrementally learning new concepts over time. The most successful approaches to alleviate this forgetting require extensive replay of previously seen data, which is problematic when memory constraints or data legality concerns exist. In this work, we consider the high-impact problem of Data-Free Class-Incremental Learning (DFCIL), where an incremental learning agent must learn new concepts over time without storing generators or training data from past tasks. One approach for DFCIL is to replay synthetic images produced by inverting a frozen copy of the learner's classification model, but we show this approach fails for common class-incremental benchmarks when using standard distillation strategies. We diagnose the cause of this failure and propose a novel incremental distillation strategy for DFCIL, contributing a modified cross-entropy training and importance-weighted feature distillation, and show that our method results in up to a 25.1\% increase in final task accuracy (absolute difference) compared to SOTA DFCIL methods for common class-incremental benchmarks. Our method even outperforms several standard replay based methods which store a coreset of images.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.09701 [cs]},
	author = {Smith, James and Hsu, Yen-Chang and Balloch, Jonathan and Shen, Yilin and Jin, Hongxia and Kira, Zsolt},
	month = aug,
	year = {2021},
	note = {arXiv: 2106.09701},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, data-free},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/N6YAXSZP/2106.html:text/html;Smith et al. - 2021 - Always Be Dreaming A New Approach for Data-Free C.pdf:/Users/nicolas/Documents/Zotero/arXiv2106.09701 [cs]2021/Smith et al. - 2021 - Always Be Dreaming A New Approach for Data-Free C.pdf:application/pdf},
}

@article{caccia_special_2021,
	title = {{SPeCiaL}: {Self}-{Supervised} {Pretraining} for {Continual} {Learning}},
	shorttitle = {{SPeCiaL}},
	url = {http://arxiv.org/abs/2106.09065},
	abstract = {This paper presents SPeCiaL: a method for unsupervised pretraining of representations tailored for continual learning. Our approach devises a meta-learning objective that differentiates through a sequential learning process. Specifically, we train a linear model over the representations to match different augmented views of the same image together, each view presented sequentially. The linear model is then evaluated on both its ability to classify images it just saw, and also on images from previous iterations. This gives rise to representations that favor quick knowledge retention with minimal forgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, and show that it can match or outperform other supervised pretraining approaches.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.09065 [cs]},
	author = {Caccia, Lucas and Pineau, Joelle},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09065},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, SPECIAL},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/RLH7FRTC/Caccia and Pineau - 2021 - SPeCiaL Self-Supervised Pretraining for Continual.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/YM46Q82D/2106.html:text/html},
}

@inproceedings{stojanov_incremental_2019-1,
	address = {Long Beach, CA, USA},
	title = {Incremental {Object} {Learning} {From} {Contiguous} {Views}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953997/},
	doi = {10.1109/CVPR.2019.00898},
	abstract = {In this work, we present CRIB (Continual Recognition Inspired by Babies), a synthetic incremental object learning environment that can produce data that models visual imagery produced by object exploration in early infancy. CRIB is coupled with a new 3D object dataset, Toys-200, that contains 200 unique toy-like object instances, and is also compatible with existing 3D datasets. Through extensive empirical evaluation of state-of-the-art incremental learning algorithms, we ﬁnd the novel empirical result that repetition can signiﬁcantly ameliorate the effects of catastrophic forgetting. Furthermore, we ﬁnd that in certain cases repetition allows for performance approaching that of batch learning algorithms. Finally, we propose an unsupervised incremental learning task with intriguing baseline results.},
	language = {en},
	urldate = {2021-08-18},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Stojanov, Stefan and Mishra, Samarth and Thai, Ngoc Anh and Dhanda, Nikhil and Humayun, Ahmad and Yu, Chen and Smith, Linda B. and Rehg, James M.},
	month = jun,
	year = {2019},
	pages = {8769--8778},
	file = {Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf:/Users/nicolas/Zotero/storage/VBC7KYY5/Stojanov et al. - 2019 - Incremental Object Learning From Contiguous Views.pdf:application/pdf},
}

@article{lin_continual_2021,
	title = {Continual {Contrastive} {Self}-supervised {Learning} for {Image} {Classification}},
	url = {http://arxiv.org/abs/2107.01776},
	abstract = {For artificial learning systems, continual learning over time from a stream of data is essential. The burgeoning studies on supervised continual learning have achieved great progress, while the study of catastrophic forgetting in unsupervised learning is still blank. Among unsupervised learning methods, self-supervise learning method shows tremendous potential on visual representation without any labeled data at scale. To improve the visual representation of self-supervised learning, larger and more varied data is needed. In the real world, unlabeled data is generated at all times. This circumstance provides a huge advantage for the learning of the self-supervised method. However, in the current paradigm, packing previous data and current data together and training it again is a waste of time and resources. Thus, a continual self-supervised learning method is badly needed. In this paper, we make the first attempt to implement the continual contrastive self-supervised learning by proposing a rehearsal method, which keeps a few exemplars from the previous data. Instead of directly combining saved exemplars with the current data set for training, we leverage self-supervised knowledge distillation to transfer contrastive information among previous data to the current network by mimicking similarity score distribution inferred by the old network over a set of saved exemplars. Moreover, we build an extra sample queue to assist the network to distinguish between previous and current data and prevent mutual interference while learning their own feature representation. Experimental results show that our method performs well on CIFAR100 and ImageNet-Sub. Compared with the baselines, which learning tasks without taking any technique, we improve the image classification top-1 accuracy by 1.60\% on CIFAR100, 2.86\% on ImageNet-Sub and 1.29\% on ImageNet-Full under 10 incremental steps setting.},
	urldate = {2021-08-17},
	journal = {arXiv:2107.01776 [cs]},
	author = {Lin, Zhiwei and Wang, Yongtao and Lin, Hongxiang},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.01776},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/7UKJ5R3E/2107.html:text/html;Lin et al. - 2021 - Continual Contrastive Self-supervised Learning for.pdf:/Users/nicolas/Documents/Zotero/arXiv2107.01776 [cs]2021/Lin et al. - 2021 - Continual Contrastive Self-supervised Learning for.pdf:application/pdf},
}

@article{moons_representation_nodate,
	title = {Representation {Learning} for {Automated} {Document} {Classification}},
	language = {en},
	author = {Moons, Elias},
	pages = {132},
	file = {Moons - Representation Learning for Automated Document Cla.pdf:/Users/nicolas/Zotero/storage/KUVSRVX4/Moons - Representation Learning for Automated Document Cla.pdf:application/pdf},
}

@article{caccia_online_2020-1,
	title = {Online {Learned} {Continual} {Compression} with {Adaptive} {Quantization} {Modules}},
	url = {http://arxiv.org/abs/1911.08019},
	abstract = {We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments.},
	urldate = {2021-08-17},
	journal = {arXiv:1911.08019 [cs, stat]},
	author = {Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Pineau, Joelle},
	month = aug,
	year = {2020},
	note = {arXiv: 1911.08019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, AQM},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/A65XS5WZ/1911.html:text/html;Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive.pdf:/Users/nicolas/Documents/Zotero/arXiv1911.08019 [cs, stat]2020/Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive.pdf:application/pdf},
}

@inproceedings{sun_ilcoc_2021,
	title = {{ILCOC}: {An} {Incremental} {Learning} {Framework} {Based} on {Contrastive} {One}-{Class} {Classifiers}},
	shorttitle = {{ILCOC}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Sun_ILCOC_An_Incremental_Learning_Framework_Based_on_Contrastive_One-Class_Classifiers_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-08-12},
	author = {Sun, Wenju and Zhang, Jing and Wang, Danyu and Geng, Yangli-ao and Li, Qingyong},
	year = {2021},
	pages = {3580--3588},
	file = {Snapshot:/Users/nicolas/Zotero/storage/NFKJY3SW/Sun_ILCOC_An_Incremental_Learning_Framework_Based_on_Contrastive_One-Class_Classifiers_CVPRW_20.html:text/html;Sun et al. - 2021 - ILCOC An Incremental Learning Framework Based on .pdf:/Users/nicolas/Documents/Zotero/2021/Sun et al. - 2021 - ILCOC An Incremental Learning Framework Based on .pdf:application/pdf},
}

@article{song_multi-label_2020,
	title = {Multi-label {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/2007.09852},
	abstract = {Variational mutual information (MI) estimators are widely used in unsupervised representation learning methods such as contrastive predictive coding (CPC). A lower bound on MI can be obtained from a multi-class classification problem, where a critic attempts to distinguish a positive sample drawn from the underlying joint distribution from \$(m-1)\$ negative samples drawn from a suitable proposal distribution. Using this approach, MI estimates are bounded above by \${\textbackslash}log m\$, and could thus severely underestimate unless \$m\$ is very large. To overcome this limitation, we introduce a novel estimator based on a multi-label classification problem, where the critic needs to jointly identify multiple positive samples at the same time. We show that using the same amount of negative samples, multi-label CPC is able to exceed the \${\textbackslash}log m\$ bound, while still being a valid lower bound of mutual information. We demonstrate that the proposed approach is able to lead to better mutual information estimation, gain empirical improvements in unsupervised representation learning, and beat a current state-of-the-art knowledge distillation method over 10 out of 13 tasks.},
	urldate = {2021-08-09},
	journal = {arXiv:2007.09852 [cs, stat]},
	author = {Song, Jiaming and Ermon, Stefano},
	month = dec,
	year = {2020},
	note = {arXiv: 2007.09852},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/NUSJ6ZX3/Song and Ermon - 2020 - Multi-label Contrastive Predictive Coding.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/F4ZXXJFG/2007.html:text/html},
}

@article{mai_online_2021-1,
	title = {Online {Continual} {Learning} in {Image} {Classification}: {An} {Empirical} {Survey}},
	shorttitle = {Online {Continual} {Learning} in {Image} {Classification}},
	url = {http://arxiv.org/abs/2101.10423},
	abstract = {Online continual learning for image classification studies the problem of learning to classify images from an online stream of data and tasks, where tasks may include new classes (class incremental) or data nonstationarity (domain incremental). One of the key challenges of continual learning is to avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence of more recent tasks. Over the past few years, many methods and tricks have been introduced to address this problem, but many have not been fairly and systematically compared under a variety of realistic and practical settings. To better understand the relative advantages of various approaches and the settings where they work best, this survey aims to (1) compare state-of-the-art methods such as MIR, iCARL, and GDumb and determine which works best at different experimental settings; (2) determine if the best class incremental methods are also competitive in domain incremental setting; (3) evaluate the performance of 7 simple but effective trick such as "review" trick and nearest class mean (NCM) classifier to assess their relative impact. Regarding (1), we observe iCaRL remains competitive when the memory buffer is small; GDumb outperforms many recently proposed methods in medium-size datasets and MIR performs the best in larger-scale datasets. For (2), we note that GDumb performs quite poorly while MIR -- already competitive for (1) -- is also strongly competitive in this very different but important setting. Overall, this allows us to conclude that MIR is overall a strong and versatile method across a wide variety of settings. For (3), we find that all 7 tricks are beneficial, and when augmented with the "review" trick and NCM classifier, MIR produces performance levels that bring online continual learning much closer to its ultimate goal of matching offline training.},
	urldate = {2021-08-09},
	journal = {arXiv:2101.10423 [cs]},
	author = {Mai, Zheda and Li, Ruiwen and Jeong, Jihwan and Quispe, David and Kim, Hyunwoo and Sanner, Scott},
	month = jun,
	year = {2021},
	note = {arXiv: 2101.10423},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/ZA96YRTW/2101.html:text/html;Mai et al. - 2021 - Online Continual Learning in Image Classification.pdf:/Users/nicolas/Documents/Zotero/arXiv2101.10423 [cs]2021/Mai et al. - 2021 - Online Continual Learning in Image Classification2.pdf:application/pdf},
}

@article{masuyama_multi-label_2021-1,
	title = {Multi-label {Classification} via {Adaptive} {Resonance} {Theory}-based {Clustering}},
	url = {http://arxiv.org/abs/2103.01511},
	abstract = {This paper proposes a multi-label classification algorithm capable of continual learning by applying an Adaptive Resonance Theory (ART)-based clustering algorithm and the Bayesian approach for label probability computation. The ART-based clustering algorithm adaptively and continually generates prototype nodes corresponding to given data, and the generated nodes are used as classifiers. The label probability computation independently counts the number of label appearances for each class and calculates the Bayesian probabilities. Thus, the label probability computation can cope with an increase in the number of labels. Experimental results with synthetic and real-world multi-label datasets show that the proposed algorithm has competitive classification performance to other well-known algorithms while realizing continual learning.},
	urldate = {2021-08-09},
	journal = {arXiv:2103.01511 [cs]},
	author = {Masuyama, Naoki and Nojima, Yusuke and Loo, Chu Kiong and Ishibuchi, Hisao},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.01511},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/9EM69QSE/2103.html:text/html;Masuyama et al. - 2021 - Multi-label Classification via Adaptive Resonance .pdf:/Users/nicolas/Documents/Zotero/arXiv2103.01511 [cs]2021/Masuyama et al. - 2021 - Multi-label Classification via Adaptive Resonance .pdf:application/pdf},
}

@inproceedings{wang_cifdm_2021,
	address = {New York, NY, USA},
	series = {{SIGIR} '21},
	title = {{CIFDM}: {Continual} and {Interactive} {Feature} {Distillation} for {Multi}-{Label} {Stream} {Learning}},
	isbn = {978-1-4503-8037-9},
	shorttitle = {{CIFDM}},
	url = {https://doi.org/10.1145/3404835.3463096},
	doi = {10.1145/3404835.3463096},
	abstract = {Multi-label learning algorithms have attracted more and more attention as of recent. This is mainly because real-world data is generally associated with multiple and non-exclusive labels, which could correspond to different objects, scenes, actions, and attributes. In this paper, we consider the following challenging multi-label stream scenario: the new labels emerge continuously in the changing environments, and are assigned to the previous data. In this setting, data mining solutions must be able to learn the new concepts and avoid catastrophic forgetting simultaneously. We propose a novel continual and interactive feature distillation-based learning framework (CIFDM), to effectively classify instances with novel labels. We utilize the knowledge from the previous tasks to learn new knowledge to solve the current task. Then, the system compresses historical and novel knowledge and preserves it while waiting for new emerging tasks. CIFDM consists of three components: 1) a knowledge bank that stores the existing feature-level compressed knowledge, and predicts the observed labels so far; 2) a pioneer module that aims to learn and predict new emerged labels based on knowledge bank.; 3) an interactive knowledge compression function which is used to compress and transfer the new knowledge to the bank, and then apply the current compressed knowledge to initialize the label embedding of the pioneer for the next task.},
	urldate = {2021-08-09},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Yigong and Wang, Zhuoyi and Lin, Yu and Khan, Latifur and Li, Dingcheng},
	month = jul,
	year = {2021},
	keywords = {incremental learning, multi-label, neural network, stream mining},
	pages = {2121--2125},
	file = {Wang et al. - 2021 - CIFDM Continual and Interactive Feature Distillat.pdf:/Users/nicolas/Documents/Zotero/Association for Computing Machinery2021/Wang et al. - 2021 - CIFDM Continual and Interactive Feature Distillat.pdf:application/pdf},
}

@article{roseberry_self-adjusting_2021,
	title = {Self-{Adjusting} k {Nearest} {Neighbors} for {Continual} {Learning} from {Multi}-{Label} {Drifting} {Data} {Streams}},
	volume = {442},
	doi = {10.1016/j.neucom.2021.02.032},
	abstract = {Drifting data streams and multi-label data are both challenging problems. Multi-label instances may simultaneously be associated with many labels and classifiers must predict the complete set of labels. Learning from data streams requires algorithms able to learn from potentially unbounded data that is constantly changing. When multi-label data arrives as a stream, the challenges of both problems must be addressed, but additional challenges unique to the combined problem also arise. Each label may experience different concept drifts, simultaneously or distinctly, and parameter optimizations may be different for each label. In this paper we present a self-adapting algorithm for drifting, multi-label data streams, that can adapt to a variety of concepts drifts, is robust to data-level difficulties, and mitigates the necessity to tune multiple parameters. The window of retained instances self-adjusts in size to retain only the current concept, enabling efficient response to abrupt concept drift. The value k is self-adapting for each label, relieving the necessity to tune and allowing it to change, over time, for each label individually. A novel, label-based mechanism disables individual labels that contribute to error, while another punitive measure removes erroneous instances entirely, increasing robustness to noise, concept drift and label differences. Extensive experiments on 35 multi-label streams and generators demonstrate the superiority and advantages of the self-adapting mechanisms proposed compared to existing state-of-the-art methods.},
	journal = {Neurocomputing},
	author = {Roseberry, Martha and Krawczyk, Bartosz and Djenouri, Youcef and Cano, Alberto},
	month = jun,
	year = {2021},
	pages = {10--25},
	file = {Roseberry et al. - 2021 - Self-Adjusting k Nearest Neighbors for Continual L.pdf:/Users/nicolas/Documents/Zotero/Neurocomputing2021/Roseberry et al. - 2021 - Self-Adjusting k Nearest Neighbors for Continual L.pdf:application/pdf},
}

@article{kim_imbalanced_2020,
	title = {Imbalanced {Continual} {Learning} with {Partitioning} {Reservoir} {Sampling}},
	url = {http://arxiv.org/abs/2009.03632},
	abstract = {Continual learning from a sequential stream of data is a crucial challenge for machine learning research. Most studies have been conducted on this topic under the single-label classification setting along with an assumption of balanced label distribution. This work expands this research horizon towards multi-label classification. In doing so, we identify unanticipated adversity innately existent in many multi-label datasets, the long-tailed distribution. We jointly address the two independently solved problems, Catastropic Forgetting and the long-tailed label distribution by first empirically showing a new challenge of destructive forgetting of the minority concepts on the tail. Then, we curate two benchmark datasets, COCOseq and NUS-WIDEseq, that allow the study of both intra- and inter-task imbalances. Lastly, we propose a new sampling strategy for replay-based approach named Partitioning Reservoir Sampling (PRS), which allows the model to maintain a balanced knowledge of both head and tail classes. We publicly release the dataset and the code in our project page.},
	urldate = {2021-08-09},
	journal = {arXiv:2009.03632 [cs, stat]},
	author = {Kim, Chris Dongjoo and Jeong, Jinseo and Kim, Gunhee},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.03632},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/FGJVX7H2/2009.html:text/html;Kim et al. - 2020 - Imbalanced Continual Learning with Partitioning Re.pdf:/Users/nicolas/Documents/Zotero/arXiv2009.03632 [cs, stat]2020/Kim et al. - 2020 - Imbalanced Continual Learning with Partitioning Re.pdf:application/pdf},
}

@article{alberghini_continual_2021,
	title = {{CONTINUAL} {LEARNING} {FOR} {MULTI}-{LABEL} {DRIFTING} {DATA} {STREAMS} {USING} {HOMOGENEOUS} {ENSEMBLE} {OF} {SELF}-{ADJUSTING} {NEAREST} {NEIGHBORS}},
	url = {https://scholarscompass.vcu.edu/etd/6562},
	journal = {Theses and Dissertations},
	author = {Alberghini, Gavin},
	month = jan,
	year = {2021},
	file = {"CONTINUAL LEARNING FOR MULTI-LABEL DRIFTING DATA STREAMS USING HOMOGEN" by Gavin Alberghini:/Users/nicolas/Zotero/storage/ZZDE29L7/6562.html:text/html},
}

@article{he_unsupervised_2021-1,
	title = {Unsupervised {Continual} {Learning} {Via} {Pseudo} {Labels}},
	url = {http://arxiv.org/abs/2104.07164},
	abstract = {Continual learning aims to learn new tasks incrementally using less computation and memory resources instead of retraining the model from scratch whenever new task arrives. However, existing approaches are designed in supervised fashion assuming all data from new tasks have been manually annotated, which are not practical for many real-life applications. In this work, we propose to use pseudo label instead of the ground truth to make continual learning feasible in unsupervised mode. The pseudo labels of new data are obtained by applying global clustering algorithm and we propose to use the model updated from last incremental step as the feature extractor. Due to the scarcity of existing work, we introduce a new benchmark experimental protocol for unsupervised continual learning of image classification task under class-incremental setting where no class label is provided for each incremental learning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC) datasets by incorporating the pseudo label with various existing supervised approaches and show promising results in unsupervised scenario.},
	urldate = {2021-08-06},
	journal = {arXiv:2104.07164 [cs]},
	author = {He, Jiangpeng and Zhu, Fengqing},
	month = jul,
	year = {2021},
	note = {arXiv: 2104.07164
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/4W63EKHI/He and Zhu - 2021 - Unsupervised Continual Learning Via Pseudo Labels.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Q74Z6RT3/2104.html:text/html},
}

@inproceedings{chaudhry_continual_2020,
	title = {Continual {Learning} in {Low}-rank {Orthogonal} {Subspaces}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/70d85f35a1fdc0ab701ff78779306407-Abstract.html},
	urldate = {2021-08-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chaudhry, Arslan and Khan, Naeemullah and Dokania, Puneet and Torr, Philip},
	year = {2020},
	keywords = {CL},
	pages = {9900--9911},
	file = {Chaudhry et al. - 2020 - Continual Learning in Low-rank Orthogonal Subspace.pdf:/Users/nicolas/Documents/Zotero/Curran Associates, Inc.2020/Chaudhry et al. - 2020 - Continual Learning in Low-rank Orthogonal Subspace.pdf:application/pdf},
}

@inproceedings{yang_deep_2019,
	address = {Long Beach, CA, USA},
	title = {Deep {Spectral} {Clustering} {Using} {Dual} {Autoencoder} {Network}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953592/},
	doi = {10.1109/CVPR.2019.00419},
	abstract = {The clustering methods have recently absorbed evenincreasing attention in learning and vision. Deep clustering combines embedding and clustering together to obtain optimal embedding subspace for clustering, which can be more effective compared with conventional clustering methods. In this paper, we propose a joint learning framework for discriminative embedding and spectral clustering. We ﬁrst devise a dual autoencoder network, which enforces the reconstruction constraint for the latent representations and their noisy versions, to embed the inputs into a latent space for clustering. As such the learned latent representations can be more robust to noise. Then the mutual information estimation is utilized to provide more discriminative information from the inputs. Furthermore, a deep spectral clustering method is applied to embed the latent representations into the eigenspace and subsequently clusters them, which can fully exploit the relationship between inputs to achieve optimal clustering results. Experimental results on benchmark datasets show that our method can signiﬁcantly outperform state-of-the-art clustering approaches.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Xu and Deng, Cheng and Zheng, Feng and Yan, Junchi and Liu, Wei},
	month = jun,
	year = {2019},
	pages = {4061--4070},
	file = {Yang et al. - 2019 - Deep Spectral Clustering Using Dual Autoencoder Ne.pdf:/Users/nicolas/Zotero/storage/SB32QYNC/Yang et al. - 2019 - Deep Spectral Clustering Using Dual Autoencoder Ne.pdf:application/pdf},
}

@article{kumar_unified_2020,
	title = {A {Unified} {Bayesian} {Framework} for {Discriminative} and {Generative} {Continual} {Learning}},
	url = {https://openreview.net/forum?id=98fWAc-sFkv},
	abstract = {Continual Learning is a learning paradigm where learning systems are trained on a sequence of tasks. The goal here is to perform well on the current task without suffering from a performance drop...},
	language = {en},
	urldate = {2021-08-03},
	author = {Kumar, Abhishek and Chatterjee, Sunabha and Rai, Piyush},
	month = sep,
	year = {2020},
	file = {Kumar et al. - 2020 - A Unified Bayesian Framework for Discriminative an.pdf:/Users/nicolas/Documents/Zotero/2020/Kumar et al. - 2020 - A Unified Bayesian Framework for Discriminative an.pdf:application/pdf;Snapshot:/Users/nicolas/Zotero/storage/YHPCAUMI/forum.html:text/html},
}

@article{pratama_unsupervised_2021-1,
	title = {Unsupervised {Continual} {Learning} via {Self}-{Adaptive} {Deep} {Clustering} {Approach}},
	url = {http://arxiv.org/abs/2106.14563},
	abstract = {Unsupervised continual learning remains a relatively uncharted territory in the existing literature because the vast majority of existing works call for unlimited access of ground truth incurring expensive labelling cost. Another issue lies in the problem of task boundaries and task IDs which must be known for model's updates or model's predictions hindering feasibility for real-time deployment. Knowledge Retention in Self-Adaptive Deep Continual Learner, (KIERA), is proposed in this paper. KIERA is developed from the notion of flexible deep clustering approach possessing an elastic network structure to cope with changing environments in the timely manner. The centroid-based experience replay is put forward to overcome the catastrophic forgetting problem. KIERA does not exploit any labelled samples for model updates while featuring a task-agnostic merit. The advantage of KIERA has been numerically validated in popular continual learning problems where it shows highly competitive performance compared to state-of-the art approaches. Our implementation is available in {\textbackslash}textit\{{\textbackslash}url\{https://github.com/ContinualAL/KIERA\}\}.},
	urldate = {2021-08-03},
	journal = {arXiv:2106.14563 [cs]},
	author = {Pratama, Mahardhika and Ashfahani, Andri and Lughofer, Edwin},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.14563},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/5TC93IC2/2106.html:text/html;Pratama et al. - 2021 - Unsupervised Continual Learning via Self-Adaptive .pdf:/Users/nicolas/Documents/Zotero/arXiv2106.14563 [cs]2021/Pratama et al. - 2021 - Unsupervised Continual Learning via Self-Adaptive .pdf:application/pdf},
}

@article{loshchilov_sgdr_2017,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {http://arxiv.org/abs/1608.03983},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	urldate = {2021-07-23},
	journal = {arXiv:1608.03983 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = may,
	year = {2017},
	note = {arXiv: 1608.03983},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, cosine, lr, scheduler},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/MULW49WZ/Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/CVG4S5F8/1608.html:text/html},
}

@article{chuang_debiased_2020,
	title = {Debiased {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2007.00224},
	abstract = {A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.},
	urldate = {2021-07-07},
	journal = {arXiv:2007.00224 [cs, stat]},
	author = {Chuang, Ching-Yao and Robinson, Joshua and Yen-Chen, Lin and Torralba, Antonio and Jegelka, Stefanie},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.00224},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, supervised contrastive learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/84IW33QV/2007.html:text/html;Chuang et al. - 2020 - Debiased Contrastive Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv2007.00224 [cs, stat]2020/Chuang et al. - 2020 - Debiased Contrastive Learning.pdf:application/pdf},
}

@misc{noauthor_gt-riplcontinual-learning-benchmark_2021,
	title = {{GT}-{RIPL}/{Continual}-{Learning}-{Benchmark}},
	copyright = {MIT},
	url = {https://github.com/GT-RIPL/Continual-Learning-Benchmark},
	abstract = {Evaluate three types of task shifting with popular continual learning algorithms.},
	urldate = {2021-06-21},
	publisher = {GT-RIPL},
	month = jun,
	year = {2021},
	note = {original-date: 2018-11-30T16:40:23Z},
	keywords = {survey, incremental-learning, deep-learning, artificial-neural-networks, continual-learning, continuous-learning, lifelong-learning},
}

@misc{mai_raptormaionline-continual-learning_2021,
	title = {{RaptorMai}/online-continual-learning},
	url = {https://github.com/RaptorMai/online-continual-learning},
	abstract = {A collection of online continual learning paper implementations for computer vision in PyTorch, including ASER(AAAI-21), SCR(CVPR21-W) and a survey under review.},
	urldate = {2021-06-14},
	author = {Mai, Zheda (Marco)},
	month = jun,
	year = {2021},
	note = {original-date: 2021-01-29T20:54:21Z},
	keywords = {incremental-learning, computer-vision, deep-learning, continual-learning, lifelong-learning, catastrophic-forgetting, class-incremental-learning, convolutional-neural-networks, incremental-continual-learning, online-continual-learning, SCR},
}

@misc{mai_raptormaicontinual_learning_papers_2021,
	title = {{RaptorMai}/continual\_learning\_papers},
	url = {https://github.com/RaptorMai/continual_learning_papers},
	abstract = {Relevant papers in Continual Learning},
	urldate = {2021-06-14},
	author = {Mai, Zheda (Marco)},
	month = feb,
	year = {2021},
	note = {original-date: 2021-02-14T04:45:17Z},
	keywords = {CL},
}

@article{noauthor_theory_nodate,
	title = {Theory of {Deep} {Learning}},
	language = {en},
	pages = {118},
	file = {Theory of Deep Learning.pdf:/Users/nicolas/Zotero/storage/JKUVJ7MY/Theory of Deep Learning.pdf:application/pdf},
}

@article{aljundi_memory_2018,
	title = {Memory {Aware} {Synapses}: {Learning} what (not) to forget},
	shorttitle = {Memory {Aware} {Synapses}},
	url = {http://arxiv.org/abs/1711.09601},
	abstract = {Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule,which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting \${\textless}\$subject, predicate, object\${\textgreater}\$ triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.},
	urldate = {2021-06-09},
	journal = {arXiv:1711.09601 [cs, stat]},
	author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
	month = oct,
	year = {2018},
	note = {arXiv: 1711.09601},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, MAS},
	file = {Aljundi et al. - 2018 - Memory Aware Synapses Learning what (not) to forg.pdf:/Users/nicolas/Documents/Zotero/arXiv1711.09601 [cs, stat]2018/Aljundi et al. - 2018 - Memory Aware Synapses Learning what (not) to forg.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IYJYL5DB/1711.html:text/html},
}

@article{aljundi_task-free_2019,
	title = {Task-{Free} {Continual} {Learning}},
	url = {http://arxiv.org/abs/1812.03596},
	abstract = {Methods proposed in the literature towards continual deep learning typically operate in a task-based sequential learning setup. A sequence of tasks is learned, one at a time, with all data of current task available but not of previous or future tasks. Task boundaries and identities are known at all times. This setup, however, is rarely encountered in practical applications. Therefore we investigate how to transform continual learning to an online setup. We develop a system that keeps on learning over time in a streaming fashion, with data distributions gradually changing and without the notion of separate tasks. To this end, we build on the work on Memory Aware Synapses, and show how this method can be made online by providing a protocol to decide i) when to update the importance weights, ii) which data to use to update them, and iii) how to accumulate the importance weights at each update step. Experimental results show the validity of the approach in the context of two applications: (self-)supervised learning of a face recognition model by watching soap series and learning a robot to avoid collisions.},
	urldate = {2021-06-09},
	journal = {arXiv:1812.03596 [cs, stat]},
	author = {Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne},
	month = aug,
	year = {2019},
	note = {arXiv: 1812.03596},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Aljundi et al. - 2019 - Task-Free Continual Learning.pdf:/Users/nicolas/Documents/Zotero/arXiv1812.03596 [cs, stat]2019/Aljundi et al. - 2019 - Task-Free Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/6XG57FEE/1812.html:text/html},
}

@misc{noauthor_deepminddeepmind-research_2021,
	title = {deepmind/deepmind-research},
	copyright = {Apache-2.0},
	url = {https://github.com/deepmind/deepmind-research},
	abstract = {This repository contains implementations and illustrative code to accompany DeepMind publications},
	urldate = {2021-06-08},
	publisher = {DeepMind},
	month = jun,
	year = {2021},
	note = {original-date: 2019-01-15T09:54:13Z},
	keywords = {CURL},
}

@misc{taylor_camerontaylorflstam_2021,
	title = {{CameronTaylorFL}/stam},
	url = {https://github.com/CameronTaylorFL/stam},
	abstract = {STAM Code},
	urldate = {2021-06-08},
	author = {Taylor, Cameron},
	month = may,
	year = {2021},
	note = {original-date: 2021-05-10T16:45:59Z},
}

@article{von_luxburg_tutorial_2007,
	title = {A tutorial on spectral clustering},
	volume = {17},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-007-9033-z},
	doi = {10.1007/s11222-007-9033-z},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved eﬃciently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the ﬁrst glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe diﬀerent graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several diﬀerent approaches. Advantages and disadvantages of the diﬀerent spectral clustering algorithms are discussed.},
	language = {en},
	number = {4},
	urldate = {2021-06-07},
	journal = {Statistics and Computing},
	author = {von Luxburg, Ulrike},
	month = dec,
	year = {2007},
	keywords = {tutorial},
	pages = {395--416},
	file = {von Luxburg - 2007 - A tutorial on spectral clustering.pdf:/Users/nicolas/Zotero/storage/CYYXLREJ/von Luxburg - 2007 - A tutorial on spectral clustering.pdf:application/pdf},
}

@article{smith_unsupervised_2021,
	title = {Unsupervised {Progressive} {Learning} and the {STAM} {Architecture}},
	url = {http://arxiv.org/abs/1904.02021},
	abstract = {We first pose the Unsupervised Progressive Learning (UPL) problem: an online representation learning problem in which the learner observes a non-stationary and unlabeled data stream, learning a growing number of features that persist over time even though the data is not stored or replayed. To solve the UPL problem we propose the Self-Taught Associative Memory (STAM) architecture. Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical features rather than specific examples. We evaluate STAM representations using clustering and classification tasks. While there are no existing learning scenarios that are directly comparable to UPL, we compare the STAM architecture with two recent continual learning models, Memory Aware Synapses (MAS) and Gradient Episodic Memories (GEM), after adapting them in the UPL setting.},
	urldate = {2021-06-07},
	journal = {arXiv:1904.02021 [cs, q-bio, stat]},
	author = {Smith, James and Taylor, Cameron and Baer, Seth and Dovrolis, Constantine},
	month = may,
	year = {2021},
	note = {arXiv: 1904.02021
version: 6},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition, I.2.6, UPL},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Z8DX4EZ4/1904.html:text/html;Smith et al. - 2021 - Unsupervised Progressive Learning and the STAM Arc.pdf:/Users/nicolas/Documents/Zotero/arXiv1904.02021 [cs, q-bio, stat]2021/Smith et al. - 2021 - Unsupervised Progressive Learning and the STAM Arc.pdf:application/pdf},
}

@misc{kim_puzzle_2020,
	title = {Puzzle {Mix}: {Exploiting} {Saliency} and {Local} {Statistics} for {Optimal} {Mixup}},
	shorttitle = {Puzzle {Mix}},
	url = {http://arxiv.org/abs/2009.06962},
	doi = {10.48550/arXiv.2009.06962},
	abstract = {While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets. The source code is available at https://github.com/snu-mllab/PuzzleMix.},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Kim, Jang-Hyun and Choo, Wonho and Song, Hyun Oh},
	month = dec,
	year = {2020},
	note = {arXiv:2009.06962 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/F3ZEKEHL/2009.html:text/html;Kim et al_2020_Puzzle Mix.pdf:/Users/nicolas/Documents/Zotero/Kim et al_2020_Puzzle Mix.pdf:application/pdf},
}

@misc{walawalkar_attentive_2020,
	title = {Attentive {CutMix}: {An} {Enhanced} {Data} {Augmentation} {Approach} for {Deep} {Learning} {Based} {Image} {Classification}},
	shorttitle = {Attentive {CutMix}},
	url = {http://arxiv.org/abs/2003.13048},
	abstract = {Convolutional neural networks (CNN) are capable of learning robust representation with different regularization methods and activations as convolutional layers are spatially correlated. Based on this property, a large variety of regional dropout strategies have been proposed, such as Cutout [1], DropBlock [2], CutMix [3], etc. These methods aim to promote the network to generalize better by partially occluding the discriminative parts of objects. However, all of them perform this operation randomly, without capturing the most important region(s) within an object. In this paper, we propose Attentive CutMix, a naturally enhanced augmentation strategy based on CutMix [3]. In each training iteration, we choose the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. Our proposed method is simple yet effective, easy to implement and can boost the baseline signiﬁcantly. Extensive experiments on CIFAR-10/100 datasets with various CNN architectures (in a uniﬁed setting) demonstrate the effectiveness of our proposed method, which consistently outperforms the baseline CutMix and other methods by a signiﬁcant margin.},
	language = {en},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Walawalkar, Devesh and Shen, Zhiqiang and Liu, Zechun and Savvides, Marios},
	month = apr,
	year = {2020},
	note = {arXiv:2003.13048 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Walawalkar et al. - 2020 - Attentive CutMix An Enhanced Data Augmentation Ap.pdf:/Users/nicolas/Zotero/storage/9VXB6I7F/Walawalkar et al. - 2020 - Attentive CutMix An Enhanced Data Augmentation Ap.pdf:application/pdf},
}

@misc{walawalkar_attentive_2020-1,
	title = {Attentive {CutMix}: {An} {Enhanced} {Data} {Augmentation} {Approach} for {Deep} {Learning} {Based} {Image} {Classification}},
	shorttitle = {Attentive {CutMix}},
	url = {http://arxiv.org/abs/2003.13048},
	doi = {10.48550/arXiv.2003.13048},
	abstract = {Convolutional neural networks (CNN) are capable of learning robust representation with different regularization methods and activations as convolutional layers are spatially correlated. Based on this property, a large variety of regional dropout strategies have been proposed, such as Cutout, DropBlock, CutMix, etc. These methods aim to promote the network to generalize better by partially occluding the discriminative parts of objects. However, all of them perform this operation randomly, without capturing the most important region(s) within an object. In this paper, we propose Attentive CutMix, a naturally enhanced augmentation strategy based on CutMix. In each training iteration, we choose the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. Our proposed method is simple yet effective, easy to implement and can boost the baseline significantly. Extensive experiments on CIFAR-10/100, ImageNet datasets with various CNN architectures (in a unified setting) demonstrate the effectiveness of our proposed method, which consistently outperforms the baseline CutMix and other methods by a significant margin.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Walawalkar, Devesh and Shen, Zhiqiang and Liu, Zechun and Savvides, Marios},
	month = apr,
	year = {2020},
	note = {arXiv:2003.13048 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/G8MNEK5Z/Walawalkar et al. - 2020 - Attentive CutMix An Enhanced Data Augmentation Ap.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/2F5RRJUU/2003.html:text/html},
}

@misc{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {http://arxiv.org/abs/1710.09412},
	doi = {10.48550/arXiv.1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = apr,
	year = {2018},
	note = {arXiv:1710.09412 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/XNE96JX3/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/UHAR9RV8/1710.html:text/html},
}

@misc{yu_scale_2022-1,
	title = {{SCALE}: {Online} {Self}-{Supervised} {Lifelong} {Learning} without {Prior} {Knowledge}},
	shorttitle = {{SCALE}},
	url = {http://arxiv.org/abs/2208.11266},
	doi = {10.48550/arXiv.2208.11266},
	abstract = {Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries) which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios and the current studies, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. We conduct preliminary analyses and show that existing approaches fail to learn useful information in this setup. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on-the-fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgetting loss, and an online memory update for uniform subset selection. All three components are designed to work collaboratively to maximize learning performance. We perform comprehensive experiments of SCALE under iid and four non-iid data streams. The results show that SCALE outperforms the best state-of-the-art algorithm in all settings with improvements up to 3.83\%, 2.77\% and 5.86\% in terms of kNN accuracy on CIFAR-10, CIFAR-100, and SubImageNet datasets.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Yu, Xiaofan and Guo, Yunhui and Gao, Sicun and Rosing, Tajana},
	month = nov,
	year = {2022},
	note = {arXiv:2208.11266 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/XGZ4IP3T/Yu et al. - 2022 - SCALE Online Self-Supervised Lifelong Learning wi.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/V98SEAS4/2208.html:text/html},
}

@misc{douillard_dytox_2022,
	title = {{DyTox}: {Transformers} for {Continual} {Learning} with {DYnamic} {TOken} {eXpansion}},
	shorttitle = {{DyTox}},
	url = {http://arxiv.org/abs/2111.11326},
	abstract = {Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. A recent trend indicates that dynamic architectures based on an expansion of the parameters can reduce catastrophic forgetting efﬁciently in continual learning. However, existing approaches often require a task identiﬁer at test-time, need complex tuning to balance the growing number of parameters, and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks without signiﬁcant overhead.},
	language = {en},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Douillard, Arthur and Ramé, Alexandre and Couairon, Guillaume and Cord, Matthieu},
	month = aug,
	year = {2022},
	note = {arXiv:2111.11326 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Douillard et al. - 2022 - DyTox Transformers for Continual Learning with DY.pdf:/Users/nicolas/Zotero/storage/P7CZ26N3/Douillard et al. - 2022 - DyTox Transformers for Continual Learning with DY.pdf:application/pdf},
}

@inproceedings{fini_self-supervised_2022,
	address = {New Orleans, LA, USA},
	title = {Self-{Supervised} {Models} are {Continual} {Learners}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878593/},
	doi = {10.1109/CVPR52688.2022.00940},
	abstract = {Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings. Code: github.com/DonkeyShot21/cassle.},
	language = {en},
	urldate = {2022-12-19},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Fini, Enrico and Da Costa, Victor G. Turrisi and Alameda-Pineda, Xavier and Ricci, Elisa and Alahari, Karteek and Mairal, Julien},
	month = jun,
	year = {2022},
	pages = {9611--9620},
	file = {Fini et al. - 2022 - Self-Supervised Models are Continual Learners.pdf:/Users/nicolas/Zotero/storage/TXAV8UEU/Fini et al. - 2022 - Self-Supervised Models are Continual Learners.pdf:application/pdf},
}

@misc{madaan_representational_2022,
	title = {Representational {Continuity} for {Unsupervised} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2110.06976},
	abstract = {Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-ofdistribution tasks than SCL. Furthermore, we ﬁnd that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks’ instances to alleviate catastrophic forgetting for unsupervised representations. We release our code online.},
	language = {en},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Madaan, Divyam and Yoon, Jaehong and Li, Yuanchun and Liu, Yunxin and Hwang, Sung Ju},
	month = apr,
	year = {2022},
	note = {arXiv:2110.06976 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Madaan et al. - 2022 - Representational Continuity for Unsupervised Conti.pdf:/Users/nicolas/Zotero/storage/FMUAVLVK/Madaan et al. - 2022 - Representational Continuity for Unsupervised Conti.pdf:application/pdf},
}

@article{chawla_smote_2002,
	title = {{SMOTE}: synthetic minority over-sampling technique},
	volume = {16},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy.},
	number = {1},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
	month = jun,
	year = {2002},
	pages = {321--357},
}

@misc{yun_cutmix_2019,
	title = {{CutMix}: {Regularization} {Strategy} to {Train} {Strong} {Classifiers} with {Localizable} {Features}},
	shorttitle = {{CutMix}},
	url = {http://arxiv.org/abs/1905.04899},
	abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classiﬁers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefﬁciency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efﬁcient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classiﬁcation tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classiﬁer, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch.},
	language = {en},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
	month = aug,
	year = {2019},
	note = {arXiv:1905.04899 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:/Users/nicolas/Zotero/storage/QGIA6UGS/Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:application/pdf},
}

@misc{davari_probing_2022-1,
	title = {Probing {Representation} {Forgetting} in {Supervised} and {Unsupervised} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2203.13381},
	doi = {10.48550/arXiv.2203.13381},
	abstract = {Continual Learning research typically focuses on tackling the phenomenon of catastrophic forgetting in neural networks. Catastrophic forgetting is associated with an abrupt loss of knowledge previously learned by a model when the task, or more broadly the data distribution, being trained on changes. In supervised learning problems this forgetting, resulting from a change in the model's representation, is typically measured or observed by evaluating the decrease in old task performance. However, a model's representation can change without losing knowledge about prior tasks. In this work we consider the concept of representation forgetting, observed by using the difference in performance of an optimal linear classifier before and after a new task is introduced. Using this tool we revisit a number of standard continual learning benchmarks and observe that, through this lens, model representations trained without any explicit control for forgetting often experience small representation forgetting and can sometimes be comparable to methods which explicitly control for forgetting, especially in longer task sequences. We also show that representation forgetting can lead to new insights on the effect of model capacity and loss function used in continual learning. Based on our results, we show that a simple yet competitive approach is to learn representations continually with standard supervised contrastive learning while constructing prototypes of class samples when queried on old samples.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Davari, MohammadReza and Asadi, Nader and Mudur, Sudhir and Aljundi, Rahaf and Belilovsky, Eugene},
	month = apr,
	year = {2022},
	note = {arXiv:2203.13381 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/A7NG4F7K/2203.html:text/html;Davari et al_2022_Probing Representation Forgetting in Supervised and Unsupervised Continual.pdf:/Users/nicolas/Documents/Zotero/Davari et al_2022_Probing Representation Forgetting in Supervised and Unsupervised Continual.pdf:application/pdf},
}

@misc{xie_general_2022,
	title = {General {Incremental} {Learning} with {Domain}-aware {Categorical} {Representations}},
	url = {http://arxiv.org/abs/2204.04078},
	doi = {10.48550/arXiv.2204.04078},
	abstract = {Continual learning is an important problem for achieving human-level intelligence in real-world applications as an agent must continuously accumulate knowledge in response to streaming data/tasks. In this work, we consider a general and yet under-explored incremental learning problem in which both the class distribution and class-specific domain distribution change over time. In addition to the typical challenges in class incremental learning, this setting also faces the intra-class stability-plasticity dilemma and intra-class domain imbalance problems. To address above issues, we develop a novel domain-aware continual learning method based on the EM framework. Specifically, we introduce a flexible class representation based on the von Mises-Fisher mixture model to capture the intra-class structure, using an expansion-and-reduction strategy to dynamically increase the number of components according to the class complexity. Moreover, we design a bi-level balanced memory to cope with data imbalances within and across classes, which combines with a distillation loss to achieve better inter- and intra-class stability-plasticity trade-off. We conduct exhaustive experiments on three benchmarks: iDigits, iDomainNet and iCIFAR-20. The results show that our approach consistently outperforms previous methods by a significant margin, demonstrating its superiority.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Xie, Jiangwei and Yan, Shipeng and He, Xuming},
	month = oct,
	year = {2022},
	note = {arXiv:2204.04078 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/WV85LCJE/2204.html:text/html;Xie et al_2022_General Incremental Learning with Domain-aware Categorical Representations.pdf:/Users/nicolas/Documents/Zotero/Xie et al_2022_General Incremental Learning with Domain-aware Categorical Representations.pdf:application/pdf},
}

@article{wang_continual_2022-1,
	title = {Continual {Learning} through {Retrieval} and {Imagination}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20837},
	doi = {10.1609/aaai.v36i8.20837},
	abstract = {Continual learning is an intellectual ability of artificial agents to learn new streaming labels from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting.},
	language = {en},
	number = {8},
	urldate = {2022-12-20},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Zhen and Liu, Liu and Duan, Yiqun and Tao, Dacheng},
	month = jun,
	year = {2022},
	note = {Number: 8},
	keywords = {Machine Learning (ML)},
	pages = {8594--8602},
	file = {Wang et al_2022_Continual Learning through Retrieval and Imagination.pdf:/Users/nicolas/Documents/Zotero/Wang et al_2022_Continual Learning through Retrieval and Imagination.pdf:application/pdf},
}

@inproceedings{gatys_image_2016,
	title = {Image {Style} {Transfer} {Using} {Convolutional} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html},
	urldate = {2022-12-20},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	year = {2016},
	pages = {2414--2423},
	file = {Gatys et al_2016_Image Style Transfer Using Convolutional Neural Networks.pdf:/Users/nicolas/Documents/Zotero/Gatys et al_2016_Image Style Transfer Using Convolutional Neural Networks.pdf:application/pdf},
}

@article{jackson_style_nodate,
	title = {Style {Augmentation}: {Data} {Augmentation} via {Style} {Randomization}},
	abstract = {We introduce style augmentation, a new form of data augmentation based on random style transfer, for improving the robustness of Convolutional Neural Networks (CNN) over both classiﬁcation and regression based tasks. During training, style augmentation randomizes texture, contrast and color, while preserving shape and semantic content. This is accomplished by adapting an arbitrary style transfer network to perform style randomization, by sampling target style embeddings from a multivariate normal distribution instead of computing them from a style image. In addition to standard classiﬁcation experiments, we investigate the effect of style augmentation (and data augmentation generally) on domain transfer tasks. We ﬁnd that data augmentation signiﬁcantly improves robustness to domain shift, and can be used as a simple, domain agnostic alternative to domain adaptation. Comparing style augmentation against a mix of seven traditional augmentation techniques, we ﬁnd that it can be readily combined with them to improve network performance. We validate the efﬁcacy of our technique with domain transfer experiments in classiﬁcation and monocular depth estimation illustrating superior performance over benchmark tasks.},
	language = {en},
	author = {Jackson, Philip T and Atapour-Abarghouei, Amir and Bonner, Stephen and Breckon, Toby P and Obara, Boguslaw},
	file = {Jackson et al. - Style Augmentation Data Augmentation via Style Ra.pdf:/Users/nicolas/Zotero/storage/RHKU9V3N/Jackson et al. - Style Augmentation Data Augmentation via Style Ra.pdf:application/pdf},
}

@misc{huang_arbitrary_2017,
	title = {Arbitrary {Style} {Transfer} in {Real}-time with {Adaptive} {Instance} {Normalization}},
	url = {http://arxiv.org/abs/1703.06868},
	doi = {10.48550/arXiv.1703.06868},
	abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Huang, Xun and Belongie, Serge},
	month = jul,
	year = {2017},
	note = {arXiv:1703.06868 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/EDFE5J6R/1703.html:text/html;Huang_Belongie_2017_Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization.pdf:/Users/nicolas/Documents/Zotero/Huang_Belongie_2017_Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization.pdf:application/pdf},
}

@inproceedings{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	urldate = {2022-12-20},
	author = {Krizhevsky, A.},
	year = {2009},
}

@inproceedings{le_tiny_2015,
	title = {Tiny {ImageNet} {Visual} {Recognition} {Challenge}},
	urldate = {2022-12-20},
	author = {Le, Ya and Yang, Xuan S.},
	year = {2015},
}

@article{hsu_re-evaluating_2019,
      title={Re-evaluating continual learning scenarios: A categorization and case for strong baselines},
      author={Hsu, Yen-Chang and Liu, Yen-Cheng and Ramasamy, Anita and Kira, Zsolt},
      journal={arXiv preprint arXiv:1810.12488},
      year={2018}
}


@inproceedings{he_online_2022,
	title = {Online {Continual} {Learning} via {Candidates} {Voting}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/He_Online_Continual_Learning_via_Candidates_Voting_WACV_2022_paper.html},
	language = {en},
	urldate = {2022-12-21},
	author = {He, Jiangpeng and Zhu, Fengqing},
	year = {2022},
	pages = {3154--3163},
	file = {He_Zhu_2022_Online Continual Learning via Candidates Voting.pdf:/Users/nicolas/Documents/Zotero/He_Zhu_2022_Online Continual Learning via Candidates Voting.pdf:application/pdf},
}

@misc{agarap_deep_2019,
	title = {Deep {Learning} using {Rectified} {Linear} {Units} ({ReLU})},
	url = {http://arxiv.org/abs/1803.08375},
	doi = {10.48550/arXiv.1803.08375},
	abstract = {We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer \$h\_\{n - 1\}\$ in a neural network, then multiply it by weight parameters \${\textbackslash}theta\$ to get the raw scores \$o\_\{i\}\$. Afterwards, we threshold the raw scores \$o\_\{i\}\$ by \$0\$, i.e. \$f(o) = {\textbackslash}max(0, o\_\{i\})\$, where \$f(o)\$ is the ReLU function. We provide class predictions \${\textbackslash}hat\{y\}\$ through argmax function, i.e. argmax \$f(x)\$.},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Agarap, Abien Fred},
	month = feb,
	year = {2019},
	note = {arXiv:1803.08375 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/4GUCVC2P/Agarap - 2019 - Deep Learning using Rectified Linear Units (ReLU).pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/KH5TCVDA/1803.html:text/html},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/7YI7NWKF/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/B6DQZMRQ/1405.html:text/html},
}

@article{nikoloutsopoulos_online_nodate,
	title = {Online {Continual} {Learning} from {Imbalanced} {Data} with {Kullback}-{Leibler}-loss based replay buffer updates},
	abstract = {We propose an online replay-based Continual Learning policy, in which the learner stores data points to a local buffer and replays it during training. The core of our contribution is a new replay buffer contents update policy that combines a Kullback-Leibler (K-L) loss and an appropriate modification of the celebrated Reservoir Sampling algorithm. The decisions at each time are, whether the newly arriving training data points will be inserted in the buffer, and which existing data points from the buffer will be substituted. We update the buffer content so that the proportion of stored data points from different classes in the buffer approximates a target distribution that depends on the empirical distribution of classes seen in the training data stream. We parameterize the target distribution with a single parameter that allows us to model different target class distributions in the buffer, such as the class distribution that is present in the training data stream, the uniform class distribution, and a distribution with class percentages that are inversely proportional to those in the training data stream. We evaluate our method on MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100, and we show that our method is superior to the state-of-the-art Reservoir Sampling algorithm. Our main finding is that the best (in terms of accuracy and forgetting) value of the parameter that determines the distribution of classes in the buffer versus that of the stream depends on statistics of the training data and on the dataset itself. Our work paves the way for further work to learn this parameter in the realistic scenario that it is unknown, thus contributing to the objective of an optimal replay-based continual learning approach that adapts to the specifics of each scenario.},
	language = {en},
	author = {Nikoloutsopoulos, Sotirios and Koutsopoulos, Iordanis and Titsias, Michalis K},
	file = {Nikoloutsopoulos et al. - Online Continual Learning from Imbalanced Data wit.pdf:/Users/nicolas/Zotero/storage/H6MYRG8W/Nikoloutsopoulos et al. - Online Continual Learning from Imbalanced Data wit.pdf:application/pdf},
}

@inproceedings{guo_online_2022,
	title = {Online {Continual} {Learning} through {Mutual} {Information} {Maximization}},
	urldate = {2023-01-13},
    pages        = {8109--8126},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	author = {Guo, Yiduo and Liu, Bing and Zhao, Dongyan},
	year = {2022},
}

@article{zhu_class-incremental_nodate,
	title = {Class-{Incremental} {Learning} via {Dual} {Augmentation}},
	abstract = {Deep learning systems typically suffer from catastrophic forgetting of past knowledge when acquiring new skills continually. In this paper, we emphasize two dilemmas, representation bias and classiﬁer bias in class-incremental learning, and present a simple and novel approach that employs explicit class augmentation (classAug) and implicit semantic augmentation (semanAug) to address the two biases, respectively. On the one hand, we propose to address the representation bias by learning transferable and diverse representations. Speciﬁcally, we investigate the feature representations in incremental learning based on spectral analysis and present a simple technique called classAug, to let the model see more classes during training for learning representations transferable across classes. On the other hand, to overcome the classiﬁer bias, semanAug implicitly involves the simultaneous generating of an inﬁnite number of instances of old classes in the deep feature space, which poses tighter constraints to maintain the decision boundary of previously learned classes. Without storing any old samples, our method can perform comparably with representative data replay based approaches.},
	language = {en},
	author = {Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin},
	file = {Zhu et al. - Class-Incremental Learning via Dual Augmentation.pdf:/Users/nicolas/Zotero/storage/D5RZNHLL/Zhu et al. - Class-Incremental Learning via Dual Augmentation.pdf:application/pdf},
}

@inproceedings{wang_hybrid_2022,
	title = {Hybrid {Contrastive} {Quantization} for {Efficient} {Cross}-{View} {Video} {Retrieval}},
	url = {http://arxiv.org/abs/2202.03384},
	doi = {10.1145/3485447.3512022},
	abstract = {With the recent boom of video-based social platforms (e.g., YouTube and TikTok), video retrieval using sentence queries has become an important demand and attracts increasing research attention. Despite the decent performance, existing text-video retrieval models in vision and language communities are impractical for large-scale Web search because they adopt brute-force search based on highdimensional embeddings. To improve efficiency, Web search engines widely apply vector compression libraries (e.g., FAISS [26]) to post-process the learned embeddings. Unfortunately, separate compression from feature encoding degrades the robustness of representations and incurs performance decay. To pursue a better balance between performance and efficiency, we propose the first quantized representation learning method for cross-view video retrieval, namely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both coarse-grained and fine-grained quantizations with transformers, which provide complementary understandings for texts and videos and preserve comprehensive semantic information. By performing Asymmetric-Quantized Contrastive Learning (AQCL) across views, HCQ aligns texts and videos at coarse-grained and multiple fine-grained levels. This hybrid-grained learning strategy serves as strong supervision on the cross-view video quantization model, where contrastive learning at different levels can be mutually promoted. Extensive experiments on three Web video benchmark datasets demonstrate that HCQ achieves competitive performance with state-of-the-art non-compressed retrieval methods while showing high efficiency in storage and computation.},
	language = {en},
	urldate = {2023-01-29},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	author = {Wang, Jinpeng and Chen, Bin and Liao, Dongliang and Zeng, Ziyun and Li, Gongfu and Xia, Shu-Tao and Xu, Jin},
	month = apr,
	year = {2022},
	note = {arXiv:2202.03384 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Social and Information Networks, Computer Science - Multimedia},
	pages = {3020--3030},
	file = {Wang et al. - 2022 - Hybrid Contrastive Quantization for Efficient Cros.pdf:/Users/nicolas/Zotero/storage/PD9C5AZK/Wang et al. - 2022 - Hybrid Contrastive Quantization for Efficient Cros.pdf:application/pdf},
}

@article{hu_weighted_2002,
	title = {The weighted likelihood},
	volume = {30},
	issn = {03195724, 1708945X},
	url = {http://doi.wiley.com/10.2307/3316141},
	doi = {10.2307/3316141},
	language = {en},
	number = {3},
	urldate = {2023-02-11},
	journal = {Canadian Journal of Statistics},
	author = {Hu, Feifang and Zidek, James V.},
	month = sep,
	year = {2002},
	pages = {347--371},
	file = {Hu and Zidek - 2002 - The weighted likelihood.pdf:/Users/nicolas/Zotero/storage/BAH3D9PY/Hu and Zidek - 2002 - The weighted likelihood.pdf:application/pdf},
}

@article{lin_anchor_2022,
	title = {Anchor {Assisted} {Experience} {Replay} for {Online} {Class}-{Incremental} {Learning}},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2022.3219605},
	abstract = {Online class-incremental learning (OCIL) studies the problem of mitigating the phenomenon of catastrophic forgetting while learning new classes from a continuously non-stationary data stream. Existing approaches mainly constrain the updating of parameters to prevent the drift of previous classes that reflects the movement of samples in the embedding space. Although this kind of drift can be relieved to some extent by existing approaches, it is usually inevitable. Therefore, only prevention of drift is not enough, and we also need to further compensate for it. To this end, for each previous class, we exploit the sample with the smallest loss value as its anchor, which can representatively characterize the corresponding class. Based on the assistance of anchors, we present a novel Anchor Assisted Experience Replay (AAER) method that not only prevents the drift but also compensates for the inevitable drift to overcome the catastrophic forgetting. Specifically, we design a Drift-Prevention with Anchor (DPA) operation, which plays a preventive role by reducing the drift implicitly as well as encouraging the samples with the same label cluster tightly. Moreover, we propose a Drift-Compensation with Anchor (DCA) operation that contains two remedy mechanisms: one is Forward-offset which keeps embedding of previous data but estimates new classification centers; the other is just the opposite named Backward-offset, which keeps the old classification centers unchanged but updates the embedding of previous data. We conduct extensive experiments on three real-world datasets, and empirical results consistently demonstrate the superior performance of AAER over various state-of-the-art methods.},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Lin, Huiwei and Feng, Shanshan and Li, Xutao and Li, Wentao and Ye, Yunming},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Task analysis, Airplanes, Atmospheric modeling, Automobiles, Deep Learning, Image Recognition, Memory management, Neural Networks, Online Class-Incremental Learning, Reservoirs, Training},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/nicolas/Zotero/storage/FQ6K6HFS/stamp.html:text/html;Lin et al_2022_Anchor Assisted Experience Replay for Online Class-Incremental Learning.pdf:/Users/nicolas/Documents/Zotero/Lin et al_2022_Anchor Assisted Experience Replay for Online Class-Incremental Learning.pdf:application/pdf},
}

@article{liang_new_2023,
  author       = {Guoqiang Liang and
                  Zhaojie Chen and
                  Zhaoqiang Chen and
                  Shiyu Ji and
                  Yanning Zhang},
  title        = {New Insights on Relieving Task-Recency Bias for Online Class Incremental
                  Learning},
  journal      = {{IEEE} Trans. Circuits Syst. Video Technol.},
  volume       = {34},
  number       = {5},
  pages        = {3451--3464},
  year         = {2024},
}


@incollection{chaudhry_riemannian_2018,
	title = {Riemannian {Walk} for {Incremental} {Learning}: {Understanding} {Forgetting} and {Intransigence}},
	volume = {11215},
	shorttitle = {Riemannian {Walk} for {Incremental} {Learning}},
	url = {http://arxiv.org/abs/1801.10112},
	abstract = {Incremental learning (IL) has received a lot of attention recently, however, the literature lacks a precise problem deﬁnition, proper evaluation settings, and metrics tailored speciﬁcally for the IL problem. One of the main objectives of this work is to ﬁll these gaps so as to provide a common ground for better understanding of IL. The main challenge for an IL algorithm is to update the classiﬁer whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, IL also suffers from a problem we call intransigence, inability of a model to update its knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of IL algorithms. We present RWalk, a generalization of EWC++ (our efﬁcient version of EWC [7]) and Path Integral [26] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various IL algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off between forgetting and intransigence.},
	language = {en},
	urldate = {2023-03-03},
	author = {Chaudhry, Arslan and Dokania, Puneet K. and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
	year = {2018},
	doi = {10.1007/978-3-030-01252-6_33},
	note = {arXiv:1801.10112 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {556--572},
	file = {Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf:/Users/nicolas/Zotero/storage/FN8CW9HL/Chaudhry et al. - 2018 - Riemannian Walk for Incremental Learning Understa.pdf:application/pdf},
}

@inproceedings{gu_not_2022,
	title = {Not {Just} {Selection}, but {Exploration}: {Online} {Class}-{Incremental} {Continual} {Learning} via {Dual} {View} {Consistency}},
	shorttitle = {Not {Just} {Selection}, but {Exploration}},
	language = {en},
	urldate = {2023-03-15},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Gu, Yanan and Yang, Xu and Wei, Kun and Deng, Cheng},
	month = jun,
	year = {2022},
	pages = {7432--7441},
	file = {Gu et al. - 2022 - Not Just Selection, but Exploration Online Class-.pdf:/Users/nicolas/Zotero/storage/EHKUTUXU/Gu et al. - 2022 - Not Just Selection, but Exploration Online Class-.pdf:application/pdf},
}

@inproceedings{buzzega_dark_2020,
  title={Dark experience for general continual learning: a strong, simple baseline},
  author={Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15920--15930},
  year={2020}
}
@inproceedings{
caccia_new_2022,
title={New Insights on Reducing Abrupt Representation Change in Online Continual Learning},
author={Lucas Caccia and Rahaf Aljundi and Nader Asadi and Tinne Tuytelaars and Joelle Pineau and Eugene Belilovsky},
booktitle={International Conference on Learning Representations},
year={2022},
}

@inproceedings{bang_online_2022,
  title={Online continual learning on a contaminated data stream with blurry task boundaries},
  author={Bang, Jihwan and Koh, Hyunseo and Park, Seulki and Song, Hwanjun and Ha, Jung-Woo and Choi, Jonghyun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9275--9284},
  year={2022}
}


@misc{michel_contrastive_2022-1,
	title = {Contrastive {Learning} for {Online} {Semi}-{Supervised} {General} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2207.05615},
	abstract = {We study Online Continual Learning with missing labels and propose SemiCon, a new contrastive loss designed for partly labeled data. We demonstrate its eﬃciency by devising a memory-based method trained on an unlabeled data stream, where every data added to memory is labeled using an oracle. Our approach outperforms existing semi-supervised methods when few labels are available, and obtain similar results to state-of-the-art supervised methods while using only 2.6\% of labels on Split-CIFAR10 and 10\% of labels on SplitCIFAR100.},
	language = {en},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Michel, Nicolas and Negrel, Romain and Chierchia, Giovanni and Bercher, Jean-François},
	month = nov,
	year = {2022},
	note = {arXiv:2207.05615 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Michel et al. - 2022 - Contrastive Learning for Online Semi-Supervised Ge.pdf:/Users/nicolas/Zotero/storage/XZPYPXGK/Michel et al. - 2022 - Contrastive Learning for Online Semi-Supervised Ge.pdf:application/pdf},
}

@misc{lin_pcr_2023,
	title = {{PCR}: {Proxy}-based {Contrastive} {Replay} for {Online} {Class}-{Incremental} {Continual} {Learning}},
	shorttitle = {{PCR}},
	url = {http://arxiv.org/abs/2304.04408},
	abstract = {Online class-incremental continual learning is a speciﬁc task of continual learning. It aims to continuously learn new classes from data stream and the samples of data stream are seen only once, which suffers from the catastrophic forgetting issue, i.e., forgetting historical knowledge of old classes. Existing replay-based methods effectively alleviate this issue by saving and replaying part of old data in a proxy-based or contrastive-based replay manner. Although these two replay manners are effective, the former would incline to new classes due to class imbalance issues, and the latter is unstable and hard to converge because of the limited number of samples. In this paper, we conduct a comprehensive analysis of these two replay manners and ﬁnd that they can be complementary. Inspired by this ﬁnding, we propose a novel replay-based method called proxybased contrastive replay (PCR). The key operation is to replace the contrastive samples of anchors with corresponding proxies in the contrastive-based way. It alleviates the phenomenon of catastrophic forgetting by effectively addressing the imbalance issue, as well as keeps a faster convergence of the model. We conduct extensive experiments on three real-world benchmark datasets, and empirical results consistently demonstrate the superiority of PCR over various state-of-the-art methods 1.},
	language = {en},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Lin, Huiwei and Zhang, Baoquan and Feng, Shanshan and Li, Xutao and Ye, Yunming},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04408 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Lin et al. - 2023 - PCR Proxy-based Contrastive Replay for Online Cla.pdf:/Users/nicolas/Zotero/storage/9FDS5B9E/Lin et al. - 2023 - PCR Proxy-based Contrastive Replay for Online Cla.pdf:application/pdf},
}

@misc{yoon_scalable_2020,
	title = {Scalable and {Order}-robust {Continual} {Learning} with {Additive} {Parameter} {Decomposition}},
	url = {http://arxiv.org/abs/1902.09432},
	abstract = {While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, some issues remain to be tackled to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efﬁcient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which instead of learning a completely shared set of weights, represents the parameters for each task as a sum of task-shared and sparse task-adaptive parameters. With our Additive Parameter Decomposition (APD), the task-adaptive parameters for earlier tasks remain mostly unaffected, where we update them only to reﬂect the changes made to the task-shared parameters. This decomposition of parameters effectively prevents catastrophic forgetting and order-sensitivity, while being computation- and memory-efﬁcient. Further, we can achieve even better scalability with APD using hierarchical knowledge consolidation, which clusters the task-adaptive parameters to obtain hierarchically shared parameters. We validate our network with APD, APD-Net, on multiple benchmark datasets against state-of-the-art continual learning methods, which it largely outperforms in accuracy, scalability, and order-robustness.},
	language = {en},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Yoon, Jaehong and Kim, Saehoon and Yang, Eunho and Hwang, Sung Ju},
	month = feb,
	year = {2020},
	note = {arXiv:1902.09432 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, I.2.10},
	file = {Yoon et al. - 2020 - Scalable and Order-robust Continual Learning with .pdf:/Users/nicolas/Zotero/storage/8J7E9HGQ/Yoon et al. - 2020 - Scalable and Order-robust Continual Learning with .pdf:application/pdf},
}

@article{wang_comprehensive_2023,
	title = {A {Comprehensive} {Survey} of {Continual} {Learning}: {Theory}, {Method} and {Application}},
	shorttitle = {A {Comprehensive} {Survey} of {Continual} {Learning}},
	abstract = {To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic signiﬁcance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efﬁciency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of continual learning in terms of the current trends, cross-directional prospects and interdisciplinary connections with neuroscience, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this ﬁeld and beyond.},
	language = {en},
	urldate = {2023-04-20},
	author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
	month = jan,
	year = {2023},
    journal= {arXiv preprint arXiv:2302.00487},
}
@inproceedings{tarvainen2017mean,
  author       = {Antti Tarvainen and
                  Harri Valpola},
  title        = {Mean teachers are better role models: Weight-averaged consistency
                  targets improve semi-supervised deep learning results},
  booktitle    = {5th International Conference on Learning Representations},
  year         = {2017},
}

@inproceedings{pham2021dualnet,
  author       = {Quang Pham and
                  Chenghao Liu and
                  Steven C. H. Hoi},
  title        = {DualNet: Continual Learning, Fast and Slow},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems},
  pages        = {16131--16144},
  year         = {2021},
}



@misc{buzzega_rethinking_2020,
	title = {Rethinking {Experience} {Replay}: a {Bag} of {Tricks} for {Continual} {Learning}},
	shorttitle = {Rethinking {Experience} {Replay}},
	url = {http://arxiv.org/abs/2010.05595},
	doi = {10.48550/arXiv.2010.05595},
	abstract = {In Continual Learning, a Neural Network is trained on a stream of data whose distribution shifts over time. Under these assumptions, it is especially challenging to improve on classes appearing later in the stream while remaining accurate on previous ones. This is due to the infamous problem of catastrophic forgetting, which causes a quick performance degradation when the classifier focuses on learning new categories. Recent literature proposed various approaches to tackle this issue, often resorting to very sophisticated techniques. In this work, we show that naive rehearsal can be patched to achieve similar performance. We point out some shortcomings that restrain Experience Replay (ER) and propose five tricks to mitigate them. Experiments show that ER, thus enhanced, displays an accuracy gain of 51.2 and 26.9 percentage points on the CIFAR-10 and CIFAR-100 datasets respectively (memory buffer size 1000). As a result, it surpasses current state-of-the-art rehearsal-based methods.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Calderara, Simone},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05595 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/2WZ2LKNF/2010.html:text/html;Buzzega et al_2020_Rethinking Experience Replay.pdf:/Users/nicolas/Documents/Zotero/Buzzega et al_2020_Rethinking Experience Replay.pdf:application/pdf},
}

@misc{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {http://arxiv.org/abs/2103.03230},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	month = jun,
	year = {2021},
	note = {arXiv:2103.03230 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	file = {Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf:/Users/nicolas/Zotero/storage/VV6F5EZD/Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf:application/pdf},
}

@article{fisher_dispersion_1997,
	title = {Dispersion on a sphere},
	volume = {217},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1953.0064},
	doi = {10.1098/rspa.1953.0064},
	abstract = {Any topological framework requires the development of a theory of errors of characteristic and appropriate mathematical form. The paper develops a form of theory which appears to be appropriate to measurements of position on a sphere. The primary problems of estimation as applied to the true direction, and the precision of observations, are discussed in the subcases which arise. The simultaneous distribution of the amplitude and direction of the vector sum of a number of random unit vectors of given precision, is demonstrated. From this is derived the test of significance appropriate to a worker whose knowledge of precision lies entirely in the internal evidence of the sample. This is the analogue of ‘Student’s’ test in the Gaussian theory of errors. The general formulae obtained are illustrated using measurements of the direction of remanent magnetization in the directly and inversely magnetized lava flows obtained in Iceland by Mr J. Hospers.},
	number = {1130},
	urldate = {2023-04-28},
	journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
	author = {Fisher, Ronald Aylmer},
	month = jan,
	year = {1997},
	note = {Publisher: Royal Society},
	pages = {295--305},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/XYH3K4ZS/Fisher - 1997 - Dispersion on a sphere.pdf:application/pdf},
}

@misc{hasnat_von_2017,
	title = {von {Mises}-{Fisher} {Mixture} {Model}-based {Deep} learning: {Application} to {Face} {Verification}},
	shorttitle = {von {Mises}-{Fisher} {Mixture} {Model}-based {Deep} learning},
	url = {http://arxiv.org/abs/1706.04264},
	abstract = {A number of pattern recognition tasks, e.g., face veriﬁcation, can be boiled down to classiﬁcation or clustering of unit length directional feature vectors whose distance can be simply computed by their angle. In this paper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical foundation for an effective deep-learning of such directional features and derive a novel vMF Mixture Loss and its corresponding vMF deep features. The proposed vMF feature learning achieves the characteristics of discriminative learning, i.e., compacting the instances of the same class while increasing the distance of instances from different classes. Moreover, it subsumes a number of popular loss functions as well as an effective method in deep learning, namely normalization. We conduct extensive experiments on face veriﬁcation using 4 different challenging face datasets, i.e., LFW, YouTube faces, CACD and IJB-A. Results show the effectiveness and excellent generalization ability of the proposed approach as it achieves stateof-the-art results on the LFW, YouTube faces and CACD datasets and competitive results on the IJB-A dataset.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Hasnat, Md Abul and Bohné, Julien and Milgram, Jonathan and Gentric, Stéphane and Chen, Liming},
	month = dec,
	year = {2017},
	note = {arXiv:1706.04264 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hasnat et al. - 2017 - von Mises-Fisher Mixture Model-based Deep learning.pdf:/Users/nicolas/Zotero/storage/29VZP44E/Hasnat et al. - 2017 - von Mises-Fisher Mixture Model-based Deep learning.pdf:application/pdf},
}

@misc{hasnat_von_2017-1,
	title = {von {Mises}-{Fisher} {Mixture} {Model}-based {Deep} learning: {Application} to {Face} {Verification}},
	shorttitle = {von {Mises}-{Fisher} {Mixture} {Model}-based {Deep} learning},
	url = {http://arxiv.org/abs/1706.04264},
	doi = {10.48550/arXiv.1706.04264},
	abstract = {A number of pattern recognition tasks, {\textbackslash}textit\{e.g.\}, face verification, can be boiled down to classification or clustering of unit length directional feature vectors whose distance can be simply computed by their angle. In this paper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical foundation for an effective deep-learning of such directional features and derive a novel vMF Mixture Loss and its corresponding vMF deep features. The proposed vMF feature learning achieves the characteristics of discriminative learning, {\textbackslash}textit\{i.e.\}, compacting the instances of the same class while increasing the distance of instances from different classes. Moreover, it subsumes a number of popular loss functions as well as an effective method in deep learning, namely normalization. We conduct extensive experiments on face verification using 4 different challenging face datasets, {\textbackslash}textit\{i.e.\}, LFW, YouTube faces, CACD and IJB-A. Results show the effectiveness and excellent generalization ability of the proposed approach as it achieves state-of-the-art results on the LFW, YouTube faces and CACD datasets and competitive results on the IJB-A dataset.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Hasnat, Md Abul and Bohné, Julien and Milgram, Jonathan and Gentric, Stéphane and Chen, Liming},
	month = dec,
	year = {2017},
	note = {arXiv:1706.04264 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/YBA3SLXB/Hasnat et al. - 2017 - von Mises-Fisher Mixture Model-based Deep learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/TA2TB2LB/1706.html:text/html},
}

@article{wang_continual_2022-2,
	title = {Continual {Learning} through {Retrieval} and {Imagination}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20837},
	doi = {10.1609/aaai.v36i8.20837},
	abstract = {Continual learning is an intellectual ability of artificial agents to learn new streaming labels from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting.},
	language = {en},
	number = {8},
	urldate = {2023-04-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Zhen and Liu, Liu and Duan, Yiqun and Tao, Dacheng},
	month = jun,
	year = {2022},
	note = {Number: 8},
	keywords = {Machine Learning (ML)},
	pages = {8594--8602},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/QVHKITZQ/Wang et al. - 2022 - Continual Learning through Retrieval and Imaginati.pdf:application/pdf},
}

@misc{yu_scale_2023,
	title = {{SCALE}: {Online} {Self}-{Supervised} {Lifelong} {Learning} without {Prior} {Knowledge}},
	shorttitle = {{SCALE}},
	url = {http://arxiv.org/abs/2208.11266},
	doi = {10.48550/arXiv.2208.11266},
	abstract = {Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries), which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on the fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgetting loss, and an online memory update for uniform subset selection. All three components are designed to work collaboratively to maximize learning performance. We perform comprehensive experiments of SCALE under iid and four non-iid data streams. The results show that SCALE outperforms the state-of-the-art algorithm in all settings with improvements up to 3.83\%, 2.77\% and 5.86\% in terms of kNN accuracy on CIFAR-10, CIFAR-100, and TinyImageNet datasets.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Yu, Xiaofan and Guo, Yunhui and Gao, Sicun and Rosing, Tajana},
	month = apr,
	year = {2023},
	note = {arXiv:2208.11266 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/AS8ZSE76/Yu et al. - 2023 - SCALE Online Self-Supervised Lifelong Learning wi.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/BMPITFCE/2208.html:text/html},
}

@misc{asao_convergence_2022,
	title = {Convergence of neural networks to {Gaussian} mixture distribution},
	url = {http://arxiv.org/abs/2204.12100},
	abstract = {We give a proof that, under relatively mild conditions, fully-connected feed-forward deep random neural networks converge to a Gaussian mixture distribution as only the width of the last hidden layer goes to inﬁnity. We conducted experiments for a simple model which supports our result. Moreover, it gives a detailed description of the convergence, namely, the growth of the last hidden layer gets the distribution closer to the Gaussian mixture, and the other layer successively get the Gaussian mixture closer to the normal distribution.},
	language = {en},
	urldate = {2023-05-04},
	publisher = {arXiv},
	author = {Asao, Yasuhiko and Sakamoto, Ryotaro and Takagi, Shiro},
	month = apr,
	year = {2022},
	note = {arXiv:2204.12100 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Probability},
	file = {Asao et al. - 2022 - Convergence of neural networks to Gaussian mixture.pdf:/Users/nicolas/Zotero/storage/5ZUWETDX/Asao et al. - 2022 - Convergence of neural networks to Gaussian mixture.pdf:application/pdf},
}

@inproceedings{zhang_tricl_2023,
	title = {{TRICL}: {Triplet} {Continual} {Learning}},
	shorttitle = {{TRICL}},
	doi = {10.1109/ICASSP49357.2023.10095851},
	abstract = {A class-incremental learning agent learns online with a neverending stream of data in one training epoch. In this setting, the agent suffers from severe catastrophic forgetting due to the absence of data from the observed classes after learning data from new classes. Besides, the prototypes rapidly become outdated as the agent adapts to new data sequentially, and the previous example embeddings spread out in an unforeseen way, which exacerbates forgetting (i.e., concept drift). Based on this observation, we propose a replay-based method, called TriCL, which gathers the embeddings near the prototype from the same class and separates the embeddings from the different class prototypes. TriCL leverages an improved triplet loss without extra arranged input data triplets. To facilitate rapid convergence between the same class samples, we design a memory update algorithm for decreasing the variance among the buffered samples from the same class. Furthermore, we make a prototype compensation strategy for preventing drift. Compared to the state-of-the-art benchmarks, the experiments demonstrate that our proposed method presents improved performance in the online class-incremental learning setting.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhang, Xianchao and Wang, Guanglu and Zhang, Xiaotong and Liu, Han and Yin, Zhengxi and Yang, Wentao},
	month = jun,
	year = {2023},
	keywords = {Task analysis, Training, Acoustics, Benchmark testing, Online Continual Learning, Prototypes, Signal processing, Signal processing algorithms, Triplet Loss},
	pages = {1--5},
}

@inproceedings{zhang_tricl_2023-1,
	title = {{TRICL}: {Triplet} {Continual} {Learning}},
	shorttitle = {{TRICL}},
	doi = {10.1109/ICASSP49357.2023.10095851},
	abstract = {A class-incremental learning agent learns online with a neverending stream of data in one training epoch. In this setting, the agent suffers from severe catastrophic forgetting due to the absence of data from the observed classes after learning data from new classes. Besides, the prototypes rapidly become outdated as the agent adapts to new data sequentially, and the previous example embeddings spread out in an unforeseen way, which exacerbates forgetting (i.e., concept drift). Based on this observation, we propose a replay-based method, called TriCL, which gathers the embeddings near the prototype from the same class and separates the embeddings from the different class prototypes. TriCL leverages an improved triplet loss without extra arranged input data triplets. To facilitate rapid convergence between the same class samples, we design a memory update algorithm for decreasing the variance among the buffered samples from the same class. Furthermore, we make a prototype compensation strategy for preventing drift. Compared to the state-of-the-art benchmarks, the experiments demonstrate that our proposed method presents improved performance in the online class-incremental learning setting.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhang, Xianchao and Wang, Guanglu and Zhang, Xiaotong and Liu, Han and Yin, Zhengxi and Yang, Wentao},
	month = jun,
	year = {2023},
	keywords = {Task analysis, Training, Acoustics, Benchmark testing, Online Continual Learning, Prototypes, Signal processing, Signal processing algorithms, Triplet Loss},
	pages = {1--5},
}

@article{kong_trust-region_2023,
	title = {Trust-{Region} {Adaptive} {Frequency} for {Online} {Continual} {Learning}},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-023-01775-0},
	doi = {10.1007/s11263-023-01775-0},
	abstract = {In the paradigm of online continual learning, one neural network is exposed to a sequence of tasks, where the data arrive in an online fashion and previously seen data are not accessible. Such online fashion causes insufficient learning and severe forgetting on past tasks issues, preventing a good stability-plasticity trade-off, where ideally the network is expected to have high plasticity to adapt to new tasks well and have the stability to prevent forgetting on old tasks simultaneously. To solve these issues, we propose a trust-region adaptive frequency approach, which alternates between standard-process and intra-process updates. Specifically, the standard-process replays data stored in a coreset and interleaves the data with current data, and the intra-process updates the network parameters based on the coreset. Furthermore, to improve the unsatisfactory performance stemming from online fashion, the frequency of the intra-process is adjusted based on a trust region, which is measured by the confidence score of current data. During the intra-process, we distill the dark knowledge to retain useful learned knowledge. Moreover, to store more representative data in the coreset, a confidence-based coreset selection is presented in an online manner. The experimental results on standard benchmarks show that the proposed method significantly outperforms state-of-art continual learning algorithms.},
	language = {en},
	urldate = {2023-05-10},
	journal = {International Journal of Computer Vision},
	author = {Kong, Yajing and Liu, Liu and Qiao, Maoying and Wang, Zhen and Tao, Dacheng},
	month = apr,
	year = {2023},
	keywords = {Catastrophic forgetting, Deep learning, Online continual learning, Trust-region},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/5CQ44HV4/Kong et al. - 2023 - Trust-Region Adaptive Frequency for Online Continu.pdf:application/pdf},
}

@misc{wang_understanding_2022,
	title = {Understanding {Contrastive} {Representation} {Learning} through {Alignment} and {Uniformity} on the {Hypersphere}},
	url = {http://arxiv.org/abs/2005.10242},
	abstract = {Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets conﬁrm the strong agreement between both metrics and downstream task performance. Directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. Project Page: ssnl.github.io/hypersphere.},
	language = {en},
	urldate = {2023-05-10},
	publisher = {arXiv},
	author = {Wang, Tongzhou and Isola, Phillip},
	month = aug,
	year = {2022},
	note = {arXiv:2005.10242 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Wang and Isola - 2022 - Understanding Contrastive Representation Learning .pdf:/Users/nicolas/Zotero/storage/7YNRW24P/Wang and Isola - 2022 - Understanding Contrastive Representation Learning .pdf:application/pdf},
}

@article{michel_banner_nodate,
	title = {Banner {Click} {Through} {Rate} {Classification} {Using} {Deep} {Convolutional} {Neural} {Network}},
	author = {MICHEL, Nicolas and SAKATA, Hayato and KURITA, Keita and YAMASAKI, Toshihiko},
}

@inproceedings{grill_bootstrap_2020-1,
	title = {Bootstrap {Your} {Own} {Latent} - {A} {New} {Approach} to {Self}-{Supervised} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {21271--21284},
}

@inproceedings{koh_online_2023,
  title={Online Boundary-Free Continual Learning by Scheduled Data Prior},
  author={Koh, Hyunseo and Seo, Minhyuk and Bang, Jihwan and Song, Hwanjun and Hong, Deokki and Park, Seulki and Ha, Jung-Woo and Choi, Jonghyun},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{han_online_2022,
	title = {Online {Continual} {Learning} via the {Meta}-learning update with {Multi}-scale {Knowledge} {Distillation} and {Data} {Augmentation}},
	volume = {113},
	issn = {0952-1976},
	language = {en},
	urldate = {2023-05-17},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Han, Ya-nan and Liu, Jian-wei},
	month = aug,
	year = {2022},
	keywords = {Continual learning, Data augmentation, Knowledge distillation, Meta-learning, The stability–plasticity dilemma},
}

@misc{zhou_deep_2023,
	title = {Deep {Class}-{Incremental} {Learning}: {A} {Survey}},
	shorttitle = {Deep {Class}-{Incremental} {Learning}},
	url = {http://arxiv.org/abs/2302.03648},
	doi = {10.48550/arXiv.2302.03648},
	abstract = {Deep models, e.g., CNNs and Vision Transformers, have achieved impressive achievements in many vision tasks in the closed world. However, novel classes emerge from time to time in our ever-changing world, requiring a learning system to acquire new knowledge continually. For example, a robot needs to understand new instructions, and an opinion monitoring system should analyze emerging topics every day. Class-Incremental Learning (CIL) enables the learner to incorporate the knowledge of new classes incrementally and build a universal classifier among all seen classes. Correspondingly, when directly training the model with new class instances, a fatal problem occurs -- the model tends to catastrophically forget the characteristics of former ones, and its performance drastically degrades. There have been numerous efforts to tackle catastrophic forgetting in the machine learning community. In this paper, we survey comprehensively recent advances in deep class-incremental learning and summarize these methods from three aspects, i.e., data-centric, model-centric, and algorithm-centric. We also provide a rigorous and unified evaluation of 16 methods in benchmark image classification tasks to find out the characteristics of different algorithms empirically. Furthermore, we notice that the current comparison protocol ignores the influence of memory budget in model storage, which may result in unfair comparison and biased results. Hence, we advocate fair comparison by aligning the memory budget in evaluation, as well as several memory-agnostic performance measures. The source code to reproduce these evaluations is available at https://github.com/zhoudw-zdw/CIL\_Survey/},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhou, Da-Wei and Wang, Qi-Wei and Qi, Zhi-Hong and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03648 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/CSTZTI3M/Zhou et al. - 2023 - Deep Class-Incremental Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/GHQTY5KH/2302.html:text/html},
}

@misc{zhou_online_2022,
	title = {Online {Continual} {Adaptation} with {Active} {Self}-{Training}},
	url = {http://arxiv.org/abs/2106.06526},
	doi = {10.48550/arXiv.2106.06526},
	abstract = {Models trained with offline data often suffer from continual distribution shifts and expensive labeling in changing environments. This calls for a new online learning paradigm where the learner can continually adapt to changing environments with limited labels. In this paper, we propose a new online setting -- Online Active Continual Adaptation, where the learner aims to continually adapt to changing distributions using both unlabeled samples and active queries of limited labels. To this end, we propose Online Self-Adaptive Mirror Descent (OSAMD), which adopts an online teacher-student structure to enable online self-training from unlabeled data, and a margin-based criterion that decides whether to query the labels to track changing distributions. Theoretically, we show that, in the separable case, OSAMD has an \$O(\{T\}{\textasciicircum}\{2/3\})\$ dynamic regret bound under mild assumptions, which is aligned with the \${\textbackslash}Omega(T{\textasciicircum}\{2/3\})\$ lower bound of online learning algorithms with full labels. In the general case, we show a regret bound of \$O(\{T\}{\textasciicircum}\{2/3\} + {\textbackslash}alpha{\textasciicircum}* T)\$, where \${\textbackslash}alpha{\textasciicircum}*\$ denotes the separability of domains and is usually small. Our theoretical results show that OSAMD can fast adapt to changing environments with active queries. Empirically, we demonstrate that OSAMD achieves favorable regrets under changing environments with limited labels on both simulated and real-world data, which corroborates our theoretical findings.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhou, Shiji and Zhao, Han and Zhang, Shanghang and Wang, Lianzhe and Chang, Heng and Wang, Zhi and Zhu, Wenwu},
	month = mar,
	year = {2022},
	note = {arXiv:2106.06526 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/TRTS23HB/Zhou et al. - 2022 - Online Continual Adaptation with Active Self-Train.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/V2AG27KT/2106.html:text/html},
}

@misc{zhao_maintaining_2019,
	title = {Maintaining {Discrimination} and {Fairness} in {Class} {Incremental} {Learning}},
	url = {http://arxiv.org/abs/1911.07053},
	abstract = {Deep neural networks (DNNs) have been applied in class incremental learning, which aims to solve common realworld problems of learning new classes continually. One drawback of standard DNNs is that they are prone to catastrophic forgetting. Knowledge distillation (KD) is a commonly used technique to alleviate this problem. In this paper, we demonstrate it can indeed help the model to output more discriminative results within old classes. However, it cannot alleviate the problem that the model tends to classify objects into new classes, causing the positive effect of KD to be hidden and limited. We observed that an important factor causing catastrophic forgetting is that the weights in the last fully connected (FC) layer are highly biased in class incremental learning. In this paper, we propose a simple and effective solution motivated by the aforementioned observations to address catastrophic forgetting. Firstly, we utilize KD to maintain the discrimination within old classes. Then, to further maintain the fairness between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal training process. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings. Experimental results show that the proposed method can effectively alleviate catastrophic forgetting and signiﬁcantly outperform state-of-the-art methods.},
	language = {en},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhao, Bowen and Xiao, Xi and Gan, Guojun and Zhang, Bin and Xia, Shutao},
	month = nov,
	year = {2019},
	note = {arXiv:1911.07053 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhao et al. - 2019 - Maintaining Discrimination and Fairness in Class I.pdf:/Users/nicolas/Zotero/storage/UFMSZX4G/Zhao et al. - 2019 - Maintaining Discrimination and Fairness in Class I.pdf:application/pdf},
}

@misc{zhou_model_2023,
	title = {A {Model} or 603 {Exemplars}: {Towards} {Memory}-{Efficient} {Class}-{Incremental} {Learning}},
	shorttitle = {A {Model} or 603 {Exemplars}},
	url = {http://arxiv.org/abs/2205.13218},
	abstract = {Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO's competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO},
	language = {en},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhou, Da-Wei and Wang, Qi-Wei and Ye, Han-Jia and Zhan, De-Chuan},
	month = feb,
	year = {2023},
	note = {arXiv:2205.13218 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhou et al. - 2023 - A Model or 603 Exemplars Towards Memory-Efficient.pdf:/Users/nicolas/Zotero/storage/3SSE8E55/Zhou et al. - 2023 - A Model or 603 Exemplars Towards Memory-Efficient.pdf:application/pdf},
}

@inproceedings{wu_large_2019,
  title={Large scale incremental learning},
  author={Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={374--382},
  year={2019}
}


@misc{zhou_model_2023-1,
	title = {A {Model} or 603 {Exemplars}: {Towards} {Memory}-{Efficient} {Class}-{Incremental} {Learning}},
	shorttitle = {A {Model} or 603 {Exemplars}},
	url = {http://arxiv.org/abs/2205.13218},
	abstract = {Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO's competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO},
	language = {en},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhou, Da-Wei and Wang, Qi-Wei and Ye, Han-Jia and Zhan, De-Chuan},
	month = feb,
	year = {2023},
	note = {arXiv:2205.13218 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhou et al. - 2023 - A Model or 603 Exemplars Towards Memory-Efficient.pdf:/Users/nicolas/Zotero/storage/3AZ2SX92/Zhou et al. - 2023 - A Model or 603 Exemplars Towards Memory-Efficient.pdf:application/pdf},
}

@article{he_incremental_nodate,
	title = {Incremental {Learning} in {Online} {Scenario}},
	abstract = {Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-speciﬁc data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modiﬁed cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art ofﬂine incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classiﬁcation based on our complete framework using the Food-101 dataset.},
	language = {en},
	author = {He, Jiangpeng and Mao, Runyu and Shao, Zeman and Zhu, Fengqing},
	file = {He et al. - Incremental Learning in Online Scenario.pdf:/Users/nicolas/Zotero/storage/FUPT5X28/He et al. - Incremental Learning in Online Scenario.pdf:application/pdf},
}

@article{he_incremental_nodate-1,
	title = {Incremental {Learning} in {Online} {Scenario}},
	abstract = {Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-speciﬁc data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modiﬁed cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art ofﬂine incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classiﬁcation based on our complete framework using the Food-101 dataset.},
	language = {en},
	author = {He, Jiangpeng and Mao, Runyu and Shao, Zeman and Zhu, Fengqing},
	file = {He et al. - Incremental Learning in Online Scenario.pdf:/Users/nicolas/Zotero/storage/SHYJH3P4/He et al. - Incremental Learning in Online Scenario.pdf:application/pdf},
}

@misc{jung_new_2023,
	title = {New {Insights} for the {Stability}-{Plasticity} {Dilemma} in {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2302.08741},
	doi = {10.48550/arXiv.2302.08741},
	abstract = {The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Jung, Dahuin and Lee, Dongjin and Hong, Sunwon and Jang, Hyemi and Bae, Ho and Yoon, Sungroh},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08741 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/2XD7SH22/Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/Q8Y3AM5Q/2302.html:text/html},
}

@misc{zhang_simple_2022,
	title = {A simple but strong baseline for online continual learning: {Repeated} {Augmented} {Rehearsal}},
	shorttitle = {A simple but strong baseline for online continual learning},
	url = {http://arxiv.org/abs/2209.13917},
	abstract = {Online continual learning (OCL) aims to train neural networks incrementally from a non-stationary data stream with a single pass through data. Rehearsal-based methods attempt to approximate the observed input distributions over time with a small memory and revisit them later to avoid forgetting. Despite their strong empirical performance, rehearsal methods still suffer from a poor approximation of past data’s loss landscape with memory samples. This paper revisits the rehearsal dynamics in online settings. We provide theoretical insights on the inherent memory overﬁtting risk from the viewpoint of biased and dynamic empirical risk minimization, and examine the merits and limits of repeated rehearsal. Inspired by our analysis, a simple and intuitive baseline, repeated augmented rehearsal (RAR), is designed to address the underﬁtting-overﬁtting dilemma of online rehearsal. Surprisingly, across four rather different OCL benchmarks, this simple baseline outperforms vanilla rehearsal by 9\%-17\% and also signiﬁcantly improves the state-of-the-art rehearsal-based methods MIR, ASER, and SCR. We also demonstrate that RAR successfully achieves an accurate approximation of the loss landscape of past data and high-loss ridge aversion in its learning trajectory. Extensive ablation studies are conducted to study the interplay between repeated and augmented rehearsal, and reinforcement learning (RL) is applied to dynamically adjust the hyperparameters of RAR to balance the stability-plasticity trade-off online. Code is available at https://github.com/YaqianZhang/RepeatedAugmentedRehearsal.},
	language = {en},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Zhang, Yaqian and Pfahringer, Bernhard and Frank, Eibe and Bifet, Albert and Lim, Nick Jin Sean and Jia, Yunzhe},
	month = nov,
	year = {2022},
	note = {arXiv:2209.13917 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zhang et al. - 2022 - A simple but strong baseline for online continual .pdf:/Users/nicolas/Zotero/storage/AMS6R3TU/Zhang et al. - 2022 - A simple but strong baseline for online continual .pdf:application/pdf},
}

@misc{zhang_simple_2022-1,
	title = {A simple but strong baseline for online continual learning: {Repeated} {Augmented} {Rehearsal}},
	shorttitle = {A simple but strong baseline for online continual learning},
	url = {http://arxiv.org/abs/2209.13917},
	abstract = {Online continual learning (OCL) aims to train neural networks incrementally from a non-stationary data stream with a single pass through data. Rehearsal-based methods attempt to approximate the observed input distributions over time with a small memory and revisit them later to avoid forgetting. Despite their strong empirical performance, rehearsal methods still suffer from a poor approximation of past data’s loss landscape with memory samples. This paper revisits the rehearsal dynamics in online settings. We provide theoretical insights on the inherent memory overﬁtting risk from the viewpoint of biased and dynamic empirical risk minimization, and examine the merits and limits of repeated rehearsal. Inspired by our analysis, a simple and intuitive baseline, repeated augmented rehearsal (RAR), is designed to address the underﬁtting-overﬁtting dilemma of online rehearsal. Surprisingly, across four rather different OCL benchmarks, this simple baseline outperforms vanilla rehearsal by 9\%-17\% and also signiﬁcantly improves the state-of-the-art rehearsal-based methods MIR, ASER, and SCR. We also demonstrate that RAR successfully achieves an accurate approximation of the loss landscape of past data and high-loss ridge aversion in its learning trajectory. Extensive ablation studies are conducted to study the interplay between repeated and augmented rehearsal, and reinforcement learning (RL) is applied to dynamically adjust the hyperparameters of RAR to balance the stability-plasticity trade-off online. Code is available at https://github.com/YaqianZhang/RepeatedAugmentedRehearsal.},
	language = {en},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Zhang, Yaqian and Pfahringer, Bernhard and Frank, Eibe and Bifet, Albert and Lim, Nick Jin Sean and Jia, Yunzhe},
	month = nov,
	year = {2022},
	note = {arXiv:2209.13917 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zhang et al. - 2022 - A simple but strong baseline for online continual .pdf:/Users/nicolas/Zotero/storage/4R6GKD4E/Zhang et al. - 2022 - A simple but strong baseline for online continual .pdf:application/pdf},
}

@misc{zhao_decoupled_2022,
	title = {Decoupled {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2203.08679},
	doi = {10.48550/arXiv.2203.08679},
	abstract = {State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the "difficulty" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megvii-research/mdistiller.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
	month = jul,
	year = {2022},
	note = {arXiv:2203.08679 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/GTLMD248/Zhao et al. - 2022 - Decoupled Knowledge Distillation.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/B2XQ5PGC/2203.html:text/html},
}

@misc{sachdeva_data_2023,
	title = {Data {Distillation}: {A} {Survey}},
	shorttitle = {Data {Distillation}},
	url = {http://arxiv.org/abs/2301.04272},
	abstract = {The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as eﬀective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for diﬀerent data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.},
	language = {en},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Sachdeva, Noveen and McAuley, Julian},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04272 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
	file = {Sachdeva and McAuley - 2023 - Data Distillation A Survey.pdf:/Users/nicolas/Zotero/storage/DC2D2E7Y/Sachdeva and McAuley - 2023 - Data Distillation A Survey.pdf:application/pdf},
}

@misc{zhao_decoupled_2022-1,
	title = {Decoupled {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2203.08679},
	doi = {10.48550/arXiv.2203.08679},
	abstract = {State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the "difficulty" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megvii-research/mdistiller.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
	month = jul,
	year = {2022},
	note = {arXiv:2203.08679 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/SAACCV9B/Zhao et al. - 2022 - Decoupled Knowledge Distillation.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/WJ83EVFK/2203.html:text/html},
}

@article{dong_class-incremental_2023,
	title = {Class-incremental object detection},
	volume = {139},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320323001887},
	doi = {10.1016/j.patcog.2023.109488},
	abstract = {Deep learning architectures have shown remarkable results in the object detection task. However, they experience a critical performance drop when they are required to learn new classes incrementally without forgetting old ones. This catastrophic forgetting phenomenon impedes the deployment of artificial intelligence in real word scenarios where systems need to learn new and different representations over time. Recently, many incremental learning methods have been proposed to avoid the catastrophic forgetting problem. However, current state-of-the-art class-incremental learning strategies aim at preserving the knowledge of old classes while learning new ones sequentially, which would encounter other problems as follows: (1) In the process of preserving information of old classes, only a small portion of data in the previous tasks are kept and replayed during training, which inevitably incurs bias that is favorable for the new classes but malicious to the old classes. (2) With the knowledge of previous classes distilled into the new model, a sub-optimal solution for the new task is obtained since the preserving process of previous classes sabotages the training of new classes. To address these issues, termed as Information Asymmetry (IA), we propose a double-head framework which preserves the knowledge of old classes and learns the knowledge of new classes separately. Specifically, we transfer the knowledge of the previous model to the current learned one for overcoming the catastrophic forgetting problem. Furthermore, considering that IA would introduce impacts on the training of the new model, we propose a Non-Affection mask to distill the knowledge of the interested regions at the feature level. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods on PASCAL VOC and MS COCO datasets.},
	language = {en},
	urldate = {2023-05-19},
	journal = {Pattern Recognition},
	author = {Dong, Na and Zhang, Yongqiang and Ding, Mingli and Bai, Yancheng},
	month = jul,
	year = {2023},
	keywords = {Deep learning, Class-incremental learning, Information asymmetry, Non-affection distillation, Object detection},
	pages = {109488},
	file = {ScienceDirect Snapshot:/Users/nicolas/Zotero/storage/UEQY6FSY/S0031320323001887.html:text/html},
}

@misc{wang_positive_2022,
	title = {Positive {Pair} {Distillation} {Considered} {Harmful}: {Continual} {Meta} {Metric} {Learning} for {Lifelong} {Object} {Re}-{Identification}},
	shorttitle = {Positive {Pair} {Distillation} {Considered} {Harmful}},
	url = {http://arxiv.org/abs/2210.01600},
	doi = {10.48550/arXiv.2210.01600},
	abstract = {Lifelong object re-identification incrementally learns from a stream of re-identification tasks. The objective is to learn a representation that can be applied to all tasks and that generalizes to previously unseen re-identification tasks. The main challenge is that at inference time the representation must generalize to previously unseen identities. To address this problem, we apply continual meta metric learning to lifelong object re-identification. To prevent forgetting of previous tasks, we use knowledge distillation and explore the roles of positive and negative pairs. Based on our observation that the distillation and metric losses are antagonistic, we propose to remove positive pairs from distillation to robustify model updates. Our method, called Distillation without Positive Pairs (DwoPP), is evaluated on extensive intra-domain experiments on person and vehicle re-identification datasets, as well as inter-domain experiments on the LReID benchmark. Our experiments demonstrate that DwoPP significantly outperforms the state-of-the-art. The code is here: https://github.com/wangkai930418/DwoPP\_code},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Wang, Kai and Wu, Chenshen and Bagdanov, Andy and Liu, Xialei and Yang, Shiqi and Jui, Shangling and van de Weijer, Joost},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01600 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/7WVHHUEQ/Wang et al. - 2022 - Positive Pair Distillation Considered Harmful Con.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IIDAW46X/2210.html:text/html},
}

@article{gou_multi-target_2023,
	title = {Multi-target {Knowledge} {Distillation} via {Student} {Self}-reflection},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-023-01792-z},
	doi = {10.1007/s11263-023-01792-z},
	abstract = {Knowledge distillation is a simple yet effective technique for deep model compression, which aims to transfer the knowledge learned by a large teacher model to a small student model. To mimic how the teacher teaches the student, existing knowledge distillation methods mainly adapt an unidirectional knowledge transfer, where the knowledge extracted from different intermedicate layers of the teacher model is used to guide the student model. However, it turns out that the students can learn more effectively through multi-stage learning with a self-reflection in the real-world education scenario, which is nevertheless ignored by current knowledge distillation methods. Inspired by this, we devise a new knowledge distillation framework entitled multi-target knowledge distillation via student self-reflection or MTKD-SSR, which can not only enhance the teacher’s ability in unfolding the knowledge to be distilled, but also improve the student’s capacity of digesting the knowledge. Specifically, the proposed framework consists of three target knowledge distillation mechanisms: a stage-wise channel distillation (SCD), a stage-wise response distillation (SRD), and a cross-stage review distillation (CRD), where SCD and SRD transfer feature-based knowledge (i.e., channel features) and response-based knowledge (i.e., logits) at different stages, respectively; and CRD encourages the student model to conduct self-reflective learning after each stage by a self-distillation of the response-based knowledge. Experimental results on five popular visual recognition datasets, CIFAR-100, Market-1501, CUB200-2011, ImageNet, and Pascal VOC, demonstrate that the proposed framework significantly outperforms recent state-of-the-art knowledge distillation methods.},
	language = {en},
	urldate = {2023-05-19},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Xiong, Xiangshuo and Yu, Baosheng and Du, Lan and Zhan, Yibing and Tao, Dacheng},
	month = apr,
	year = {2023},
	keywords = {Deep learning, Knowledge distillation, Model compression, Self-reflection learning},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/7QMB2ARF/Gou et al. - 2023 - Multi-target Knowledge Distillation via Student Se.pdf:application/pdf},
}

@misc{verwimp_rehearsal_2021,
	title = {Rehearsal revealed: {The} limits and merits of revisiting samples in continual learning},
	shorttitle = {Rehearsal revealed},
	url = {http://arxiv.org/abs/2104.07446},
	doi = {10.48550/arXiv.2104.07446},
	abstract = {Learning from non-stationary data streams and overcoming catastrophic forgetting still poses a serious challenge for machine learning research. Rather than aiming to improve state-of-the-art, in this work we provide insight into the limits and merits of rehearsal, one of continual learning's most established methods. We hypothesize that models trained sequentially with rehearsal tend to stay in the same low-loss region after a task has finished, but are at risk of overfitting on its sample memory, hence harming generalization. We provide both conceptual and strong empirical evidence on three benchmarks for both behaviors, bringing novel insights into the dynamics of rehearsal and continual learning in general. Finally, we interpret important continual learning works in the light of our findings, allowing for a deeper understanding of their successes.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Verwimp, Eli and De Lange, Matthias and Tuytelaars, Tinne},
	month = apr,
	year = {2021},
	note = {arXiv:2104.07446 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/SC95QJ2R/2104.html:text/html;Verwimp et al_2021_Rehearsal revealed.pdf:/Users/nicolas/Documents/Zotero/Verwimp et al_2021_Rehearsal revealed.pdf:application/pdf},
}

@inproceedings{ahn_ss-il_2021,
	title = {{SS}-{IL}: {Separated} {Softmax} for {Incremental} {Learning}},
	shorttitle = {{SS}-{IL}},
	language = {en},
	urldate = {2023-05-31},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Ahn, Hongjoon and Kwak, Jihwan and Lim, Subin and Bang, Hyeonsu and Kim, Hyojun and Moon, Taesup},
	year = {2021},
}

@misc{pernici_class-incremental_2020,
	title = {Class-incremental {Learning} with {Pre}-allocated {Fixed} {Classifiers}},
	url = {http://arxiv.org/abs/2010.08657},
	abstract = {In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the ﬁnal classiﬁer nodes to accommodate the new classes.},
	language = {en},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Pernici, Federico and Bruni, Matteo and Baecchi, Claudio and Turchini, Francesco and Del Bimbo, Alberto},
	month = oct,
	year = {2020},
	note = {arXiv:2010.08657 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Pernici et al. - 2020 - Class-incremental Learning with Pre-allocated Fixe.pdf:/Users/nicolas/Zotero/storage/RJ5QUHV3/Pernici et al. - 2020 - Class-incremental Learning with Pre-allocated Fixe.pdf:application/pdf},
}

@misc{pernici_class-incremental_2020-1,
	title = {Class-incremental {Learning} with {Pre}-allocated {Fixed} {Classifiers}},
	url = {http://arxiv.org/abs/2010.08657},
	abstract = {In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the ﬁnal classiﬁer nodes to accommodate the new classes.},
	language = {en},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Pernici, Federico and Bruni, Matteo and Baecchi, Claudio and Turchini, Francesco and Del Bimbo, Alberto},
	month = oct,
	year = {2020},
	note = {arXiv:2010.08657 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Pernici et al. - 2020 - Class-incremental Learning with Pre-allocated Fixe.pdf:/Users/nicolas/Zotero/storage/IY7Y4TR4/Pernici et al. - 2020 - Class-incremental Learning with Pre-allocated Fixe.pdf:application/pdf},
}

@article{zhao_rethinking_nodate,
	title = {Rethinking {Gradient} {Projection} {Continual} {Learning}: {Stability} / {Plasticity} {Feature} {Space} {Decoupling}},
	abstract = {Continual learning aims to incrementally learn novel classes over time, while not forgetting the learned knowledge. Recent studies have found that learning would not forget if the updated gradient is orthogonal to the feature space. However, previous approaches require the gradient to be fully orthogonal to the whole feature space, leading to poor plasticity, as the feasible gradient direction becomes narrow when the tasks continually come, i.e., feature space is unlimitedly expanded. In this paper, we propose a space decoupling (SD) algorithm to decouple the feature space into a pair of complementary subspaces, i.e., the stability space I, and the plasticity space R. I is established by conducting space intersection between the historic and current feature space, and thus I contains more task-shared bases. R is constructed by seeking the orthogonal complementary subspace of I, and thus R mainly contains taskspecific bases. By putting distinguishing constraints on R and I, our method achieves a better balance between stability and plasticity. Extensive experiments are conducted by applying SD to gradient projection baselines, and show SD is model-agnostic and achieves SOTA results on publicly available datasets.},
	language = {en},
	author = {Zhao, Zhen and Zhang, Zhizhong and Tan, Xin and Liu, Jun and Qu, Yanyun and Xie, Yuan and Ma, Lizhuang},
	file = {Zhao et al. - Rethinking Gradient Projection Continual Learning.pdf:/Users/nicolas/Zotero/storage/X43ERQWE/Zhao et al. - Rethinking Gradient Projection Continual Learning.pdf:application/pdf},
}

@article{zhao_rethinking_nodate-1,
	title = {Rethinking {Gradient} {Projection} {Continual} {Learning}: {Stability} / {Plasticity} {Feature} {Space} {Decoupling}},
	abstract = {Continual learning aims to incrementally learn novel classes over time, while not forgetting the learned knowledge. Recent studies have found that learning would not forget if the updated gradient is orthogonal to the feature space. However, previous approaches require the gradient to be fully orthogonal to the whole feature space, leading to poor plasticity, as the feasible gradient direction becomes narrow when the tasks continually come, i.e., feature space is unlimitedly expanded. In this paper, we propose a space decoupling (SD) algorithm to decouple the feature space into a pair of complementary subspaces, i.e., the stability space I, and the plasticity space R. I is established by conducting space intersection between the historic and current feature space, and thus I contains more task-shared bases. R is constructed by seeking the orthogonal complementary subspace of I, and thus R mainly contains taskspecific bases. By putting distinguishing constraints on R and I, our method achieves a better balance between stability and plasticity. Extensive experiments are conducted by applying SD to gradient projection baselines, and show SD is model-agnostic and achieves SOTA results on publicly available datasets.},
	language = {en},
	author = {Zhao, Zhen and Zhang, Zhizhong and Tan, Xin and Liu, Jun and Qu, Yanyun and Xie, Yuan and Ma, Lizhuang},
	file = {Zhao et al. - Rethinking Gradient Projection Continual Learning.pdf:/Users/nicolas/Zotero/storage/CV9U3TFK/Zhao et al. - Rethinking Gradient Projection Continual Learning.pdf:application/pdf},
}

@inproceedings{das_weakly-supervised_2023,
	title = {Weakly-{Supervised} {Domain} {Adaptive} {Semantic} {Segmentation} {With} {Prototypical} {Contrastive} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Das_Weakly-Supervised_Domain_Adaptive_Semantic_Segmentation_With_Prototypical_Contrastive_Learning_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	author = {Das, Anurag and Xian, Yongqin and Dai, Dengxin and Schiele, Bernt},
	year = {2023},
	pages = {15434--15443},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/CGHZLVCC/Das et al. - 2023 - Weakly-Supervised Domain Adaptive Semantic Segment.pdf:application/pdf},
}

@misc{jung_new_2023-1,
	title = {New {Insights} for the {Stability}-{Plasticity} {Dilemma} in {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2302.08741},
	abstract = {The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.},
	language = {en},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Jung, Dahuin and Lee, Dongjin and Hong, Sunwon and Jang, Hyemi and Bae, Ho and Yoon, Sungroh},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08741 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:/Users/nicolas/Zotero/storage/9YIKJSMR/Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:application/pdf},
}

@misc{jung_new_2023-2,
	title = {New {Insights} for the {Stability}-{Plasticity} {Dilemma} in {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2302.08741},
	abstract = {The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.},
	language = {en},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Jung, Dahuin and Lee, Dongjin and Hong, Sunwon and Jang, Hyemi and Bae, Ho and Yoon, Sungroh},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08741 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:/Users/nicolas/Zotero/storage/KN9FX88A/Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma .pdf:application/pdf},
}

@misc{gu_summarizing_2023,
	title = {Summarizing {Stream} {Data} for {Memory}-{Restricted} {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2305.16645},
	doi = {10.48550/arXiv.2305.16645},
	abstract = {Replay-based methods have proved their effectiveness on online continual learning by rehearsing past samples from an auxiliary memory. With many efforts made on improving training schemes based on the memory, however, the information carried by each sample in the memory remains under-investigated. Under circumstances with restricted storage space, the informativeness of the memory becomes critical for effective replay. Although some works design specific strategies to select representative samples, by only employing original images, the storage space is still not well utilized. To this end, we propose to Summarize the knowledge from the Stream Data (SSD) into more informative samples by distilling the training characteristics of real images. Through maintaining the consistency of training gradients and relationship to the past tasks, the summarized samples are more representative for the stream data compared to the original images. Extensive experiments are conducted on multiple online continual learning benchmarks to support that the proposed SSD method significantly enhances the replay effects. We demonstrate that with limited extra computational overhead, SSD provides more than 3\% accuracy boost for sequential CIFAR-100 under extremely restricted memory buffer. The code is available in https://github.com/vimar-gu/SSD.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Gu, Jianyang and Wang, Kai and Jiang, Wei and You, Yang},
	month = may,
	year = {2023},
	note = {arXiv:2305.16645 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/LRL94QU4/Gu et al. - 2023 - Summarizing Stream Data for Memory-Restricted Onli.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/65SQYQBP/2305.html:text/html},
}

@misc{lee_class_2023,
	title = {Class {Conditional} {Gaussians} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/2305.19076},
	doi = {10.48550/arXiv.2305.19076},
	abstract = {Dealing with representation shift is one of the main problems in online continual learning. Current methods mainly solve this by reducing representation shift, but leave the classifier on top of the representation to slowly adapt, in many update steps, to the remaining representation shift, increasing forgetting. We propose DeepCCG, an empirical Bayesian approach to solve this problem. DeepCCG works by updating the posterior of a class conditional Gaussian classifier such that the classifier adapts instantly to representation shift. The use of a class conditional Gaussian classifier also enables DeepCCG to use a log conditional marginal likelihood loss to update the representation, which can be seen as a new type of replay. To perform the update to the classifier and representation, DeepCCG maintains a fixed number of examples in memory and so a key part of DeepCCG is selecting what examples to store, choosing the subset that minimises the KL divergence between the true posterior and the posterior induced by the subset. We demonstrate the performance of DeepCCG on a range of settings, including those with overlapping tasks which thus far have been under-explored. In the experiments, DeepCCG outperforms all other methods, evidencing its potential.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Lee, Thomas L. and Storkey, Amos},
	month = may,
	year = {2023},
	note = {arXiv:2305.19076 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nicolas/Zotero/storage/QMVVG88N/Lee and Storkey - 2023 - Class Conditional Gaussians for Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/nicolas/Zotero/storage/TVSCWIJS/2305.html:text/html},
}

@misc{mirzadeh_improved_2019,
	title = {Improved {Knowledge} {Distillation} via {Teacher} {Assistant}},
	url = {http://arxiv.org/abs/1902.03393},
	doi = {10.48550/arXiv.1902.03393},
	abstract = {Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Mirzadeh, Seyed-Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
	month = dec,
	year = {2019},
	note = {arXiv:1902.03393 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/IQRCCTVB/1902.html:text/html;Mirzadeh et al_2019_Improved Knowledge Distillation via Teacher Assistant.pdf:/Users/nicolas/Documents/Zotero/Mirzadeh et al_2019_Improved Knowledge Distillation via Teacher Assistant.pdf:application/pdf},
}

@article{huang_knowledge_nodate,
	title = {Knowledge {Distillation} from {A} {Stronger} {Teacher}},
	abstract = {Unlike existing knowledge distillation methods focus on the baseline settings, where the teacher models and training strategies are not that strong and competing as state-of-the-art approaches, this paper presents a method dubbed DIST to distill better from a stronger teacher. We empirically ﬁnd that the discrepancy of predictions between the student and a stronger teacher may tend to be fairly severer. As a result, the exact match of predictions in KL divergence would disturb the training and make existing methods perform poorly. In this paper, we show that simply preserving the relations between the predictions of teacher and student would sufﬁce, and propose a correlation-based loss to capture the intrinsic inter-class relations from the teacher explicitly. Besides, considering that different instances have different semantic similarities to each class, we also extend this relational match to the intra-class level. Our method is simple yet practical, and extensive experiments demonstrate that it adapts well to various architectures, model sizes and training strategies, and can achieve state-of-the-art performance consistently on image classiﬁcation, object detection, and semantic segmentation tasks. Code is available at: https://github.com/hunto/DIST\_KD.},
	language = {en},
	author = {Huang, Tao and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang},
	file = {Huang et al. - Knowledge Distillation from A Stronger Teacher.pdf:/Users/nicolas/Zotero/storage/E64RDBMQ/Huang et al. - Knowledge Distillation from A Stronger Teacher.pdf:application/pdf},
}

@article{huang_knowledge_nodate-1,
	title = {Knowledge {Distillation} from {A} {Stronger} {Teacher}},
	abstract = {Unlike existing knowledge distillation methods focus on the baseline settings, where the teacher models and training strategies are not that strong and competing as state-of-the-art approaches, this paper presents a method dubbed DIST to distill better from a stronger teacher. We empirically ﬁnd that the discrepancy of predictions between the student and a stronger teacher may tend to be fairly severer. As a result, the exact match of predictions in KL divergence would disturb the training and make existing methods perform poorly. In this paper, we show that simply preserving the relations between the predictions of teacher and student would sufﬁce, and propose a correlation-based loss to capture the intrinsic inter-class relations from the teacher explicitly. Besides, considering that different instances have different semantic similarities to each class, we also extend this relational match to the intra-class level. Our method is simple yet practical, and extensive experiments demonstrate that it adapts well to various architectures, model sizes and training strategies, and can achieve state-of-the-art performance consistently on image classiﬁcation, object detection, and semantic segmentation tasks. Code is available at: https://github.com/hunto/DIST\_KD.},
	language = {en},
	author = {Huang, Tao and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang},
	file = {Huang et al. - Knowledge Distillation from A Stronger Teacher.pdf:/Users/nicolas/Zotero/storage/9EBSTFBD/Huang et al. - Knowledge Distillation from A Stronger Teacher.pdf:application/pdf},
}

@article{phuong_towards_nodate,
	title = {Towards {Understanding} {Knowledge} {Distillation}},
	abstract = {Knowledge distillation, i.e. one classiﬁer being trained on the outputs of another classiﬁer, is an empirically very successful technique for knowledge transfer between classiﬁers. It has even been observed that classiﬁers learn much faster and more reliably if trained with the outputs of another classiﬁer as soft labels, instead of from ground truth data. So far, however, there is no satisfactory theoretical explanation of this phenomenon. In this work, we provide the ﬁrst insights into the working mechanisms of distillation by studying the special case of linear and deep linear classiﬁers. Speciﬁcally, we prove a generalization bound that establishes fast convergence of the expected risk of a distillation-trained linear classiﬁer. From the bound and its proof we extract three key factors that determine the success of distillation: data geometry – geometric properties of the data distribution, in particular class separation, has an immediate inﬂuence on the convergence speed of the risk; optimization bias – gradient descent optimization ﬁnds a very favorable minimum of the distillation objective; and strong monotonicity – the expected risk of the student classiﬁer always decreases when the size of the training set grows.},
	language = {en},
	author = {Phuong, Mary and Lampert, Christoph H},
	file = {Phuong and Lampert - Towards Understanding Knowledge Distillation.pdf:/Users/nicolas/Zotero/storage/7XNXZNTB/Phuong and Lampert - Towards Understanding Knowledge Distillation.pdf:application/pdf},
}

@inproceedings{michel_learning_2023,
  author       = {Nicolas Michel and
                  Giovanni Chierchia and
                  Romain Negrel and
                  Jean{-}Fran{\c{c}}ois Bercher},
  title        = {Learning Representations on the Unit Sphere: Investigating Angular
                  Gaussian and Von Mises-Fisher Distributions for Online Continual Learning},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence},
  pages        = {14350--14358},
  year         = {2024},
}



@misc{abhishek_multi-sample_2022,
	title = {Multi-{Sample} \${\textbackslash}zeta\$-mixup: {Richer}, {More} {Realistic} {Synthetic} {Samples} from a \$p\$-{Series} {Interpolant}},
	shorttitle = {Multi-{Sample} \${\textbackslash}zeta\$-mixup},
	url = {http://arxiv.org/abs/2204.03323},
	abstract = {Modern deep learning training procedures rely on model regularization techniques such as data augmentation methods, which generate training samples that increase the diversity of data and richness of label information. A popular recent method, mixup, uses convex combinations of pairs of original samples to generate new samples. However, as we show in our experiments, mixup can produce undesirable synthetic samples, where the data is sampled off the manifold and can contain incorrect labels. We propose ζ-mixup, a generalization of mixup with provably and demonstrably desirable properties that allows convex combinations of N ≥ 2 samples, leading to more realistic and diverse outputs that incorporate information from N original samples by using a p-series interpolant. We show that, compared to mixup, ζ-mixup better preserves the intrinsic dimensionality of the original datasets, which is a desirable property for training generalizable models. Furthermore, we show that our implementation of ζ-mixup is faster than mixup, and extensive evaluation on controlled synthetic and 24 real-world natural and medical image classiﬁcation datasets shows that ζ-mixup outperforms mixup and traditional data augmentation techniques.},
	language = {en},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Abhishek, Kumar and Brown, Colin J. and Hamarneh, Ghassan},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03323 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Abhishek et al. - 2022 - Multi-Sample \$zeta\$-mixup Richer, More Realistic.pdf:/Users/nicolas/Zotero/storage/YAUAYLWY/Abhishek et al. - 2022 - Multi-Sample \$zeta\$-mixup Richer, More Realistic.pdf:application/pdf},
}

@article{nie_bilateral_nodate,
	title = {Bilateral {Memory} {Consolidation} for {Continual} {Learning}},
	abstract = {Humans are proficient at continuously acquiring and integrating new knowledge. By contrast, deep models forget catastrophically, especially when tackling highly long task sequences. Inspired by the way our brains constantly rewrite and consolidate past recollections, we propose a novel Bilateral Memory Consolidation (BiMeCo) framework that focuses on enhancing memory interaction capabilities. Specifically, BiMeCo explicitly decouples model parameters into short-term memory module and long-term memory module, responsible for representation ability of the model and generalization over all learned tasks, respectively. BiMeCo encourages dynamic interactions between two memory modules by knowledge distillation and momentum-based updating for forming generic knowledge to prevent forgetting. The proposed BiMeCo is parameterefficient and can be integrated into existing methods seamlessly. Extensive experiments on challenging benchmarks show that BiMeCo significantly improves the performance of existing continual learning methods. For example, combined with the state-of-the-art method CwD [55], BiMeCo brings in significant gains of around 2\% to 6\% while using 2x fewer parameters on CIFAR-100 under ResNet-18.},
	language = {en},
	author = {Nie, Xing and Xu, Shixiong and Liu, Xiyan and Meng, Gaofeng and Huo, Chunlei and Xiang, Shiming},
	file = {Nie et al. - Bilateral Memory Consolidation for Continual Learn.pdf:/Users/nicolas/Zotero/storage/UDYXPW6Q/Nie et al. - Bilateral Memory Consolidation for Continual Learn.pdf:application/pdf},
}

@article{zhao_few-shot_nodate,
	title = {Few-{Shot} {Class}-{Incremental} {Learning} via {Class}-{Aware} {Bilateral} {Distillation}},
	abstract = {Few-Shot Class-Incremental Learning (FSCIL) aims to continually learn novel classes based on only few training samples, which poses a more challenging task than the well-studied Class-Incremental Learning (CIL) due to data scarcity. While knowledge distillation, a prevailing technique in CIL, can alleviate the catastrophic forgetting of older classes by regularizing outputs between current and previous model, it fails to consider the overﬁtting risk of novel classes in FSCIL. To adapt the powerful distillation technique for FSCIL, we propose a novel distillation structure, by taking the unique challenge of overﬁtting into account. Concretely, we draw knowledge from two complementary teachers. One is the model trained on abundant data from base classes that carries rich general knowledge, which can be leveraged for easing the overﬁtting of current novel classes. The other is the updated model from last incremental session that contains the adapted knowledge of previous novel classes, which is used for alleviating their forgetting. To combine the guidances, an adaptive strategy conditioned on the class-wise semantic similarities is introduced. Besides, for better preserving base class knowledge when accommodating novel concepts, we adopt a two-branch network with an attention-based aggregation module to dynamically merge predictions from two complementary branches. Extensive experiments on 3 popular FSCIL datasets: mini-ImageNet, CIFAR100 and CUB200 validate the effectiveness of our method by surpassing existing works by a signiﬁcant margin. Code is available at https://github.com/LinglanZhao/BiDistFSCIL.},
	language = {en},
	author = {Zhao, Linglan and Lu, Jing and Xu, Yunlu and Cheng, Zhanzhan and Guo, Dashan and Niu, Yi and Fang, Xiangzhong},
	file = {Zhao et al. - Few-Shot Class-Incremental Learning via Class-Awar.pdf:/Users/nicolas/Zotero/storage/BHNQLAFF/Zhao et al. - Few-Shot Class-Incremental Learning via Class-Awar.pdf:application/pdf},
}

@misc{noauthor_sciencedirectcom_nodate,
	title = {{ScienceDirect}.com {\textbar} {Science}, health and medical journals, full text articles and books.},
	url = {https://www-sciencedirect-com.revproxy.esiee.fr/science/article/pii/S0020025522005576/pdfft?crasolve=1&r=7dc9d68e1af67729&ts=1687660745961&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LXNjaWVuY2VkaXJlY3QtY29tLnJldnByb3h5LmVzaWVlLmZy&tsoh=d3d3LXNjaWVuY2VkaXJlY3QtY29tLnJldnByb3h5LmVzaWVlLmZy&rh=d3d3LXNjaWVuY2VkaXJlY3QtY29tLnJldnByb3h5LmVzaWVlLmZy&re=X2JsYW5rXw%3D%3D&ns_h=d3d3LXNjaWVuY2VkaXJlY3QtY29tLnJldnByb3h5LmVzaWVlLmZy&ns_e=X2JsYW5rXw%3D%3D&rh_fd=rrr(n%5Ed%60i%5E%60_dm%60%5Eo(%5Ejh)m%60qkmjst)%60nd%60%60)am&tsoh_fd=rrr(n%5Ed%60i%5E%60_dm%60%5Eo(%5Ejh)m%60qkmjst)%60nd%60%60)am&iv=82794e0460cc8a04fc082d710e61711b&token=37323838356362633032393064393765663233363263343464373862653839323763623234346662366530653765333063336637396132663931373637336362666135663966316264623630333439396563346261333330356664633a333332353935353333376463343738323165363238623062&text=7ba111d972a69950622e14f6da2eab7e34cc3ca1dc57892a4f4a3c54acb78ef996fcb83024c8b51cc3a96dade25d393b49a16b70dad37ec445e0d5277d4d18f59830755a389e66f2148deb478c94aa1c98cb1e51b0798a920916f220aab350613fb364a50eaf313713373300bde4e091ebc4d20487acec160f866bb1d8d450343a641fdfb60e41eb2b4d9a36389954993378ae5cee97e01ee68bcfa1f59cd77c4c140c6cf42f59e270808dee8a7c906ad432c2a495a2b320ac52cce7cc4604a822d497dd8aeb115a8a48d5b9dbcb374f6440410110ac71e7d3c6bf69e859e54d9dec0b7d06796a6cb1c46e36b59ff325884ad2ea81ddf8702b50bb8f0e254c89a92aa524147ed43ea65a9fa58c3d9649e7d0d3d0a4e5e4079988634dfead7eedadb76c59b3a04d7c3de8e112c76d4899&original=3f6d64353d6133303664653563333732356138353564353335383861303434666632363539267069643d312d73322e302d53303032303032353532323030353537362d6d61696e2e706466},
	urldate = {2023-06-25},
}

@inproceedings{guo_dealing_2023,
  title={Dealing with Cross-Task Class Discrimination in Online Continual Learning},
  author={Guo, Yiduo and Liu, Bing and Zhao, Dongyan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages={11878--11887},
  year={2023}
}


@inproceedings{zhao_rethinking_2023,
	title = {Rethinking {Gradient} {Projection} {Continual} {Learning}: {Stability} / {Plasticity} {Feature} {Space} {Decoupling}},
	shorttitle = {Rethinking {Gradient} {Projection} {Continual} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Rethinking_Gradient_Projection_Continual_Learning_Stability__Plasticity_Feature_Space_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-29},
	author = {Zhao, Zhen and Zhang, Zhizhong and Tan, Xin and Liu, Jun and Qu, Yanyun and Xie, Yuan and Ma, Lizhuang},
	year = {2023},
	pages = {3718--3727},
	file = {Full Text PDF:/Users/nicolas/Zotero/storage/XTRNM7R2/Zhao et al. - 2023 - Rethinking Gradient Projection Continual Learning.pdf:application/pdf},
}

@misc{huang_knowledge_2022,
	title = {Knowledge {Distillation} from {A} {Stronger} {Teacher}},
	url = {http://arxiv.org/abs/2205.10536},
	abstract = {Unlike existing knowledge distillation methods focus on the baseline settings, where the teacher models and training strategies are not that strong and competing as state-of-the-art approaches, this paper presents a method dubbed DIST to distill better from a stronger teacher. We empirically ﬁnd that the discrepancy of predictions between the student and a stronger teacher may tend to be fairly severer. As a result, the exact match of predictions in KL divergence would disturb the training and make existing methods perform poorly. In this paper, we show that simply preserving the relations between the predictions of teacher and student would sufﬁce, and propose a correlation-based loss to capture the intrinsic inter-class relations from the teacher explicitly. Besides, considering that different instances have different semantic similarities to each class, we also extend this relational match to the intra-class level. Our method is simple yet practical, and extensive experiments demonstrate that it adapts well to various architectures, model sizes and training strategies, and can achieve state-of-the-art performance consistently on image classiﬁcation, object detection, and semantic segmentation tasks. Code is available at: https://github.com/hunto/DIST\_KD.},
	language = {en},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Huang, Tao and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang},
	month = dec,
	year = {2022},
	note = {arXiv:2205.10536 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Huang et al. - 2022 - Knowledge Distillation from A Stronger Teacher.pdf:/Users/nicolas/Zotero/storage/T4QVDYVP/Huang et al. - 2022 - Knowledge Distillation from A Stronger Teacher.pdf:application/pdf},
}

@inproceedings{soutif--cormerais_improving_2023,
  title={Improving Online Continual Learning Performance and Stability with Temporal Ensembles},
  author={Soutif--Cormerais, Albin and Carta, Antonio and Van de Weijer, Joost},
  booktitle={Conference on Lifelong Learning Agents},
  pages={828--845},
  year={2023},
}

@misc{bai_recent_2021,
	title = {Recent {Advances} in {Adversarial} {Training} for {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2102.01356},
	abstract = {Adversarial training is one of the most effective approaches to defending deep learning models against adversarial examples. Unlike other defense strategies, adversarial training aims to enhance the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the ﬁrst time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives and highlight the challenges which are not fully tackled. Finally, we present potential future directions.},
	language = {en},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
	month = apr,
	year = {2021},
	note = {arXiv:2102.01356 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Bai et al. - 2021 - Recent Advances in Adversarial Training for Advers.pdf:/Users/nicolas/Zotero/storage/7LNAHKTN/Bai et al. - 2021 - Recent Advances in Adversarial Training for Advers.pdf:application/pdf},
}

@misc{kwon_enhancing_2023,
	title = {Enhancing {Accuracy} and {Robustness} through {Adversarial} {Training} in {Class} {Incremental} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2305.13678},
	doi = {10.48550/arXiv.2305.13678},
	abstract = {In real life, adversarial attack to deep learning models is a fatal security issue. However, the issue has been rarely discussed in a widely used class-incremental continual learning (CICL). In this paper, we address problems of applying adversarial training to CICL, which is well-known defense method against adversarial attack. A well-known problem of CICL is class-imbalance that biases a model to the current task by a few samples of previous tasks. Meeting with the adversarial training, the imbalance causes another imbalance of attack trials over tasks. Lacking clean data of a minority class by the class-imbalance and increasing of attack trials from a majority class by the secondary imbalance, adversarial training distorts optimal decision boundaries. The distortion eventually decreases both accuracy and robustness than adversarial training. To exclude the effects, we propose a straightforward but significantly effective method, External Adversarial Training (EAT) which can be applied to methods using experience replay. This method conduct adversarial training to an auxiliary external model for the current task data at each time step, and applies generated adversarial examples to train the target model. We verify the effects on a toy problem and show significance on CICL benchmarks of image classification. We expect that the results will be used as the first baseline for robustness research of CICL.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Kwon, Minchan and Kim, Kangil},
	month = may,
	year = {2023},
	note = {arXiv:2305.13678 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/nicolas/Zotero/storage/AF795TI4/2305.html:text/html;Kwon_Kim_2023_Enhancing Accuracy and Robustness through Adversarial Training in Class.pdf:/Users/nicolas/Documents/Zotero/Kwon_Kim_2023_Enhancing Accuracy and Robustness through Adversarial Training in Class.pdf:application/pdf},
}

@inproceedings{chrysakis2023online,
  author       = {Aristotelis Chrysakis and Marie{-}Francine Moens},
  title        = {Online Bias Correction for Task-Free Continual Learning},
  booktitle    = {The Eleventh International Conference on Learning Representations},
  year         = {2023},
}

@inproceedings{rannen2017encoder,
  title={Encoder based lifelong learning},
  author={Rannen, Amal and Aljundi, Rahaf and Blaschko, Matthew B and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1320--1328},
  year={2017}
}

@inproceedings{lin2023pcr,
  title={PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning},
  author={Lin, Huiwei and Zhang, Baoquan and Feng, Shanshan and Li, Xutao and Ye, Yunming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24246--24255},
  year={2023}
}

