%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Custom packages
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{minted}
\setminted{breaklines}
% \usepackage{subcaption}
\usepackage{amsmath}
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Rebuttal}

\begin{document}

\onecolumn
\icmltitle{Rebuttal}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords'' metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in

\section{Rebuttal instructions}
\begin{itemize}
    \item 1 response per reviewer
    \item Max 2500 character per response
    \item Each response is already very close to 2500 characters
    \item The rebuttal is in Markdown so current version is not visually accurate (same for the number of characters, but close)
\end{itemize}
\section{General Comment}
We would like to thank the reviewers for their comments and suggestions, which enabled us to significantly improve the manuscript. We respond to each reviewer individually. However, here is an overall response to common concerns.

The main concern raised by reviewers uoFY and uM3b is the novelty. In this work, we focus on Knowledge Distillation (KD) in the context of Online Continual Learning (OCL). While KD has already been applied to OCL, the arising challenges have never been formally identified and formalized. In this paper, we identify and formalize the key challenges of KD in OCL: teacher quantity, teacher quality and task boundaries. Such challenges have not been identified in previous work. From this analysis, we identify Momentum Knowledge Distillation (MKD) as a promising candidate and demonstrate that on top of solving previously identified challenges, MKD benefits from a unique plasticity-stability control property. While an Exponential Moving Average (EMA) teacher has been used previously in machine learning, we highlight its intuitive and empirical benefits for OCL and propose teacher-dependent weighting strategy. We believe that this thorough analysis and seamless integration procedure can greatly benefit to the Online Continual Learning community.
In summary:
\begin{itemize}
    \item We identify and formalize the key challenges of KD in OCL;
    \item We demonstrate that on top of solving previously identified challenges, MKD benefits from a unique plasticity-stability control property;
    \item We introduce a teacher-dependant weighting strategy for seamless integration in existing approaches;
    \item We show experimentally that MKD is especially adapted to OCL and can be combined with other approaches without any additional hyper-parameter search.
\end{itemize}
To supplement previously listed contributions, we will illustrate each of the three challenges of KD in OCL through the experiments shared in the rebuttal.
\subsection{Extra references (will be included in the final paper)}
[1] Pham, Quang, et al. ``Dualnet: Continual learning, fast and slow.'' NeurIPS (2021).

[2] Soutif–Cormerais, Albin, et al. ``Improving Online Continual Learning Performance and Stability with Temporal Ensembles.'' CoLLAs (2023). 

[3] He, Kaiming, et al. ``Momentum contrast for unsupervised visual representation learning.'' CVPR (2020).

[4] A. Tarvainen, et al. ``Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.'' NeurIPS (2017).

\section{Rebuttal to 7W39}
\subsection{Weaknesses}
1.i. The theoretical analysis is described in Section 3. We understand that referring to such analysis as theoretical might be an overstatement. As such, following reviewers' comments, we will include rebuttal experiments to illustrate such challenges and will change the sentence ``In this paper, we theoretically analyze the challenges in applying KD to OCL.'' in the abstract to ``In this paper, we analyze the challenges in applying KD to OCL and give empirical justifications.'' \\
1.ii. Our main point here is the quality of the teacher. Choosing the correct teacher to learn from is especially important. In OCL, since the model is not trained until full convergence it is even harder to do so. The table below shows performances of ER using a low quality teacher (snapshot of the model at the end of task), and a high quality teacher (snapshot of a model trained for 5 epochs on previous task), on CIFAR100 $M=5$k and two tasks. We train with eq. 2 after conducting a small hyper-parameter search on $\lambda$. Best performances are obtained when $\lambda=0.01$, and while the impact of a low quality teacher is limited, the impact of a higher quality teacher is significant.
$$
\begin{array}{l|c}
\text{Method} & \text{Avg Acc}\\
\hline
ER&49.01\pm4.6\\
ER\text{+low qual teach}&50.67\pm4.3\\
ER\text{+high qual teach}&54.61\pm3.3\\
\hline
\end{array}
$$
2. In Figure 4, we show that competitive performances can be obtained for a wide range of $\alpha$ values as long as the relation observed in Figure 5 is respected. This shows that the method can be used without additional hyper-parameter search on $(\alpha, \lambda_{\alpha})$ values. Additionally, as we detailed in Section B.4. of the appendix, such experiments on hyper-parameters have been realized on CIFAR100, $M=5$k setup only. Therefore, we did not conduct an extensive hyper-parameter search for every dataset. Rather, we directly applied the rule we observed in Figure 5, choosing a default value of $\alpha=0.01$. We believe that the obtained performances on other datasets demonstrate that our method is indeed resilient to hyper-parameter change, which is the key to OCL where hyper-parameter search is unrealistic.

\subsection{Questions}
1. Existing methods employ augmented samples instead of raw samples. In this loss, the motivation is to do distillation between augmented and non-augmented samples. The main reason to explicitly define it is for clarity. Additionally, this strategy significantly increases performance. The bottom line of Table 4 in the main draft (ER + ours (student, one view)) corresponds to our strategy without distilling between augmented and non-augmented samples, as explicitly defined in Section 5.4 ``Impact of Multiview Distillation''.

\section{uoFy}
\subsection{Weaknesses}
1. Both the Mean Teacher method [4] and our approach employ an Exponential Moving Average (EMA) teacher and an average of weights as the final model for inference. However, the original motivation is different as our objective is to leverage the plasticity-stability control property of EMA. Such quality of EMA has never been identified in previous work. Additionally, our distillation process differs during training as we leverage a teacher-dependent weighting scheme as well as a multiview-distillation.\\
2. a) \textbf{Teacher quality} is illustrated in the table below where we show the average accuracy after training on CIFAR100 with 2 tasks and 5K memory. We compare ER using a low quality teacher (snapshot of the model at the end of task), and a high quality teacher (snapshot of a model trained for 5 epochs on previous task). We train with eq. 2 after conducting a small hyper-parameter search on $\lambda$. Best performances are obtained when $\lambda=0.01$, and while the impact of a low quality teacher is limited, the impact of a higher quality teacher is significant.
$$
\begin{array}{l|c}
\text{Method}&\text{Avg Acc}\\
\hline
ER&49.01\pm4.6\\
ER\text{+low qual. teach.}&50.67\pm4.3\\
ER\text{+high qual. teach.}&54.61\pm3.3\\
\hline
\end{array}
$$
b) \textbf{Teacher quantity} depends on the model being used. When saving a model per task, the memory usage grows linearly with the number of tasks. If the model is 100 MB, then the memory usage grows by 100 MB per task. In comparison, an image from Tiny ImageNet is 12 kB.
c) Regarding \textbf{task boundaries}, experiments on OCM [5] show its importance. OCM needs task boundaries to be used since it saved a snapshot of the previous model. To adapt it to the blurry setting, we infer the task change when using OCM which results in a drop in performance. This shows that knowing task boundaries is also important for distillation. We will add such experiments and explanations in the main draft.\\
3. The first sentence of the abstract indeed refers to Class Incremental Learning (CIL) only.

\subsection{Questions}
Questions 1. and 2. have been answered in the Weaknesses section above.

[4] A. Tarvainen, et al. ``Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.'' NeurIPS (2017).

[5] Guo, Yiduo, et al. ``Online continual learning through mutual information maximization.'' ICML (2022).
\section{uM3b}
\subsection{Weaknesses}
\begin{itemize}
    \item EMA is widely used in SSL [3], however its application to OCL remains quite limited. While KD is widely used in CL, it is not necessarily straightforward to implement as it might require hyper-parameter tuning or task boundary information. In this paper, we show that EMA can be very easily adopted and combined with many state-of-the-art approaches seamlessly. In that sense, we argue that KD, or at least EMA is rather overlooked. To make this argument clearer references to [1,2,3] will be added and discussed in the related work section as well.
    \item [2] differs from our method in the sense that no distillation is used, only the temporal ensemble. We implemented [2] and ran experiments on CIFAR10, CIFAR100 and Tiny ImageNet. The results are shown in the table below, we report ER and ER+ours for comparison. Additionally, a comparison with SDP, which leverages a modified version of EMA, is already present in the main draft. It can be seen that our method outperform [2] and such comparison will be included in the final version of the paper, as well as in the publicly available code.
\end{itemize}
$$
\tiny
\begin{array}{l|lll|lll|lll}
    Method & C10-0.2k & C10-0.5k & C10-1k & C100-1k & C100-2k & C100-5k & Tiny-2k & Tiny-5k & Tiny-10k \\
    \hline
    ER           &46.33\pm2.42 &  55.73\pm2.04 &    62.99\pm2.1 &  23.0\pm0.8   &  31.55\pm1.27 &  38.05\pm1.08 &  11.39\pm0.75 &  18.97\pm1.16 &  21.52\pm3.37\\
    Temp. Ens. [2]  & 48.74\pm2.6 & 62.12\pm1.7 & 70.15\pm0.2 & 29.37\pm0.7 & 37.89\pm0.9 & 46.82\pm0.9 & 16.25\pm0.5 & 23.58\pm0.4& 29.89\pm0.5 \\
    ER + ours & 57.54\pm2.55 &  68.48\pm0.92 &   74.33\pm0.68 &  38.5\pm0.5   &  45.2\pm0.2   &  52.10\pm0.5 &  23.95\pm0.65 &  32.22\pm0.88 &  38.27\pm0.18 \\
    \hline
\end{array}
$$
\subsection{Question}
``DER scales poorly'' refers to the fact that DER performances to not increase significantly for larger memory size, contrarily to its counterpart ER.

[1] Pham, Quang, et al. ``Dualnet: Continual learning, fast and slow.'' NeurIPS (2021).

[2] Soutif–Cormerais, Albin, et al. ``Improving Online Continual Learning Performance and Stability with Temporal Ensembles.'' CoLLAs (2023).

[3] He, Kaiming, et al. ``Momentum contrast for unsupervised visual representation learning.'' CVPR (2020).


\bibliography{references}
\bibliographystyle{icml2024}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
