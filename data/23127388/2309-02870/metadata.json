{
  "title": "Rethinking Momentum Knowledge Distillation in Online Continual Learning",
  "authors": [
    "Nicolas Michel",
    "Maorong Wang",
    "Ling Xiao",
    "Toshihiko Yamasaki"
  ],
  "submission_date": "2023-09-06T09:49:20+00:00",
  "revised_dates": [
    "2024-06-05T09:30:34+00:00"
  ],
  "publication_venue": null,
  "abstract": "Online Continual Learning (OCL) addresses the problem of training neural\nnetworks on a continuous data stream where multiple classification tasks emerge\nin sequence. In contrast to offline Continual Learning, data can be seen only\nonce in OCL, which is a very severe constraint. In this context, replay-based\nstrategies have achieved impressive results and most state-of-the-art\napproaches heavily depend on them. While Knowledge Distillation (KD) has been\nextensively used in offline Continual Learning, it remains under-exploited in\nOCL, despite its high potential. In this paper, we analyze the challenges in\napplying KD to OCL and give empirical justifications. We introduce a direct yet\neffective methodology for applying Momentum Knowledge Distillation (MKD) to\nmany flagship OCL methods and demonstrate its capabilities to enhance existing\napproaches. In addition to improving existing state-of-the-art accuracy by more\nthan $10\\%$ points on ImageNet100, we shed light on MKD internal mechanics and\nimpacts during training in OCL. We argue that similar to replay, MKD should be\nconsidered a central component of OCL. The code is available at\n\\url{https://github.com/Nicolas1203/mkd_ocl}.",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "arxiv_id": "2309.02870"
}