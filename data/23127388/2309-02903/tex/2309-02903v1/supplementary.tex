% %File: anonymous-submission-latex-2024.tex
% \documentclass[letterpaper]{article} % DO NOT CHANGE THIS
% \usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
% \usepackage{times}  % DO NOT CHANGE THIS
% \usepackage{helvet}  % DO NOT CHANGE THIS
% \usepackage{courier}  % DO NOT CHANGE THIS
% \usepackage[hyphens]{url}  % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
% \def\UrlFont{\rm}  % DO NOT CHANGE THIS
% \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \frenchspacing  % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
% %
% % These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}

% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{svg}
% \usepackage{multirow}
% \usepackage{amsfonts}
% \usepackage{color}
% \usepackage{bbding}
% \usepackage{array}
% \newcommand{\smallmathcol}[1]{\scalebox{0.8}{$#1$}}
% \newcolumntype{M}{>{\small\ensuremath}c}
% % \usepackage{etoolbox}
% % \newcommand{\changemathfont}[1]{\everymath{\scriptstyle}}
% % \AtBeginEnvironment{table}{\changemathfont}

% % \usepackage{booktabs}
% \graphicspath{{imgs/}}

% %
% % These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
% \usepackage{newfloat}
% \usepackage{listings}
% \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
% \lstset{%
% 	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
% 	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
% 	aboveskip=0pt,belowskip=0pt,%
% 	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}
% %
% % Keep the \pdfinfo as shown here. There's no need
% % for you to add the /Title and /Author tags.
% \pdfinfo{
% /TemplateVersion (2024.1)
% }

% % DISALLOWED PACKAGES
% % \usepackage{authblk} -- This package is specifically forbidden
% % \usepackage{balance} -- This package is specifically forbidden
% % \usepackage{color (if used in text)
% % \usepackage{CJK} -- This package is specifically forbidden
% % \usepackage{float} -- This package is specifically forbidden
% % \usepackage{flushend} -- This package is specifically forbidden
% % \usepackage{fontenc} -- This package is specifically forbidden
% % \usepackage{fullpage} -- This package is specifically forbidden
% % \usepackage{geometry} -- This package is specifically forbidden
% % \usepackage{grffile} -- This package is specifically forbidden
% % \usepackage{hyperref} -- This package is specifically forbidden
% % \usepackage{navigator} -- This package is specifically forbidden
% % (or any other package that embeds links such as navigator or hyperref)
% % \indentfirst} -- This package is specifically forbidden
% % \layout} -- This package is specifically forbidden
% % \multicol} -- This package is specifically forbidden
% % \nameref} -- This package is specifically forbidden
% % \usepackage{savetrees} -- This package is specifically forbidden
% % \usepackage{setspace} -- This package is specifically forbidden
% % \usepackage{stfloats} -- This package is specifically forbidden
% % \usepackage{tabu} -- This package is specifically forbidden
% % \usepackage{titlesec} -- This package is specifically forbidden
% % \usepackage{tocbibind} -- This package is specifically forbidden
% % \usepackage{ulem} -- This package is specifically forbidden
% % \usepackage{wrapfig} -- This package is specifically forbidden
% % DISALLOWED COMMANDS
% % \nocopyright -- Your paper will not be published if you use this command
% % \addtolength -- This command may not be used
% % \balance -- This command may not be used
% % \baselinestretch -- Your paper will not be published if you use this command
% % \clearpage -- No page breaks of any kind may be used for the final version of your paper
% % \columnsep -- This command may not be used
% % \newpage -- No page breaks of any kind may be used for the final version of your paper
% % \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% % \pagestyle -- This command may not be used
% % \tiny -- This is not an acceptable font size.
% % \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% % \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

% \setcounter{secnumdepth}{1} %May be changed to 1 or 2 if section numbers are desired.

% \newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
% 		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
% \def\@fnsymbol#1{\ensuremath{\ifcase#1\or *\or \dagger\or \ddagger\or
%         \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
%         \or \ddagger\ddagger \else\@ctrerr\fi}}
% \usepackage{tabularray}
% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%
\newcommand{\beginappendix}{%
        \setcounter{section}{0}
        \renewcommand\thesection{\Alph{section}}
        \setcounter{table}{0}
        \renewcommand{\thetable}{A\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{A\arabic{figure}}%
     }
% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Towards Efficient Training with Negative Samples in Visual Tracking}




% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% \usepackage{bibentry}
% END REMOVE bibentry

% \begin{document}

% \maketitle







\beginappendix
\section*{Appendix}
% {$[\texttt{x}_1\texttt{,y}_1\texttt{,x}_2\texttt{,y}_2]$}

\section{Details in Encoder}
The encoder is identical to the one used in Vision Transformer (ViT).
We start by dividing a given image pair, comprising a template and a search region, into smaller image patches. Specifically, we denote the template image patch as $z \in \mathbb{R}^{3\times H_z \times W_z}$ and the search region patch as $x \in \mathbb{R}^{3\times H_x \times W_x}$.
These patches are then transformed into tokens, $\boldsymbol{H}_{z} \in \mathbb{R}^{D}$ and $\boldsymbol{H}_{x} \in \mathbb{R}^{D}$, using a linear projection layer. We also introduce a novel target-indicating token(TIT), $\boldsymbol{H}_{\texttt{i}} \in \mathbb{R}^{D}$, which aims to enhance the tracking precision by providing location-specific information.
We feed the concatenated tokens, $[\boldsymbol{H}_{\texttt{i}} ; \boldsymbol{H}_{z} ; \boldsymbol{H}_{x}]$, into a transformer encoder similar to ViT. This step facilitates simultaneous feature extraction and relation modeling, thus called one-stage.  One may notice that the information carried by TIT is only related to the template.  However, we found no difference on performance between (1) allowing the TIT to attend to all tokens from both the template and search region and (2) permitting the TIT to attend solely to tokens from the template.  For simlicity, we follow the setting in (1). The final tokens from the search region are selected and flattened to the 2D features before being input to the head for classification and localization.

\section{$[\texttt{cx,cy,w,h}]$ v.s. $[\texttt{w,h}]$ in Target-Indicating Token}

As the template is a center-cropped image of the target to be tracked, the bounding box of the target is always centered.  Thus if we formulate the bouding box as center point and the size $[\texttt{cx,cy,w,h}]$, then the center part $[\texttt{cx,cy}]$ is always $[\texttt{0,0}]$.  Therefore, here comes a question: is the first two number $[\texttt{cx,cy}]$ necessary? We conduct extra experiments on the formulation of the source to the Target-Indicating Token(TIT). As shown in Fig. \ref{table:inception_discuss}, in cotrast to our intuition, $[\texttt{cx,cy,w,h}]$ is more benefitial to the performance than merely $[\texttt{w,h}]$.  The potential reason is even if the target is always located in the center, the model has to learn this prior knowledge by itself.  As the powerful multi-modality of the Transformer, providing more information related to the task helps the model perform better.

\section{ Discretizing Resolution of Distance}
Different from object detection, there is only one target to regress in one image in single object tracking(SOT).  The analysis of the distribution of distance over all training samples in GFL\cite{GFL} is under the sample assignment from ATSS\cite{ATSS}.  In SOT, the training fashion is to crop an image around the target with $N^2$ times area as the search region.  With different resolutions for network inputs and various jittering of center position and scale, the statistical distribution of distance may differ.  Here we follow the region crop settings from OSTrack\cite{ostrack}, making a histogram of bounding box regression targets of points inside the box over all training samples on train split of GOT-10k\cite{GOT-10k}.  As shown in Fig.~\ref{distance}, using $n=16$ directly from GFL fits both 256$\times$256 with $4^2\times$ cropped region area and 384$\times$384 with $5^2\times$ cropped region area in the tracking pipeline as well.  Therfore, we keep the  discretizing resolution $n = 16$ in Eq. (1).

\section{Difference with UAST}
UAST\cite{UAST} is the first tracker who adopt the distribution-based head design from GFL\cite{GFL} and GFLv2\cite{GFLv2} in object tracking.  UAST mainly contributes to improve the original head design and make it more appropriate for tracking and bring in the concept of localization quality into the community from object detection.
However, in our method, we simply borrow the head design from GFL.  We do not claim it  as our contribution. Benefit from the distribution-based head for its ability of handling the absence of targets, it helps us train the model with both positive and negative sampels simultaneously.  Our goal is to expolre a more efficient and effective training strategy for visual object tracking.
\begin{figure}[t]
    % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    % \includesvg[width=0.48\textwidth]{imgs/train.svg}
       \includegraphics[width=1.0\linewidth]{distance.pdf}
    
       \caption{The histogram of bounding box regression targets of points inside the box over all training samples on train split of GOT-10k. Since we crop the image to different search region sizes, the distributions of distance varies.  Following the convention, the search region is 4x of target bounding box in 256$\times$256 input setting, while it is 5x in 384$\times$384 setting.}
    \label{distance}
    \end{figure}
\section{Limitation}
\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        Inception & Source                 & AO\%                                                       \\ \hline
        none      & -                      & 73.6\\
        concat    & $[\texttt{w,h}]$       & 73.9\fontsize{9.0pt}{\baselineskip}{(+0.3)} \\
        concat    & $[\texttt{cx,cy,w,h}]$ & 74.3\fontsize{9.0pt}{\baselineskip}{(+0.7)} \\ \hline
    \end{tabular}
    % \vspace{-2mm}
    \caption{
        Different ways of inception brings different effect on performance.
    }
    \label{table:inception_discuss}

\end{table}

\begin{table}
    \centering
    \begin{tabular}{c|c|ccc}
        \hline
        Model   & \#Epochs         & AUC(\%) & P(\%)  & P$_\mathrm{norm}$(\%) \\
        \hline
        OSTrack & 100 & 62.2  & 65.9 & 70.1                \\
        JN      & 60 & 63.2  & 66.9 & 71.2                \\
        \hline
    \end{tabular}
    % \vspace{0.2em}
    \caption{
        Performance on LaSOT benchmark using the models trained with GOT-10k train split only.
    }
    \label{tabel:lasot_disscus}
\end{table}
\begin{figure}[t!]
    % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    % \includesvg[width=0.48\textwidth]{imgs/train.svg}
       \includegraphics[width=1.0\linewidth]{more_vis_taxi}
    
       \caption{The visualization of attention map and the final classification map of each layer.}
    \label{taxi}
\end{figure}
\begin{figure}[t!]
    % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    % \includesvg[width=0.48\textwidth]{imgs/train.svg}
       \includegraphics[width=1.0\linewidth]{attn.pdf}
    
       \caption{The visualization of the distribution of distance from the point with highest classification score.  X-axis in each distribution diagram denotes for the non-normalized distance to the starting point.}
    \label{attn}
\end{figure}

\begin{figure*}[]
    \centering
    \includegraphics[width=0.85\textwidth]{TIT_sup} 
    % \vspace{-2mm}
    \caption{Visualization of attention map (of TIT attending to the tokens from the template) in the encoder.}
    \label{attn_map}
% \vspace{-4mm}
\end{figure*}
Our proposed tracker achieve not so much competitive performance on LaSOT. Despite the number of sequences being less than that in GOT-10k, TrackingNet and VOT chanllange, the average length of each sequence in LaSOT is much longer, making it a long-term tracking benchmark.  Long-term tracking induces more distributors and  deformation of appearance in temporal, thus requiring the tracker to predict a more accurate classification map.  As shown in Fig.~\ref{attn}, due to the multi-sample assignment in the training, the classification branch is encouraged to predict a box-like class map.  Compared with a single-point class map e.g. center class map, the most confident point predicted by the model may locate at the corner of the target bounding box, which brings more ambiguity.  However, the approach we proposed of adding negative samples is to encourage the model to have more ability of generalization instead.  To verify the generalization of our method, we test the GOT-10k trained models on LaSOT benchmark.  As shown in Tab.~\ref{tabel:lasot_disscus}, our model JN-256 surpasses the OSTrack-256 by 1.0\% AUC, which again shows the generalization of our model when facing the unseen scenarios.


% \input{lasot_tknet_table}

\section{More Visualization}
We first provide more visualization of the distribution-based head in Fig.~\ref{taxi}.  The ouput distribution of the head is sharp when it is certain about the boundary of the target. Due to occlusion, the boundaries of objects are not clear enough, thus the model predicts uncertain distribution.  We notice that most of the output distance is distributed on the samll range, e.g. all the output probability is lower then $0.05$ when $x \in (8,16)$ in Fig.~\ref{taxi}.  We keep this setting because we supervise all the point inside the G.T. bouding box for localization during traing.   The distance from the starting point to the boundary in four directions may various if the starting point located in the corner of the target.

In Fig.~\ref{attn}, we provide more visualization results for attention weights of the search region corresponding to the center part of the template (which can be seen as the target) . The results show that the model attends to the foreground objects at an early stage.

In Fig.~\ref{attn_map}, we visualize more attention map from the TIT to other image tokens in the template by averaging attention weights across heads, reshaping to 2D, and upscaling to the image's resolution. 








% \bibliography{aaai24,vot}
% \end{document}
