%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{svg}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{bbding}
\usepackage{array}
\newcommand{\smallmathcol}[1]{\scalebox{0.8}{$#1$}}
\newcolumntype{M}{>{\small\ensuremath}c}
% \usepackage{etoolbox}
% \newcommand{\changemathfont}[1]{\everymath{\scriptstyle}}
% \AtBeginEnvironment{table}{\changemathfont}

% \usepackage{booktabs}
\graphicspath{{imgs/}}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or *\or \dagger\or \ddagger\or
        \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
        \or \ddagger\ddagger \else\@ctrerr\fi}}

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Towards Efficient Training with Negative Samples in Visual Tracking}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Qingmao Wei,
    Bi Zeng,
    Guotian Zeng
}
\affiliations{
    %Afiliations
    Guangdong University of Technology\\
    tsingmoe@gmail.com
}



% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
    Current state-of-the-art (SOTA) methods in visual object tracking often require extensive computational resources and vast amounts of training data, leading to a risk of overfitting. This study introduces a more efficient training strategy to mitigate overfitting and reduce computational requirements. We balance the training process with a mix of negative and positive samples from the outset, named as Joint learning with Negative samples (JN). Negative samples refer to scenarios where the object from the template is not present in the search region, which helps to prevent the model from simply memorizing the target, and instead encourages it to use the template for object location.  To handle the negative samples effectively, we adopt a distribution-based head, which modeling the bounding box as distribution of distances to express uncertainty about the target's location in the presence of negative samples, offering an efficient way to manage the mixed sample training.  Furthermore, our approach introduces a target-indicating token. It encapsulates the target's precise location within the template image. This method provides exact boundary details with negligible computational cost but improving performance.  Our model, JN-256, exhibits superior performance on challenging benchmarks, achieving 75.8\% AO on GOT-10k and 84.1\% AUC on TrackingNet. Notably, JN-256 outperforms previous SOTA trackers that utilize larger models and higher input resolutions, even though it is trained with only half the number of data sampled used in those works.
\end{abstract}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\columnwidth]{title} % Reduce the figure size so that it is slightly narrower than the column.
    \caption{Comparison of accuracy vs. latency trade-off for different visual tracking methods, with bubble size representing the training epochs.  Trained with the train split of GOT-10k, our tracker JN-256 achieve a amazing 75.8\% AO, showing impressive one-shot tracking performance.
    %   Our method outperforms the larger trackers trained with many more epochs. 
        % The evaluation is conducted on the challenging GOT-10k benchmark, where each evaluated tracker is exclusively trained with the train split of GOT-10k.
    }
    \label{title}
    % \vspace{-4mm}
\end{figure}
\section{Introduction}
\begin{figure*}[]
    \centering
    \includegraphics[width=0.97\linewidth]{illustration} % Reduce the figure size so that it is slightly narrower than the column.
    % \vspace{-2mm}
    \caption{When the tracker is consistently presented with different frames of the same template as in (a), it may overfit by simply memorizing the target instead of learning to locate it based on the template, as shown in (b). This issue can be alleviated by introducing negative pairs, where the target does not appear in the search region. As demonstrated in (c), training with negative samples aids the model in achieving a higher average overlap, even though its training Intersection over Union (IoU) may be lower than that achieved with all-positive training.}
    \label{illu}
    % \vspace{-4mm}
\end{figure*}

Visual object tracking represents a significant challenge in computer vision and has applications in diverse domains such as surveillance, robotics, and autonomous driving. The recent integration of the Transformer~\cite{vanilla_selfattention} in object trackers~\cite{transt,TMT,ostrack,mixformer,CSWinTT,simTrack,aiatrack} has added both promise and complexity to the field. While deep learning advancements have led to notable progress, training state-of-the-art (SOTA) object trackers with Transformers remains daunting, requiring substantial time and computational resources.

% Inspired by the success of the Transformer in computer vision, several trackers have begun to incorporate it into their frameworks as standalone modules for feature fusion and relation modeling. This approach has led to marked progress, but it also presents challenges. 
STARK~\cite{STARK} and TATrack~\cite{TATrack}, who interpret the Transformer as a standalone module for feature fusion and train it from scratch, requiring an extensive amount of training samples, a problem partially mitigated by the fast convergence of works like OSTrack~\cite{ostrack} and MixFormer~\cite{mixformer}, which utilize mask-image-modeling pretraining~\cite{mae,cae,simMIM}.

However, we identify two main areas for further optimization and propose novel solutions to address them:

First, current visual tracking frameworks typically use an image pair of a template and a search region. The template image, though representing the target, offers only a vague hint of its location. To provide the model with more precise information about the target's position, we introduce a Target-Indicating Token (TIT). This token is a carefully crafted input that encodes specific details about the target's exact location within the template image. By feeding the TIT into the Transformer along with the image features, the model receives a direct cue to the target's precise boundaries. Unlike previous methods~\cite{ostrack,mixformer}, which may have only indirectly inferred the target's position, the inclusion of TIT equips the model with direct and accurate positional data about the target object. This contributes to significantly more accurate tracking performance by providing an additional layer of information.

Second, previous works often solely rely on positive pairs in training, potentially leading to overfitting. To combat this, we introduce Joint Learning with Negative samples (JN), utilizing a balanced mix of negative and positive samples from the outset. This method goes beyond the typical training routine. By incorporating negative samples—instances where the target is absent from the search region—the model learns to develop an awareness of the possibility of the target's absence. This awareness forces the model to rely more heavily on the template when searching for the target in the search region, rather than simply memorizing the target. We further support this mechanism by adopting a distribution-based head from the Generalized Focal Loss~\cite{GFL, GFLv2}. This head predicts a distribution of distances, expressing uncertainty about the target's location when negative samples are present. This nuanced approach enables more effective learning from mixed samples, leading to improved object tracking performance..

Despite training with fewer samples due to the mix of negative image pairs, our method achieves impressive performance, setting a new SOTA on multiple benchmarks. It maintains admirable inference efficiency and shows faster convergence compared to SOTA Transformer-based trackers. This method achieves a balance between accuracy and inference speed, as demonstrated in Fig.~\ref{title}.

In summary, our work contributes in the following ways:

\begin{itemize}
\item We introduce a novel approach of simultaneous learning of classification and localization with negative samples, significantly enhancing visual object tracking performance, especially with the distribution-based head.
\item We propose a Target-Indicating Token that efficiently provides location annotations for the target within the template without significant computational cost.
\item Our experiments show that our model, even with reduced training samples, outperforms existing state-of-the-art trackers on challenging benchmarks.
\end{itemize}

\section{Related Work}

% \textbf{Head and Label Assignment in Localization Task.}
% The dense prediction task in computer vision usually treats the model as two parts: a pre-trained backbone for feature extraction and a task-specified head. The head is aiming to predict the class and localization for each target in the image from the extracted features.  Many works in visual tracking borrow the head directly from object detection since both tasks predict the object class and classification. SiamFC\cite{SiamFC}, SiamR-CNN\cite{SiamR-CNN} and SiamRPN\cite{SiamRPN++} use a set of preset anchors for localization, which lower the inference speed. TransT\cite{TransT} directly outputs the bounding box of the target by an MLP, since there is only one target we want to locate in the image. Benefits from the development of head design with anchor-less in object detection\cite{fcos, centernet, cornernet}, some trackers directly adopt the heads from them.  SiamFC++\cite{siamfcpp} adopts centerness proposed in FCOS\cite{fcos} to estimate the quality of localization.  However, it has been observed that centerness alone cannot completely capture the localization quality.  Ocean proposes an object-aware branch that utilizes predicted boxes, while SiamRCR\cite{siamRCR} assigns dynamic weights to the classification loss based on the predicted IoU score.  STARK\cite{STARK} and MixFormer\cite{Mixformer} adopt the simplified Corner Head\cite{cornernet}, using two blocks of convolution layers to predict the probability of being a left-up or a bottom-down corner of a target in the feature map.   OSTrack\cite{OSTrack} uses the Center Head\cite{centernet}, which consists of two branches for classification and localization. The classification branch predicts the centerness of the target, and the localization branch regresses the size and center offset of the target.  Both center head and corner head assign only one positive sample in one image pair in visual tracking, making them inefficient in training.  UAST\citep{UAST} simply adopts head from GFL\cite{GFL, GFLv2}, which has a classification branch similar to center head, while the other branch for localization regresses the distribution of distance to the left-top-right-bottom(ltrb) boundary of a target, incorporating an estimated uncertainty representation of localization to weight the classification branch.  However, UAST focused on the uncertainty estimation of the target, while neglecting the potential of ltrb for introducing more localization samples in the feature map.  In contrast, our methods assign one bounding box to many regression targets by using ltrb representation, boosting the training process.  
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{arch} % Reduce the figure size so that it is slightly narrower than the column.
    \caption{Illustration of our proposed pipeline. The template and search region are split, flattened, and projected then concatenated with a Target-Indicating Token.  For the positive image pairs, the supervision flows on two branches. For the negative image pairs, the localization branch is supervised by the loss from the \textit{Aware Path}.}
    \label{fig_arch}
    % \vspace{-4mm}
\end{figure*}

\textbf{Training Pipeline in Object Tracking.}
The training pipelines of previous trackers are often complex. STARK~\cite{STARK} requires numerous positive samples for localization training in its initial stage, followed by a second stage that trains a separate classification module for online tracking. Mixformer~\cite{mixformer} follows a similar training paradigm, even though it employs the Transformer for both feature extraction and relation modeling. The classification module in Mixformer is more efficient than STARK, but it still requires a split stage for training. In contrast, as an offline tracker, OSTrack~\cite{ostrack} solely trains the model with positive samples throughout the entire training process. However, similar to the first training stage of online trackers, the localization branch in OSTrack predominantly encounters positive samples, which may lead to overfitting issues.
UniTrack~\cite{UniTrack}, Unicorn~\cite{unicorn}, and UNINEXT~\cite{UNINEXT} leverage both single object tracking (SOT) and multi-object tracking (MOT) datasets, allowing their models to handle multiple tasks such as SOT, MOT and video object segmentation within a unified framework. However, these methods yield only ordinary performance on the one-shot setting benchmark, GOT-10k~\cite{GOT-10k}, which specifically requires trackers to be trained solely on its own train split. This suggests that these methods heavily rely on extra data to achieve performance improvement.
To encourage the tracker to utilize information from the template instead of merely memorizing the target in the search region during the training process, we introduce negative image pairs throughout the entire training stage. 

\textbf{Multi-modality of Transformer.}
Traditionally developed for natural language processing, the Transformer architecture has demonstrated its adaptability to diverse data modalities. ViT~\cite{vit} concatenate a learnable \texttt{CLS} token to the input image embeddings to extract category information, demonstrating the potential of transformers in modeling image features along with other modalities. SwinTrack~\cite{SwinTrack} attempts to introduce the Transformer Decoder for feature interaction in object tracking and achieves improved performance by incorporating the Motion Token, which contains historical position information of the target. This further proves the transformer's capability to handle different modalities of information. Leveraging recent developments in one-stage object tracking, we design a target-indicating token that, like ViT, directly concatenates with the input image patch. This token explicitly informs the model about the precise position of the target in the template image. 



\section{Method}
% This section presents our model design and joint learning in detail.  Firtst, we briefly overview our model architecture.  Then, we demonstrate our target-indicating token.  Finally, we introduce the integration details of joint learning with various representive heads.
% \subsection{Preliminaries}
% General Focal Loss (GFL) \cite{GFL} has primarily focused on addressing the challenge of modeling localization quality in object detection tasks. Following FCOS~\cite{fcos} and ATSS~\cite{ATSS}, who represent bounding boxes as the left, top, right, and bottom (ltrb) distances from a grid point on the feature map, GFL adopts a distribution-based approach to model these distances, enabling a more nuanced understanding of localization quality. By discretizing the continuous space and learning the probabilistic distribution over these distances, GFL transforms the localization task into a classification problem.
% %  which offers potential benefits for addressing uncertainty in object tracking scenarios. 
% Given a discretizing resolution $n$, the localization branch predicts the probabilitis of distance $y$ with minimum $y_0$ and maximum $y_n (y_0 \leq y \leq y_n, n \in \mathbf{N}) $, we can have the estimated value $\hat{y}$ from the model ($\hat{y}$ also meets $y_0 \leq \hat{y}\leq y_n$):
% \begin{equation}
% \hat{y}=\int_{y_0}^{y_n} P(x) x \mathrm{~d} x =\sum_{i=0}^n P\left(y_i\right) y_i
% \end{equation}

% With prediction of probability of distance on each direction, GFLv2 \cite{GFLv2} proposed a quality-aware classification score as a product of origional classification and the quality of localization, which the latter stands for the certainty of the distribution. Specifically, top-k probabilitis of distribution and the mean value of them are sampled as $\operatorname{topkm(\mathbf{P}^\omega)}$ in each direction $w$ output by the localization and then concatenated as a statistical feature $\mathbf{F} \in \mathbb{R}^{4(k+1)}$:
% \begin{equation}
% \mathbf{F} = \operatorname{concat}(\{\operatorname{topkm(\mathbf{P}^\omega) | \omega \in \{l,t,r,b\}}\})
% \end{equation}
% The final output of quality-aware classification score is presented as $\mathbf{Q}$:
% \begin{equation}
%     \mathbf{Q} = \mathbf{C} \cdot \operatorname{m}(\mathbf{F})
% \end{equation}
% where $\operatorname{m}$ is a simple multi-layer perceptron(MLP) to project the statistical feature $\mathbf{F}$ with the dimension of origional classification score $\mathbf{C}$.

% UAST~\cite{UAST} simply adopt the head design from GFL and GFLv2 in Object Tracking, and focus on utilizing uncertainty in update of online template.  Our design also follow GFLv2, but

\subsection{Model Overview}

The overall design of our model is presented in Fig.~\ref{fig_arch}. It adopts a simple encoder-head architecture following recent tracking works~\cite{ostrack,mixformer}.
% 我们的模型是一个简单的backbone-head设计。
% backbone是一个从ViT拿过来的简单的Encoder，它的输入包括模板的patches和搜索区的patches的拼接。
% 和OSTrack一样，Encoder的作用是对模板和搜索区的图像同时进行特征提取和关系建模。
% 受ViT在分类任务中使用一个额外的class token的启发，我们将模板图像中待跟踪的目标的位置信息注入到一个token中，并把它与上述的图像patches拼接起来，一起作为Encoder的输入。
% head设计是从GFLv2拿过来的，但我们的训练策略与GFLv2不同，具体将在下一小节中做出详细介绍。

\textbf{Encoder.}
The encoder can be a general image encoder that encodes pixels into hidden feature representations, such as ConvNet, vision Transformer (ViT), ora hybrid architecture. In this work, we use the same ViT encoder as OSTrack~\cite{ostrack} for visual feature encoding. The template and search images are first split into patches, flattened and projected to generate a sequence of tokens and added with positional embedding. The tokens are concatenated with an extra target-indicating token which carries the location of the target in the template image as detailed in Sec.~\ref{sec:target_token}. we feed them into a plain ViT backbone to encode visual features. We provide more details and discussion about the encoder in Appendix A.
% The encoder is identical to the one used in Vision Transformer (ViT).
% We start by dividing a given image pair, comprising a template and a search region, into smaller image patches. Specifically, we denote the template image patch as $z \in \mathbb{R}^{3\times H_z \times W_z}$ and the search region patch as $x \in \mathbb{R}^{3\times H_x \times W_x}$.
% These patches are then transformed into tokens, $\boldsymbol{H}_{z} \in \mathbb{R}^{D}$ and $\boldsymbol{H}_{x} \in \mathbb{R}^{D}$, using a linear projection layer. We also introduce a novel target-indicating token, $\boldsymbol{H}_{\texttt{i}} \in \mathbb{R}^{D}$, which aims to enhance the tracking precision by providing location-specific information, as detailed in Sec.~\ref{sec:target_token}.
% We feed the concatenated tokens, $[\boldsymbol{H}_{\texttt{i}} ; \boldsymbol{H}_{z} ; \boldsymbol{H}_{x}]$, into a transformer encoder similar to ViT. This step facilitates simultaneous feature extraction and relation modeling, thus called one-stage.
% The features extracted from the search region are selected and flattened before being fed into the head for classification and localization. 

\textbf{Head.}
The goal of the head is to predict the bounding box or the mask of the target. In our visual object tracking model, we primarily consider three types of head modules: Center Head, Corner Head, and Distribution-based Head.

The Center Head, utilized by OSTrack, comprises a lightweight, fully convolutional module. It consists of three branches: a centerness branch, an offset branch, and a size branch. The centerness branch classifies grid cells based on how close they are to the object's center, while the offset branch refines this by providing an offset to the exact center from the predicted grid. The size branch regresses the width and height of the object. The outputs of these branches together yield a precise bounding box.

The Corner Head, employed by Mixformer~\cite{mixformer} and STARK~\cite{STARK}, operates by predicting the top-left and bottom-right corners of the bounding box. Each corner prediction is treated as a classification task with its own specific branch, which aims to accurately place the corners by generating expected values in two dimensions.

The Distribution-based Head, inspired by the Generalized Focal Loss~\cite{GFLv2} in object detection, consists of a quality-aware classification branch and a distribution-based localization branch. As shown in the head part of Fig.~\ref{fig_arch}, the localization branch models the output box as the distribution-based of left, top, right, and bottom (ltrb) distances. 
Given a discretizing resolution $n$, it predicts the probabilitis of distance $y$ with minimum $y_0$ and maximum $y_n (y_0 \leq y \leq y_n, n \in \mathbf{N}) $. The estimated distance $\hat{y}$ can be integrated from the the output:
\begin{equation}
\hat{y}=\int_{y_0}^{y_n} P(x) x \mathrm{~d} x =\sum_{i=0}^n P\left(y_i\right) y_i
\end{equation}
We provide more details about setting the discretizing resolution $n$ in Appendix C.

With prediction of distribution of distance on each direction, top-k probabilitis of distribution and the mean value of them are sampled as $\operatorname{topkm(\mathbf{P}^\omega)}$ in each direction $w$ output and then concatenated as a statistical feature $\mathbf{F} \in \mathbb{R}^{4(k+1)}$, which stands for the certainty of the distribution:
\begin{equation}
\mathbf{F} = \operatorname{concat}(\{\operatorname{topkm(\mathbf{P}^\omega) | \omega \in \{l,t,r,b\}}\})
\end{equation}
The final output of quality-aware classification score is presented as $\sigma$:
\begin{equation}
    \sigma = \mathbf{C} \cdot \mathcal{M}(\mathbf{F})
\end{equation}
where $\mathcal{M}$ is a simple multi-layer perceptron(MLP) to project the statistical feature $\mathbf{F}$ with the dimension of original classification score $\mathbf{C}$. $\mathcal{M}$ serverd as a \textit{Aware Path} between the branches, passing the supervision signal from the classification to the localization branch.  The process is shown as head part in Fig.~\ref{fig_arch}.
% ,  passing the supervision signal to the localization branch even if the 

While we found that the distribution-based head performs best in our model in subsequent experiments, this doesn't mean the other two head modules are without merit. In Sec.~\ref{loss}, we will delve deeper into how these three head modules behave when dealing with negative samples, and their loss function design.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{inception} % Reduce the figure size so that it is slightly narrower than the column.
    \caption{Comparisons of ways of position inception.  The projection modules e.g. conv or MLP are omitted. (a): adding the embedding to the features; (b) \& (c): concatenating the tokens to the features. }
    \label{inception}
% \vspace{-4mm}
\end{figure}

\subsection{Target-Indicating Token}
\label{sec:target_token}
In the context of object tracking under image-pair settings, the template image is a cropped image centered around the target to be tracked.  Throughout both the training and test phases, the precise position of the target is known, typically represented as a bounding box or a mask.  However, many recent trackers fail to fully exploit this valuable position information.

To address this limitation, some trackers, e.g. STMTrack~\cite{stmtrack} and attempt to incorporate the position information by adding an embedding genrated from the pseudo mask of the target to the extracted template features as shown in Fig.~\ref{inception}(a).  However, we observed that this approach can introduce noise and adversely affect the feature representation.
We propose a novel approach - concatenating the embedding tokens instead of adding it to the features.  The tokens, denoted as Target-indicating Token, is formulated to represent the bounding box information and are concatenated with the image features fed into the encoder.  Given the coordinates of the box of the target, a simple way is to use four tokens carries each number of the coordinates shown in Fig.~\ref{inception}(b).   In our method, we leverage an multi-layer perceptron $\mathcal{M}$ to project the box information in one single Target-indicating Token (TIT) depicted in Fig.~\ref{inception}(c), as follows:
\begin{equation}
\boldsymbol{H}_{i} = \mathbf{\mathcal{M}}(x_z, y_z, w_z, h_z)
\end{equation}
where $(x_z, y_z, w_z, h_z)$ is the bounding box of the target in the template.  As illustrated in Fig.~\ref{fig_arch}, TIT is concatenated with the visual feature tokens as the input of the Transformer Encoder.   This innovative design enables our tracker to gain a deeper understanding of the target's spatial location and directly incorporate this critical information into the feature extraction process as interpreted in Fig.~\ref{attn_map}, while introducing negalible computation.  More discussion about how TIT interacted with the image features are provided in the Appendix A \& B.

\subsection{Joint Learning with Negative Samples}
\label{sec:negative_samples}
% 头的设计和STARK的探索
As the input of the model is an image pair of the template image and search region, we define that (1) \textbf{Positive} sample is a pair where the target in the template image appears in the search region, and (2) \textbf{Negative} sample is a pair where the target in the template image does not appear in the search region.  It was observed in STARK~\cite{STARK} that joint learning of localization and classification, i.e., training with both positive and negative samples within a single stage, led to sub-optimal results.  This could be attributed to the original design of the localization head, which was found to be incompatible with the classification task.  Therefore, drawing inspiration from the head design in GFL~\cite{GFL}, we introduced a head based on uncertainty estimation into the tracker.  Using this head, we explored the effectiveness of simultaneous classification and localization learning within this framework by mixing up the negative samples during training.  With discretized distribution representation of bounding box, we encourage the box head to predict an ambiguous distribution facing negetive samples.  

\textbf{Data Sampling:}
In our training pipeline, we employ a joint learning strategy that involves mixing negative image pairs with positive ones using a ratio denoted as $\rho \in (0,1]$.  This ratio determines the proportion of positve samples in the training data.  For positive image pairs, we randomly sample two frames from a sequence within the maximum sampling gap, ensuring that both frames contain the visible target.
When generating negative image pairs, we start with a randomly sampled template.  To create a negative pair, we first attempt to sample a frame from the same sequence as the template but with an absence label, indicating that the target is not present in the frame.  If the target is consistently present in the sequence of the template image, we randomly sample a frame from another sequence, contributing to a more comprehensive representation of negative scenarios.
As we do not supervise the localization in negative pairs, the effective number of samples used for training localization is less than that for training classification.  The relationship between the effective number of samples in joint learning ($n^{*}$) and that in the all-positive learning ($n'$) is given by:
\begin{equation}
\begin{split}
n^{*} = \rho \cdot n'
\end{split}
\end{equation}
In summary, the parameter $\rho$ controls the balance between positive and negative samples in the training data, and its value directly influences the effective number of samples used for training localization. Adjusting $\rho$ allows us to fine-tune the training process and optimize the model's performance in object tracking tasks, which will be detailed discussed in Sec.~\ref{ConvergenceandRatio}.


\subsection{Loss Function with Negative Samples} 
\label{loss}
As we introduce negative pairs into the training process, we encounter differences in label assignment and loss function for different heads. 

\input{got10k_table}
(1)\textit{With Center Head:}
The output box is supervised by $l1$ loss and $\operatorname{GIoU}$ loss~\cite{generalized_iou}.  Focal loss(FL)~\cite{focal} is aopted for the classification map.  For the negative pairs, we only assign the supervise the center branch by simply assign all the label to zeros.  The final loss of center head can be written as:
% \vspace{-2mm}
\begin{equation}
\begin{split}
    \mathcal{L}_{\operatorname{center}} &=
    \frac{1}{N_{pos}}(
        \lambda_{l1}L_{l1}
        + \lambda_{\operatorname{GIoU}}L_{\operatorname{GIoU}}
        + \lambda_{\operatorname{FL}}L_{\operatorname{FL}}
    )\\
    &+ 
    \frac{1}{N_{neg}} \lambda_{\operatorname{FL}}L_{\operatorname{FL}}
\end{split}
\end{equation}


(2)\textit{With Corner Head:}
For positive pairs, the ouput box is supervised by combination of $l1$ and $\operatorname{GIoU}$ following Mixformer.
For the negative pairs, no integration is performed as no object in search region, thus we directly assign both the classification maps to all zeros and apply a standard cross-entropy(CE) loss on it.  The final loss of corner head can be written as:
% \vspace{-2mm}
\begin{equation}
    \begin{split}
        \mathcal{L}_{\operatorname{corner}} &=
        \frac{1}{N_{pos}}(
            \lambda_{l1}L_{l1}
            + \lambda_{\operatorname{GIoU}}L_{\operatorname{GIoU}}
        )\\
        &+ 
        \frac{1}{N_{neg}} \lambda_{\operatorname{CE}}L_{\operatorname{CE}}
\end{split}
\end{equation}



(3)\textit{With Distribution-based Head:}
For positive image pairs, the classification branch is encourage to predict the quality of localization, e.g. the intersection over union(IoU) between the predicted box and the Ground Truth (G.T.) box.  Therefore, we assign the calculated IoU to the points inside, while the points outside the G.T. box are assigned to zeros during training.  The target of classification is supervised by quality focal loss(QFL)~\cite{GFL}:
\begin{equation}
    \mathcal{L}_{\operatorname{QFL}}(\sigma)=-|y-\sigma|^\beta((1-y) \log (1-\sigma)+y \log (\sigma))
\end{equation}
Unlike corner head, which only calculates the loss on the expected box for localization, here we directly supervise the distribution of disctance with Distribution Focal Loss(DFL):
\begin{equation}
    \mathcal{L}_{\operatorname{DFL}} (\mathcal{S}_i, \mathcal{S}_{i+1}) = -((y_{i+1}-y)\mathrm{log} (\mathcal{S}_i)
+(y-y_i)\mathrm{log} (\mathcal{S}_{i+1}))
\end{equation}
The global minimum solution of DFL comes when $\mathcal{S}_i = \frac{y_{i+1}-y}{y_{i+1}-y_i}, \mathcal{S}_{i+1} = \frac{y-y_i}{y_{i+1}-y_i}$, thus focus on enlarging the probabilities of the values around target $y$.
For negative image pairs, we consider all points from the feature map as negative examples, setting the class label of all these points uniformly to zero.  This approach ensures an appropriate distribution of positive and negative samples, facilitating effective training and accurate predictions in our model.  The final loss of distribution-based head can be written as:
\begin{equation}
    \begin{split}
        \mathcal{L}_{\operatorname{dist.}} &=
        \frac{1}{N_{pos}}(
            \lambda_{l1}L_{l1}
            + \lambda_{\operatorname{GIoU}}L_{\operatorname{GIoU}}
            + \lambda_{\operatorname{DFL}}L_{\operatorname{DFL}}\\
            &+ L_{\operatorname{QFL}}
        ) + 
        \frac{1}{N_{neg}} \lambda_{\operatorname{QFL}}L_{\operatorname{QFL}}
\end{split}
\end{equation}

We set $\lambda_{l1} = 5$ and $\lambda_{GIoU} = 2$ in all the above heads as in~\cite{ostrack}.  In distribution-based head, we set $\lambda_{\operatorname{DFL}} = 0.2$ as in~\cite{GFL}.




\input{head_table}



\section{Experiments}
\input{lasot_tknet_table}
\input{vot2020_table}
% \input{lasot_table}

% This section first presents the implementation details and the results of our tracker on multiple benchmarks, with comparisons to state-of-the-art methods. Then, ablation studies are presented to analyze the effects of the key components in the proposed networks. We also report the results of other candidate frameworks and compare them with our method to demonstrate its superiority. Finally, visualization of of the distribution-based head is provided.  
% \vspace{-2mm}
\subsection{Setup}
\label{setup}
\textbf{Implementation Details:}
The backbone we use is a standard transformer encoder from ViT-Base.  We initialize the weight of encoder from pretrained parameters by CAE~\cite{cae}.
For testing the efficiency of our method to the baseline, we proposed two versions with different input resolutions:
\begin{itemize}
\item \textbf{JN-256.}  Backbone: ViT-Base; Search region: 256$\times$256 pixels; Template: 144$\times$144 pixels.
\item \textbf{JN-384.}  Backbone: ViT-Base; Search region: 384$\times$384 pixels; Template: 192$\times$192 pixels.
\end{itemize} 

\textbf{Training.}
For the GOT-10k~\cite{GOT-10k} benchmark, we only use the training split of GOT-10k following the one-shot protocols and train the model for 60 epochs.  For the other benchmarks, the training splits of GOT-10k, COCO~\cite{COCO}, LaSOT~\cite{LaSOT} and TrackingNet~\cite{TrackingNet} are used for training in 180 epochs. Common data augmentations including horizontal flip and brightness jittering are used in training.  For JN-256, we train the model on a single RTX 3090 GPU, holding 64 image pairs in a batch.  For JN-384, we train the model on two RTX 3090 GPUs, holding 28 image pairs per GPU, resulting in a batch size of 56.  We train the model with adam~\cite{adamw} optimizer, set the weight decay to $10^{-4}$. The initial learning rate of backbone for both 256 and 384 version are set as to $2\times10^{-5}$ and other paremeters to $2\times10^{-5}$.  We decrease the learning rate by the factor of 0.1 in the last 10\% epochs.  We set defalut ratio of positive samples $\rho$ to 0.7.


\textbf{Inference.} During inference, we adopt Hanning window penalty to utilize positional prior like scale change and motion smoothness in tracking, following the common practice~\cite{SiamFC, SwinTrack, ostrack}. The output score map is simply element-wise multiplied by the Hanning window with the same size, and we choose the box with the highest multiplied score as the target box.


\subsection{Results and Comparisons}

\textbf{GOT-10k.}
GOT-10k~\cite{GOT-10k} is a large-scale generic object tracking benchmark with 10000 video sequences, which includes 180 videos for testing. Note that it is zero-class overlap between the train subset and test subset, which requires the ability to generalization of a tracker. Following the one-shot protocol, we train our models only with the GOT-10K train set and evaluate it with other state-of-the-art tracking methods on the test set.  As reported in Tab.~\ref{table:got10k}, with the same ViT-Base backbone, our base model JN-256 achieve amazingly 75.8\% Average Overlap(AO). Note that the our model is trained with only 60 epoch, and keeps excellent speed-accuracy trade-off.  Furthermore, JN-384 obtains a new state-of-the-art AO score of 76.3\%, surpassing ARTrack-384 by 0.9\%. The results prove the generalization ability of our proposed method on unseen target classes.


\textbf{TrackaingNet.}
TrackingNet~\cite{TrackingNet} is a large-scale short-term tracking benchmark containing 511 video sequences in the test set, which covers diverse target classes. As reported in Tab.~\ref{table:tknet_lasot}, our JN-384 achieve 84.9 \%AUC, surpassing OSTrack-384 by 1.0\%. Notice that we trained our model only with half image pairs of that in OSTrack.  Moreover, our JN-256 with input resolution of 256$\times$256 also performs better than OSTrack-384 with input resolution of 384$\times$384.

\textbf{VOT2020.}
VOT2020~\cite{VOT2020} is a challenging short-term tracking benchmark that is evaluated by target segmentation results. To evaluate JN, we use AlphaRefine~\cite{Alpha-Refine} to generate segmentation masks, and the results are shown in Tab.~\ref{table:vot2020}. The overall performance is ranked by the Expected Average Overlap (EAO). Our tracker exhibits very competitive performance even with a 256-input base model.

\textbf{LaSOT.}
LaSOT~\cite{LaSOT} is a challenging long-term tracking benchmark, which contains 280 videos for testing. We compare the result of our JN with SOTA trackers in Tab.~\ref{table:tknet_lasot}, JN-256 achieves an AUC of 67.9\%, while JN-384 slightly improves the score to 68.5\% AUC. Despite the less pronounced difference, these results are substantial, considering the complexities of long-term tracking.

\subsection{Ablation Study}

\textbf{Joint Learning with Different Heads.}
We examined the joint learning strategy (comprising 70\% positive and 30\% negative samples) on different head designs: center, corner, and distribution-based heads. As shown in Tab.~\ref{integration}, when applying joint learning, the performance of the center head slightly declines. This might be due to the limited interaction between its branches. Surprisingly, the corner head shows an improvement with joint learning, which is different from the findings in STARK where joint learning negatively affected the corner head. One potential reason for this difference could be the sample ratio used in STARK's joint learning. In our method, the presence of the \textit{Aware Path} between the localization and classification branches seems to enhance the benefits of joint learning. This setup allows more interaction than the center head without causing the issues observed in the corner head.

\textbf{Different Ways of Inception of Target Position.}
\input{inception_table}
Except for our proposed target-indicating token, we test different methods (as depicted in Fig.~\ref{inception}) for the integration of target position information, as shown in Tab.~\ref{table:inception}. By adding the embedding of a pseudo mask generated from the bounding box, we observe a decrease in performance compared to the baseline without any integration. Concatenating the tokens enhances the performance, but interestingly, using only one token appears to be slightly superior to employing four tokens for each number. Additionally, we experiment with sine-cosine position embedding, which yields some improvement in performance compared to the baseline. However, this improvement is not as significant as when using a Multilayer Perceptron (MLP).

\textbf{Convergence and Ratio of Positive.}
\label{ConvergenceandRatio}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{ratio_graph} 
    % \vspace{-2mm}
    \caption{Average Overlap on GOT-10k validation set with different ratio $\rho$ and different epochs.}
    \label{ratio}
    % \vspace{-2mm}
\end{figure}
We perform experiments on the GOT-10k validation set to discern our tracker's convergence time and the best negative sample ratio for a balance between accuracy and efficiency. For stability, we compute the mean Average Overlap (AO) from the final five epochs to lessen performance fluctuations. The experimental procedure is detailed in Sec.~\ref{setup}, with the learning rate reduced to 10\% during the final 20\% of epochs.
Fig.~\ref{ratio} indicates our tracker's convergence within 60 epochs on GOT-10k, reaching an AO of 74.3\%. While training exclusively on positive samples gives better AO for fewer epochs, introducing negative samples enhances performance over extended training. Yet, overloading with negative samples can degrade results. We find the optimal performance at a ratio, $\rho=0.7$, emphasizing the significance of an appropriate negative sample ratio in training.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{TIT_attn_map_main} 
    % \vspace{-2mm}
    \caption{Visualization of attention map (of TIT attending to the tokens from the template) in the encoder.}
    \label{attn_map}
% \vspace{-4mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{distribution} 
    % \vspace{-2mm}
    \caption{Due to occlusion, the boundaries of objects are not clear enough, thus the model predicts uncertain distribution.  X-axis in each distribution diagram denotes for the non-normalized distance to the start point.}
    \label{dist}
% \vspace{-4mm}
\end{figure}



\textbf{Visualization.}
In Fig.~\ref{attn_map}, we visualize the attention from the TIT to other image tokens in the template by averaging attention weights across heads, reshaping to 2D, and upscaling to the image's resolution. By visualization, the utility of TIT in aiding target recognition becomes evident.
Moreover, Fig.~\ref{dist} depicts the distribution of distances from the point with the highest classification score in the distribution-based head. This highlights the model's response to partial occlusions, such as a penguin's head obscured by feet or a boat mostly hidden behind a fence. In these cases, the model presents a spread-out, ambiguous distance distribution.  More visualization are provided in Appendix F.

\section{Conclusion}
In this work, we introduce JN, a tracker that incorporates joint learning with negative samples. Extensive experiments demonstrate the effectiveness of our joint learning strategy, proving its utility in conjunction with a distribution-based head adopted from existing designs. Furthermore, we propose a target-indicating token that enhances the tracker's utilization of the template by directly providing the target's location within the template image, demonstrating the multi-modality of the Transformer. Even with fewer training samples, our method achieves superior performance on shor-term tracking benchmarks. We hope this work will inspire further research on training efficiency in object tracking.

\input{supplementary.tex}

\bibliography{aaai24,vot}
% \clearpage
% \maketitle
% \section*{Appendix}
\end{document}
