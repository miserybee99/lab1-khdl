@article{BERT,
    author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    year = {2019},
    journal = {arXiv.1810.04805},
    doi = {10.48550/arXiv.1810.04805}
}

@article{SNLI,
	author = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
	title = {A large annotated corpus for learning natural language inference.},
	year = {2015},
    journal = {arXiv:1508.05326},
	doi = {10.48550/arXiv.1508.05326}
}

@article{MNLI,
	author = {Adina Williams and Nikita Nangia and Samuel R. Bowman},
	title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
	year = {2017},
    journal = {arXiv.1704.05426},
	doi = {10.48550/arXiv.1704.05426}
}


@online{chatbot,
	author = {SOCIALLIBRERIA SRL},
	title = {COMPRENSIONE DELLA NECESSITÀ DI NLP NEL TUO CHATBOT},
	year = {2019},
	url = {https://www.socialibreria.com/index.php/2019/10/05/comprensione-della-necessita-di-pnl-nel-tuo-chatbot/}
}

@article{KD,
	author = {Nils Reimers and Iryna Gurevych},
	title = {Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation},
	year = {2020},
    journal = {arXiv.2004.09813},
	doi = {10.48550/arXiv.2004.09813}
}

@online{ACTIVATION,
	author = {Shashank Shanu},
	title = {Activation Functions In Neural Network},
	year = {2020},
	url = {https://insideaiml.com/blog/Activation-Functions-In-Neural-Network-1089}
}

@article{END-TO-END,
    author = {Alexander Amini and Ava Soleimany and Sertac Karaman and Daniela Rus},
    title = {Spatial Uncertainty Sampling for End-to-End Control},
    year = {2018},
    journal = {arXiv.1805.04829},
    doi = {10.48550/arXiv.1805.04829}    
}

@article{TED2020,
    title = "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
    author = "Qi, Ye  and
      Sachan, Devendra  and
      Felix, Matthieu  and
      Padmanabhan, Sarguna  and
      Neubig, Graham",
    year = "2018",
    journal = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    doi = "10.18653/v1/N18-2084",
}

@online{RNN,
    author = {MAHENDRAN VENKATACHALAM},
    title = {Recurrent Neural Networks \hyphen Remembering what\’s important},
    year = {2019},
    url = {https://gotensor.com/2019/02/28/recurrent-neural-networks-remembering-whats-important}
}

@article{GD,
    author = {Yoshua Bengio and Patrice Simard and Paolo Frasconi},
    title = {Learning long-term dependencies with gradient descent is difficult},
    year = {1994},
    journal = {},
    doi = {10.1109/72.279181}
}

@article{ATTENTION,
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    title = {Attention is all you need},
    year = {2017},
    journal = {arXiv.1706.03762},
    doi = {10.48550/arXiv.1706.03762}
}

@online{NMT,
    author = {KantanAI},
    title = {NMT has arrived \hyphen \space can we now agree on what ``quality'' means?},
    year = {2019},
    url = {https://kantanmtblog.com/2019/02/28/nmt-has-arrived-can-we-now-agree-on-what-quality-means}
}

@online{POSITIONAL,
    author = {Mehreen Saeed},
    title = {A Gentle Introduction to Positional Encoding in Transformer Models, Part 1},
    year = {2022},
    url = {https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1}    
}

@online{SELF-ATTENTION,
    author = {Raimi Karim},
    title = {Illustrated: Self-Attention},
    year = {2019},
    url = {https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a}
}

@article{RESIDUAL,
    author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    title = {Deep residual learning for image recognition},
    year = {2015},
    journal = {arXiv.1512.03385},
    doi = {10.48550/arXiv.1512.03385}
}

@article{LAYER,
    author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    title = {Layer Normalization},
    year = {2016},
    journal = {arXiv.1607.06450},
    doi = {10.48550/arXiv.1607.06450}
}

@article{LSTM,
author = {Zarzycki, Krzysztof and Ławryńczuk, Maciej},
year = {2021},
month = {08},
pages = {5625},
title = {LSTM and GRU Neural Networks as Models of Dynamical Processes Used in Predictive Control: A Comparison of Models Developed for Two Chemical Reactors},
volume = {21},
journal = {Sensors},
doi = {10.3390/s21165625}
}


@article{bilstm-orig,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@online{NLP,
    author = {SAS Analytics},
    title = {Natural Language Processing (NLP). What it is and why it matters},
    year = {2020},
    url = {https://www.sas.com/en\_us/insights/analytics/what-is-natural-language-processing-nlp.html}
}

@online{COSINE,
    author = {NIST},
    title = {COSINE DISTANCE
COSINE SIMILARITY
ANGULAR COSINE DISTANCE
ANGULAR COSINE SIMILARITY},
    year = {2017},
    url = {https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/cosdist.htm}
}

@article{GRU,
    author = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
    title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
    year = {2014},
    journal = {arXiv.1412.3555},
    doi = {10.48550/arXiv.1412.3555}
}

@article{VANISHING,
    author = {Liyuan Liu and Xiaodong Liu and Jianfeng Gao and Weizhu Chen and Jiawei Han},
    title = {Understanding the Difficulty of Training Transformers},
    year = {2020},
    journal = {},
    doi = {10.18653/v1/2020.emnlp-main.463}
}

@online{MULTIHEAD,
    author = {J. Alammar},
    title = {The Illustrated Transformer [blog post]},
    year = {2018},
    url = {https://jalammar.github.io/illustrated-transformer/}
}
@article{DBLP:journals/corr/abs-2002-04815,
  author       = {Youwei Song and
                  Jiahai Wang and
                  Zhiwei Liang and
                  Zhiyue Liu and
                  Tao Jiang},
  title        = {Utilizing {BERT} Intermediate Layers for Aspect Based Sentiment Analysis
                  and Natural Language Inference},
  journal      = {CoRR},
  volume       = {abs/2002.04815},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.04815},
  eprinttype    = {arXiv},
  eprint       = {2002.04815},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-04815.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{dong-etal-2014-adaptive,
    title = "Adaptive Recursive Neural Network for Target-dependent {T}witter Sentiment Classification",
    author = "Dong, Li  and
      Wei, Furu  and
      Tan, Chuanqi  and
      Tang, Duyu  and
      Zhou, Ming  and
      Xu, Ke",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-2009",
    doi = "10.3115/v1/P14-2009",
    pages = "49--54",
}
@inproceedings{pontiki-etal-2014-semeval,
    title = "{S}em{E}val-2014 Task 4: Aspect Based Sentiment Analysis",
    author = "Pontiki, Maria  and
      Galanis, Dimitris  and
      Pavlopoulos, John  and
      Papageorgiou, Harris  and
      Androutsopoulos, Ion  and
      Manandhar, Suresh",
    booktitle = "Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S14-2004",
    doi = "10.3115/v1/S14-2004",
    pages = "27--35",
}

@inproceedings{magnini-etal-2014-excitement,
    title = "The Excitement Open Platform for Textual Inferences",
    author = "Magnini, Bernardo  and
      Zanoli, Roberto  and
      Dagan, Ido  and
      Eichler, Kathrin  and
      Neumann, Guenter  and
      Noh, Tae-Gil  and
      Pado, Sebastian  and
      Stern, Asher  and
      Levy, Omer",
    booktitle = "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-5008",
    doi = "10.3115/v1/P14-5008",
    pages = "43--48",
}
@inproceedings{giampiccolo-etal-2007-third,
    title = "The Third {PASCAL} Recognizing Textual Entailment Challenge",
    author = "Giampiccolo, Danilo  and
      Magnini, Bernardo  and
      Dagan, Ido  and
      Dolan, Bill",
    booktitle = "Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing",
    month = jun,
    year = "2007",
    address = "Prague",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W07-1401",
    pages = "1--9",
}
@article{Bos2009TextualEA,
  title={Textual Entailment at EVALITA 2009},
  author={Johan Bos and Fabio Massimo Zanzotto and Marco Pennacchiotti},
  year={2009}
}
@inproceedings{Pakray2012RecognizingTE,
  title={Recognizing Textual Entailment in Non-english Text via Automatic Translation into English},
  author={Partha Pakray and Snehasis Neogi and Sivaji Bandyopadhyay and Alexander Gelbukh},
  booktitle={Mexican International Conference on Artificial Intelligence},
  year={2012}
}

@article{FEEDFORWARD,
    author = {Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
    title = {Transformer Feed-Forward Layers Are Key-Value Memories},
    year = {2020},
    journal = {arXiv.2012.14913},
    doi = {10.48550/arXiv.2012.14913}
}

@article{SENTENCEBERT,
    author = {Nils Reimers and Iryna Gurevych},
    title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
    year = {2019},
    journal = {arXiv.1908.10084},
    doi = {10.48550/arXiv.1908.10084}
}

@online{PINECONE,
    author = {Pinecone AI},
    title = {Sentence Transformers: Meanings in Disguise},
    year = {2020},
    url = {https://www.pinecone.io/learn/sentence-embeddings/}
}

@article{NLLB,
    author = {NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
    title = {No Language Left Behind: Scaling Human-Centered Machine Translation},
    year = {2022},
    journal = {arXiv.2207.04672},
    doi = {10.48550/arXiv.2207.04672}
}

@article{ROBERTA,
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    year = {2019},
    journal = {arXiv.1907.11692},
    doi = {10.48550/arXiv.1907.11692}
}

@online{BERTTASK,
    author = {J. Alammar},
    title = {The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)},
    year = {2018},
    url = {https://jalammar.github.io/illustrated-bert/}
}

@article{GELU,
    author = {Dan Hendrycks and Kevin Gimpel},
    title = {Gaussian Error Linear Units (GELUs)},
    year = {2020},
    journal = {arXiv.1606.08415},
    doi = {10.48550/arXiv.1606.08415}
}

@article{EVALITA,
    author = {Pierpaolo Basile and Danilo Croce and Valerio Basile and Marco Polignano},
    title = {Overview of the EVALITA 2018 Aspect-based Sentiment Analysis task (ABSITA)},
    year = {2018},
    journal = {Proceedings of the 6th evaluation campaign of Natural Language Processing and Speech tools for Italian (EVALITA’18)},
    url = {https://ceur-ws.org/Vol-2263/paper003.pdf}
}

@article{MULTILINGUAL,
    author = {Alessandro Manenti and Alfredo Braunstein},
    title = {Deep Learning techniques for Natural Language Processing: A multilingual encoder model for NLI task},
    year = {2022},
    journal = {Politecnico di Torino, Corso di laurea magistrale in Physics Of Complex Systems},
    url = {https://webthesis.biblio.polito.it/24750/}
}

@article{SIAMESE,
    author = {Lev V. Utkin and Maxim S. Kovalev and Ernest M. Kasimov},
    title = {An explanation method for Siamese neural networks},
    year = {2019},
    journal = {arXiv.1911.07702},
    doi = {10.48550/arXiv.1911.07702}
}
@article{attention-original,
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Y.},
year = {2014},
month = {09},
pages = {},
title = {Neural Machine Translation by Jointly Learning to Align and Translate},
volume = {1409},
journal = {ArXiv}
}
@article{BILSTM,
    author = {Reza Ghaeini and Sadid A. Hasan and Vivek Datla and Joey Liu and Kathy Lee and Ashequl Qadir and Yuan Ling and Aaditya Prakash and Xiaoli Z. Fern and Oladimeji Farri},
    title = {DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference},
    year = {2018},
    journal = {arXiv.1802.05577},
    doi = {10.48550/arXiv.1802.05577}
}

@article{CONCORD,
    author = {Eric Mitchell and Joseph J. Noh and Siyan Li and William S. Armstrong and Ananth Agarwal and Patrick Liu and Chelsea Finn and Christopher D. Manning},
    title = {Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference},
    year = {2022},
    journal = {arXiv.2211.11875},
    doi = {10.48550/arXiv.2211.11875}
}

@article{T5,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    year = {2019},
    journal = {arXiv.1910.10683},
    doi = {10.48550/arXiv.1910.10683}
}

@inproceedings{hypernymy,
    title = "Hypernymy Detection for Low-Resource Languages via Meta Learning",
    author = "Yu, Changlong  and
      Han, Jialong  and
      Zhang, Haisong  and
      Ng, Wilfred",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.336",
    doi = "10.18653/v1/2020.acl-main.336",
    pages = "3651--3656"
}

@article{XLNET,
    author = {Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
    title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
    year = {2019},
    journal = {arXiv.1906.08237},
    doi = {10.48550/arXiv.1906.08237}
}

@article{HINTON,
    author = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
    title = {Distilling the Knowledge in a Neural Network},
    year = {2015},
    journal = {arXiv.1503.02531},
    doi = {10.48550/arXiv.1503.02531}
}

@article{TRANSFORMERS,
    author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Remi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander Rush},
    title = {Transformers: State-of-the-Art Natural Language Processing},
    year = {2020},
    journal = {arXiv.1910.03771},
    doi = {/10.48550/arXiv.1910.03771}
}