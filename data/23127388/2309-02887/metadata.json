{
  "title": "A deep Natural Language Inference predictor without language-specific training data",
  "authors": [
    "Lorenzo Corradi",
    "Alessandro Manenti",
    "Francesca Del Bonifro",
    "Francesco Setti",
    "Dario Del Sorbo"
  ],
  "submission_date": "2023-09-06T10:20:59+00:00",
  "revised_dates": [
    "2023-09-06T10:20:59+00:00"
  ],
  "publication_venue": null,
  "abstract": "In this paper we present a technique of NLP to tackle the problem of\ninference relation (NLI) between pairs of sentences in a target language of\nchoice without a language-specific training dataset. We exploit a generic\ntranslation dataset, manually translated, along with two instances of the same\npre-trained model - the first to generate sentence embeddings for the source\nlanguage, and the second fine-tuned over the target language to mimic the\nfirst. This technique is known as Knowledge Distillation. The model has been\nevaluated over machine translated Stanford NLI test dataset, machine translated\nMulti-Genre NLI test dataset, and manually translated RTE3-ITA test dataset. We\nalso test the proposed architecture over different tasks to empirically\ndemonstrate the generality of the NLI task. The model has been evaluated over\nthe native Italian ABSITA dataset, on the tasks of Sentiment Analysis,\nAspect-Based Sentiment Analysis, and Topic Recognition. We emphasise the\ngenerality and exploitability of the Knowledge Distillation technique that\noutperforms other methodologies based on machine translation, even though the\nformer was not directly trained on the data it was tested over.",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "arxiv_id": "2309.02887"
}