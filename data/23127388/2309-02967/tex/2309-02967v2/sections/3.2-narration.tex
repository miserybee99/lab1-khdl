\subsection{Narration}
\label{sec:narration}

\begin{figure*}[htbp]
\centering
\includegraphics{figures/narration.pdf}
\caption{
\changed{Creating narration and animations involves multiple sub-tasks for \gpt{}, depicted by the inner blue boxes and arrows.
The starting position of the arrow indicates the information contained within the prompt (with ``Data Table'' being a necessary element for each prompt). 
Each blue box represents the output generated by \gpt{}.
(a1)-(a8) are part of prompt components.
We use dotted gray lines and numbers to illustrate the prompt usage during the process.
(b) The insight structure in JSON format.
Sentences and phrases will be added in animation generation.
Italic text with a blue background represents an example of the prompt output, using the ``airport chart'' in \autoref{fig:ChartUnder}(a), to demonstrate how the process works.
}} 
\label{fig:narration}
\end{figure*}

In this section, we introduce methods to generate audio narration.
High-quality audio narrations can add informative context and direct users' attention~\cite{rubab2023exploring}.
From the previous analysis~\cite{cheng2022investigating}, narration often involves data contexts and insights.
We accept a similar structure: contextual information following interesting insights.

Researchers have adopted various methods to generate insights~\cite{ma2021MetaInsight, ding2019QuickInsights, deng2022dashbot}.
Inspired by the remarkable proficiency of large language models (LLMs) in handling open-ended queries, we select \gpt{} to generate narrations.
When presented with a text $prompt$, the \gpt{} model generates a string-format answer. 
Adhering to the fundamental principles~\footnote{https://beta.openai.com/docs/guides/completion/prompt-design} of ``show and tell'', ``provide quality data'', and ``check settings'', we devise distinct prompts for the following various tasks.

Since GPT-3 cannot process images directly, we use the chart specification as input. 
We create a data table by extracting the ``ac-data'' attribute from the labeled SVG.
In addition, we gather meta information such as the chart's title, x-axis, and y-axis labels. 
This data table and meta-information serve as the foundational components of each prompt (\autoref{fig:narration}(a1)).
Considering the token limitation of prompts (the maximum number of words or tokens that can be used as input to the model at once), we represent the table in a CSV format.
Moreover, we follow established design guidelines \cite{openai2023} and develop distinct prompts tailored to each specific task.

\subsubsection{Narration with Contextual Information}
Our narration starts with a sentence that describes the chart.
We follow the ``Level 1 semantic content'' framework proposed by Lundgard~\cite{lundgard2021accessible}, which specifies that the description should encompass the chart type, title, legend, encoding channels, and axis information. 
Thus, we design the prompt as illustrated in \autoref{fig:narration}(a2), incorporating the basic chart information from \autoref{fig:narration}(a1) to fulfill our requirements.
The resulting output of this step from \gpt{} will consist of two sentences: the first about the chart type and title and the second addressing the legend, encoding channels, and axis information.
\changed{The example sentences are depicted in \autoref{fig:narration}(a2).}


\subsubsection{Narration with Insights}
Engaging insights is the foundation of a high-quality narration~\cite{srinivasan2019augmenting}.
Following the definitions of facts provided in a prior study~\cite{wang2020DataShot}, we have eight facts as prospective insights for three types of charts: value, proportion, difference, trend, rank, aggregation, extreme, and outlier. 
Within each insight type, there can be various sub-types. 
For example, the proportion insight may encompass majority and minority data. Consequently, we expand the sub-insights, as detailed in \autoref{table:subinsights}, based on the definitions of these eight insights.

\begin{table}[htbp]
\centering
\caption{The table presents the insight type and \\corresponding sub-insight types.}
\begin{tabular}{ll}
\toprule
Insight Type & Sub-insight Type \\ \midrule
Proportion  &  Majority, minority.\\
Extreme   &  Maximum, minimum.\\
Distribution  & Normal, uniform, none.\\
Aggregation & Average, sum, count. \\
Trend & Fluctuate, increase, decrease, \\
& fluctuate increase, fluctuate decrease. \\
\bottomrule
\end{tabular}

\label{table:subinsights}
\end{table}
In the beginning, we expected that \gpt{} would produce several relevant insights from the data table. 
However, this complex task requires a significant amount of input text comprising multiple sentences, which may exceed the maximum allowable number of tokens.
Consequently, we subdivide the task into three sub-tasks: generating all insights, distilling insights, and creating narration.

We first generate all insights with individual prompts for each insight.
These prompts consist of insight definitions (\autoref{fig:narration}(a3)) referenced from previous research~\cite{wang2020DataShot, shi2021autoclips} and basic chart information (\autoref{fig:narration}(a1)).
To ensure efficient data analysis, we require \gpt{} to generate JSON output (\autoref{fig:narration}(b))~\cite{{wei2023leveraging}}. Sometimes, extra explanatory text is added to the output, so we utilize regular expressions to preserve the intended format. Examples can be found in the supplementary.
In this step, we ask \gpt{} to simultaneously identify relevant data for each sub-insight. 
From our experiment and recent researches~\cite{zong2022survey}, \gpt{}'s proficiency in handling mathematical computations, particularly those involving complex calculations (\textit{extreme} and \textit{rank} insights), is limited.
\minor{To address this, we introduce a validation procedure to verify output values. In cases where inaccuracies are identified, heuristic approaches are applied to generate insights, as demonstrated in the supplementary material.}
    
To craft a compelling data narrative, we focus on the salient insights generated by \gpt{}. Thus, we aim to distill insights.
With all insights and the basic chart information (\autoref{fig:narration}(a1)), we task \gpt{} with comparing the relationship between each insight and the visual representation of the data (\autoref{fig:narration}(a4)).
After processing the input data, \gpt{} will generate an output that lists several relevant insight types. 
Taking the example of the ``airport chart'', the result is \textit{``trend''} and \textit{``difference''}.

We then create the narration.
A captivating narrative should interconnect each sentence and flow cohesively rather than being disjointed and independent.
Therefore, we feed all distilled insights into the prompt (\autoref{fig:narration}(a5)), to elicit a coherent and clear narration from \gpt{}.

\subsubsection{Rephrase the Narration}
After acquiring the two segments of narration, we combine them as a new prompt for \gpt{} to refine the final narration (\autoref{fig:narration}(a6)). 
We expect the sentences in the two parts to have smooth transitions between them. 
Keeping the original meaning and essential words, such as the title and axis, \gpt{} will create a refined and cohesive narration by seamlessly connecting the contextual and insightful sentences.
Moreover, we require \gpt{} to re-assign the narration as two parts for the next step. 
\changed{As shown in \autoref{fig:narration}(a6), the red words indicate the adjusted version, and the words in the brackets are omitted.}
Finally, we derive a new narration for contextual information and a new narration for insights.

% tts
We then leverage text-to-speech techniques (Azure TTS API\footnote{https://azure.microsoft.com/services/cognitive-services/text-to-speech}) to convert the narration into an audio narrator.

