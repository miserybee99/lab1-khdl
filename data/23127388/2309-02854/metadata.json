{
  "title": "A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques",
  "authors": [
    "Max Landauer",
    "Florian Skopik",
    "Markus Wurzenberger"
  ],
  "submission_date": "2023-09-06T09:31:17+00:00",
  "revised_dates": [
    "2023-09-06T09:31:17+00:00"
  ],
  "publication_venue": "Proceedings of the ACM on Software Engineering (FSE 2024)",
  "abstract": "Log data store event execution patterns that correspond to underlying\nworkflows of systems or applications. While most logs are informative, log data\nalso include artifacts that indicate failures or incidents. Accordingly, log\ndata are often used to evaluate anomaly detection techniques that aim to\nautomatically disclose unexpected or otherwise relevant system behavior\npatterns. Recently, detection approaches leveraging deep learning have\nincreasingly focused on anomalies that manifest as changes of sequential\npatterns within otherwise normal event traces. Several publicly available data\nsets, such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop, have since become\nstandards for evaluating these anomaly detection techniques, however, the\nappropriateness of these data sets has not been closely investigated in the\npast. In this paper we therefore analyze six publicly available log data sets\nwith focus on the manifestations of anomalies and simple techniques for their\ndetection. Our findings suggest that most anomalies are not directly related to\nsequential manifestations and that advanced detection techniques are not\nrequired to achieve high detection rates on these data sets.",
  "categories": [
    "cs.LG"
  ],
  "arxiv_id": "2309.02854"
}