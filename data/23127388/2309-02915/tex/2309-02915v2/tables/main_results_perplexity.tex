%\begin{table}
\begin{wraptable}{r}{0.6\textwidth}
\centering
{\scalebox{1}{\begin{tabular}{lrr}
\toprule
\textbf{Model} &\multicolumn{2}{c}{\textbf{Perplexity} $\downarrow$}  \\\cmidrule{2-3}
\textbf{} &\textbf{Twitter} &\textbf{YouTube} \\\midrule
GCM* & 4331.85 &4323.15  \\
CM-XLM* & 5413.22 &1603.59 \\
VACS & 361.05 & 552.35 \\
\hline
Transformer & 680.07 & 473.84 \\
%(+) Distillation & 445.41 & 375.24 \\
%(+) Meta Distillation & 797.84 & 506.82 \\
\hline
\rowcolor{maroon!10}  {\modelname} & \textbf{297.43} & \textbf{196.21}  \\
 \hdashline
 %\rowcolor{maroon!10} (+) Distillation & 377.64 & 294.87 \\
%\rowcolor{maroon!10} (+) Meta Distillation & 567.90 & 414.42 \\
%\hdashline
\rowcolor{maroon!10}  (-) Contextual Persona & 320.79 & 295.26 \\
\rowcolor{maroon!10}  (-) Speaker ID & 582.24 & 371.70 \\
\rowcolor{maroon!10}  (-) Alignment & 377.63 & 294.83 \\
\rowcolor{maroon!10}  (-) FAME & 864.98 & 583.09 \\
\bottomrule
\end{tabular}
}}
\caption{Intrinsic evaluation of the competing models based on perplexity ($\downarrow$: lower value indicates better performance). For models highlighted with *, perplexity is calculated with word-level generation. \color{black} \textbf{Bold} indicates the best results among all the models.}
\label{tab:main_result_perplexity}
%\vspace{-8mm}
%\end{table}
\end{wraptable}