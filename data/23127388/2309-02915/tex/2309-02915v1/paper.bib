
@Article{chow:68,
  author = 	 {C. K. Chow and C. N. Liu},
  title = 	 {Approximating discrete probability distributions with dependence trees},
  journal = 	 {IEEE Transactions on Information Theory},
  year = 	 {1968},
  volume = 	 {IT-14},
  number = 	 {3},
  pages = 	 {462--467}}


@Book{pearl:88,
  author = 	 {Judea Pearl},
  title = 	 {Probabilistic {R}easoning in {I}ntelligent {S}ystems: 
		  {N}etworks of {P}lausible {I}nference},
  publisher = 	 {Morgan Kaufman Publishers},
  year = 	 {1988},
  address = 	 {San Mateo, CA}
}

%% Journal article
@article{bib1,
  author		= "Campbell, S. L. and Gear, C. W.",
  title			= "The index of general nonlinear {D}{A}{E}{S}",
  journal		= "Numer. {M}ath.",
  volume		= "72",
  number		= "2",
  pages			= "173--196",
  year			= "1995"
}

%% Journal article with DOI
@article{bib2,
  author		= "Slifka, M. K. and Whitton, J. L.",
  title			= "Clinical implications of dysregulated cytokine production",
  journal		= "J. {M}ol. {M}ed.",
  volume		= "78",
  pages			= "74--80",
  year			= "2000",
  doi			= "10.1007/s001090000086"
}

%% Journal article
@article{bib3,
  author		= "Hamburger, C.",
  title			= "Quasimonotonicity, regularity and duality for nonlinear systems of 
					partial differential equations",
  journal		= "Ann. Mat. Pura. Appl.",
  volume		= "169",
  number		= "2",
  pages			= "321--354",
  year			= "1995"
}

%% book, authored
@book{bib4,
  author		= "Geddes, K. O. and Czapor, S. R. and Labahn, G.",
  title			= "Algorithms for {C}omputer {A}lgebra",
  address		= "Boston",
  publisher		= "Kluwer",
  year			= "1992"
}

%% Item 8. Book, chapter
@incollection{bib5,
  author		= "Broy, M.",
  title			= "Software engineering---from auxiliary to key technologies",
  editor		= "Broy, M. and Denert, E.",
  booktitle		= "Software Pioneers",
  pages			= "10--13",
  address		= "New {Y}ork",
  publisher		= "Springer",
  year			= "1992"
}

%% Book, edited
@book{bib6,
  editor		= "Seymour, R. S.",
  title			= "Conductive {P}olymers",
  address		= "New {Y}ork",
  publisher		= "Plenum",
  year			= "1981"
}

%% Chapter in a book in a series with volume titles
@inproceedings{bib7,
  author		= "Smith, S. E.",
  title			= "Neuromuscular blocking drugs in man",
  editor		= "Zaimis, E.",
  volume		= "42",
  booktitle		= "Neuromuscular junction. {H}andbook of experimental pharmacology",
  pages			= "593--660",
  address		= "Heidelberg",
  publisher		= "Springer",
  year			= "1976"
}

%% Paper presented at a conference
@misc{bib8,
  author		= "Chung, S. T. and Morris, R. L.",
  title			= "Isolation and characterization of plasmid deoxyribonucleic acid from 
					Streptomyces fradiae",
  year			= "1978",
  note			= "Paper presented at the 3rd international symposium on the genetics 
					of industrial microorganisms, University of {W}isconsin, {M}adison, 
					4--9 June 1978"
}

%% Data citation example
@misc{bib9,
  author		= "Hao, Z. and AghaKouchak, A. and Nakhjiri, N. and Farahmand, A.",
  title			= "Global integrated drought monitoring and prediction system (GIDMaPS) data sets", 
  year			= "2014",
  note			= "figshare \url{https://doi.org/10.6084/m9.figshare.853801}"
}

%% Preprint citation example
@misc{bib10, 
  author		= "Babichev, S. A. and Ries, J. and Lvovsky, A. I.",
  title			= "Quantum scissors: teleportation of single-mode optical states by means 
					of a nonlocal single photon", 
  year			= "2002",
  note			= "Preprint at \url{https://arxiv.org/abs/quant-ph/0208066v1}"
}

@article{bib11,
  author		= "Beneke, M. and Buchalla, G. and Dunietz, I.",
  title			= "Mixing induced {CP} asymmetries in inclusive {B} decays",
  journal		= "Phys. {L}ett.",
  volume		= "B393",
  year			= "1997",
  pages			= "132-142",
  archivePrefix		= "arXiv",
  eprint		= "0707.3168",
  primaryClass		= "gr-gc"
}

@softmisc{bib12,
  author		= "Stahl, B.",
  title			= "deep{SIP}: deep learning of {S}upernova {I}a {P}arameters",
  version		= "0.42",
  keywords		= "Software",
  howpublished		= "Astrophysics {S}ource {C}ode {L}ibrary",
  year			= "2020",
  month			= "Jun",
  eid			= "ascl:2006.023",
  pages			= "ascl:2006.023",
  archivePrefix		= "ascl",
  eprint		= "2006.023",
  adsurl		= "{https://ui.adsabs.harvard.edu/abs/2020ascl.soft06023S}",
  adsnote		= "Provided by the SAO/NASA Astrophysics Data System"
}

@article{bib13,
  author = "Abbott, T. M. C. and others",
  collaboration = "DES",
  title = "{Dark Energy Survey Year 1 Results: Constraints on Extended Cosmological Models from Galaxy Clustering and Weak Lensing}",
  eprint = "1810.02499",
  archivePrefix = "arXiv",
  primaryClass = "astro-ph.CO",
  reportNumber = "FERMILAB-PUB-18-507-PPD",
  doi = "10.1103/PhysRevD.99.123505",
  journal = "Phys. Rev. D",
  volume = "99",
  number = "12",
  pages = "123505",
  year = "2019"
}

%%============================================================================%%
%% while using chicago reference style, both abbreviated and expanded form of %%
%% author name format is acceptable. Refer below example for expanded form    %%
%%============================================================================%%

%%  author		= "{Cameron, Deborah}", - single author
%%  author		= "{Saito, Yukio} and {Hyuga, Hiroyuki}", - double author 

%%======================================%%
%% Example for author names with suffix %%
%%======================================%%

%%  author		= "{Price, R. A. Jr} and {Curry, N. {III}} and McCann, K. E. and 
%%					Fielding, J. L. and {Abercrombie, E. Jr}",
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}


@inproceedings{samanta_deep_2019,
	address = {Macao, China},
	title = {A {Deep} {Generative} {Model} for {Code} {Switched} {Text}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/719},
	doi = {10.24963/ijcai.2019/719},
	abstract = {Code-switching, the interleaving of two or more languages within a sentence or discourse is pervasive in multilingual societies. Accurate language models for code-switched text are critical for NLP tasks. State-of-the-art data-intensive neural language models are difﬁcult to train well from scarce language-labeled code-switched text. A potential solution is to use deep generative models to synthesize large volumes of realistic code-switched text. Although generative adversarial networks and variational autoencoders can synthesize plausible monolingual text from continuous latent space, they cannot adequately address code-switched text, owing to their informal style and complex interplay between the constituent languages. We introduce VACS, a novel variational autoencoder architecture speciﬁcally tailored to code-switching phenomena. VACS encodes to and decodes from a two-level hierarchical representation, which models syntactic contextual signals in the lower level, and language switching signals in the upper layer. Decoding representations sampled from prior produced well-formed, diverse code-switched sentences. Extensive experiments show that using synthetic codeswitched text with natural monolingual data results in signiﬁcant (33.06\%) drop in perplexity.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Samanta, Bidisha and Reddy, Sharmila and Jagirdar, Hussain and Ganguly, Niloy and Chakrabarti, Soumen},
	month = aug,
	year = {2019},
	pages = {5175--5181},
	file = {Samanta et al. - 2019 - A Deep Generative Model for Code Switched Text.pdf:/Users/victor/Zotero/storage/EBN9UIMY/Samanta et al. - 2019 - A Deep Generative Model for Code Switched Text.pdf:application/pdf},
}

@inproceedings{song_exploiting_2019,
	address = {Macao, China},
	title = {Exploiting {Persona} {Information} for {Diverse} {Generation} of {Conversational} {Responses}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/721},
	doi = {10.24963/ijcai.2019/721},
	abstract = {In human conversations, due to their personalities in mind, people can easily carry out and maintain the conversations. Giving conversational context with persona information to a chatbot, how to exploit the information to generate diverse and sustainable conversations is still a non-trivial task. Previous work on persona-based conversational models successfully make use of predeﬁned persona information and have shown great promise in delivering more realistic responses. And they all learn with the assumption that given a source input, there is only one target response. However, in human conversations, there are massive appropriate responses to a given input message. In this paper, we propose a memory-augmented architecture to exploit persona information from context and incorporate a conditional variational autoencoder model together to generate diverse and sustainable conversations. We evaluate the proposed model on a benchmark persona-chat dataset. Both automatic and human evaluations show that our model can deliver more diverse and more engaging personabased responses than baseline approaches.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Song, Haoyu and Zhang, Wei-Nan and Cui, Yiming and Wang, Dong and Liu, Ting},
	month = aug,
	year = {2019},
	pages = {5190--5196},
	file = {Song et al. - 2019 - Exploiting Persona Information for Diverse Generat.pdf:/Users/victor/Zotero/storage/DLALL8R2/Song et al. - 2019 - Exploiting Persona Information for Diverse Generat.pdf:application/pdf},
}

@article{hinton_distilling_2015,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@inproceedings{mondal2022cocoa,
  title={CoCoa: An Encoder-Decoder Model for Controllable Code-switched Generation},
  author={Mondal, Sneha and Pathak, Shreya and Jyothi, Preethi and Raghuveer, Aravindan and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={2466--2479},
  year={2022}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{vaswani_attention_2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2022-06-14},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {Number: arXiv:1810.04805
arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/victor/Zotero/storage/C7I4E5BA/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{le_self-attentive_2020,
  title={Self-attentive associative memory},
  author={Le, Hung and Tran, Truyen and Venkatesh, Svetha},
  booktitle={International Conference on Machine Learning},
  pages={5682--5691},
  year={2020},
  organization={PMLR}
}

@inproceedings{chen_accurate_2020,
    title = "Accurate Word Alignment Induction from Neural Machine Translation",
    author = "Chen, Yun  and
      Liu, Yang  and
      Chen, Guanhua  and
      Jiang, Xin  and
      Liu, Qun",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.42",
    doi = "10.18653/v1/2020.emnlp-main.42",
    pages = "566--576",
    abstract = "Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.",
}

@inproceedings{gupta_semi-supervised_2020,
	address = {Online},
	title = {A {Semi}-supervised {Approach} to {Generate} the {Code}-{Mixed} {Text} using {Pre}-trained {Encoder} and {Transfer} {Learning}},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.206},
	doi = {10.18653/v1/2020.findings-emnlp.206},
	abstract = {Code-mixing, the interleaving of two or more languages within a sentence or discourse is ubiquitous in multilingual societies. The lack of code-mixed training data is one of the major concerns for the development of end-to-end neural network-based models to be deployed for a variety of natural language processing (NLP) applications. A potential solution is to either manually create or crowd-source the code-mixed labelled data for the task at hand, but that requires much human efforts and often not feasible because of the language specific diversity in the code-mixed text. To circumvent the data scarcity issue, we propose an effective deep learning approach for automatically generating the code-mixed text from English to multiple languages without any parallel data. In order to train the neural network, we create synthetic code-mixed texts from the available parallel corpus by modelling various linguistic properties of code-mixing. Our codemixed text generator is built upon the encoderdecoder framework, where the encoder is augmented with the linguistic and task-agnostic features obtained from the transformer based language model. We also transfer the knowledge from a neural machine translation (NMT) to warm-start the training of code-mixed generator. Experimental results and in-depth analysis show the effectiveness of our proposed code-mixed text generation on eight diverse language pairs.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Gupta, Deepak and Ekbal, Asif and Bhattacharyya, Pushpak},
	year = {2020},
	pages = {2267--2280},
	file = {Gupta et al. - 2020 - A Semi-supervised Approach to Generate the Code-Mi.pdf:/Users/victor/Zotero/storage/K6X9XXWX/Gupta et al. - 2020 - A Semi-supervised Approach to Generate the Code-Mi.pdf:application/pdf},
}

@inproceedings{rizvi_gcm_2021,
	address = {Online},
	title = {{GCM}: {A} {Toolkit} for {Generating} {Synthetic} {Code}-mixed {Text}},
	shorttitle = {{GCM}},
	url = {https://aclanthology.org/2021.eacl-demos.24},
	doi = {10.18653/v1/2021.eacl-demos.24},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Rizvi, Mohd Sanad Zaki and Srinivasan, Anirudh and Ganu, Tanuja and Choudhury, Monojit and Sitaram, Sunayana},
	year = {2021},
	pages = {205--211},
	file = {Rizvi et al. - 2021 - GCM A Toolkit for Generating Synthetic Code-mixed.pdf:/Users/victor/Zotero/storage/BDX22JYX/Rizvi et al. - 2021 - GCM A Toolkit for Generating Synthetic Code-mixed.pdf:application/pdf},
}

@misc{khanuja_muril_2021,
	title = {{MuRIL}: {Multilingual} {Representations} for {Indian} {Languages}},
	shorttitle = {{MuRIL}},
	url = {http://arxiv.org/abs/2103.10730},
	abstract = {India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011). India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today's state-of-the-art multilingual systems perform suboptimally on Indian (IN) languages. This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn't help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both translated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native to Latin script) test sets of the chosen datasets and demonstrate the efficacy of MuRIL in handling transliterated data.},
	language = {en},
	urldate = {2022-06-14},
	publisher = {arXiv},
	author = {Khanuja, Simran and Bansal, Diksha and Mehtani, Sarvesh and Khosla, Savya and Dey, Atreyee and Gopalan, Balaji and Margam, Dilip Kumar and Aggarwal, Pooja and Nagipogu, Rajiv Teja and Dave, Shachi and Gupta, Shruti and Gali, Subhash Chandra Bose and Subramanian, Vish and Talukdar, Partha},
	month = apr,
	year = {2021},
	note = {Number: arXiv:2103.10730
arXiv:2103.10730 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {1911.02116.pdf:/Users/victor/Zotero/storage/NK3N4N6D/1911.02116.pdf:application/pdf;2020.calcs-1.5.pdf:/Users/victor/Zotero/storage/TWKDTXPT/2020.calcs-1.5.pdf:application/pdf;A Sociolinguistic Study of Linguistic Variation and Code Matrix In Kanpur.pdf:/Users/victor/Zotero/storage/V2HN6GZB/A Sociolinguistic Study of Linguistic Variation and Code Matrix In Kanpur.pdf:application/pdf;Khanuja et al. - 2021 - MuRIL Multilingual Representations for Indian Lan.pdf:/Users/victor/Zotero/storage/8JGVMUYN/Khanuja et al. - 2021 - MuRIL Multilingual Representations for Indian Lan.pdf:application/pdf;The_Dialectics_of_Hinglish_A_Perspective.pdf:/Users/victor/Zotero/storage/LWR85RF6/The_Dialectics_of_Hinglish_A_Perspective.pdf:application/pdf},
}

@inproceedings{sengupta_hit_2021,
  title={HIT-A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation},
  author={Sengupta, Ayan and Bhattacharjee, Sourabh Kumar and Chakraborty, Tanmoy and Akhtar, Md Shad},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={4625--4639},
  year={2021}
}

@inproceedings{srivastava_hinge_2021,
  title={HinGE: A Dataset for Generation and Evaluation of Code-Mixed Hinglish Text},
  author={Srivastava, Vivek and Singh, Mayank},
  booktitle={Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems},
  pages={200--208},
  year={2021}
}

@misc{wang_speaker-aware_2021,
	title = {A {Speaker}-aware {Parallel} {Hierarchical} {Attentive} {Encoder}-{Decoder} {Model} for {Multi}-turn {Dialogue} {Generation}},
	url = {http://arxiv.org/abs/2110.06823},
	abstract = {This paper presents a novel open-domain dialogue generation model emphasizing the differentiation of speakers in multi-turn conversations. Differing from prior work that solely relies on the content of conversation history to generate a response, we argue that capturing relative social relations among utterances (i.e., generated by either the same speaker or different persons) benefits the machine capturing fine-grained context information from a conversation history to improve context coherence in the generated response. Given that, we propose a speaker-aware Parallel Hierarchical Attentive Encoder-Decoder (PHAED) model that aims to model each utterance with the awareness of its speaker and contextual associations with the same speaker's previous messages. Specifically, in a conversation involving two speakers, we regard the utterances from one speaker as responses and those from the other as queries. After understanding queries via our encoder with inner-query and inter-query encodings, our decoder reuses the hidden states of previously generated responses, instead of reconstructing these by the encoder, to generate a new response. Our empirical results show that PHAED outperforms the state-of-the-art in both automatic and human evaluations. Furthermore, our ablation study shows that dialogue models with speaker tokens can generally decrease the possibility of generating non-coherent responses regarding the conversation context.},
	language = {en},
	urldate = {2022-06-14},
	publisher = {arXiv},
	author = {Wang, Zihao and Jiang, Ming and Wang, Junli},
	month = oct,
	year = {2021},
	note = {Number: arXiv:2110.06823
arXiv:2110.06823 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {1911.00536.pdf:/Users/victor/Zotero/storage/J4MHE4N5/1911.00536.pdf:application/pdf;Wang et al. - 2021 - A Speaker-aware Parallel Hierarchical Attentive En.pdf:/Users/victor/Zotero/storage/2UTHTATR/Wang et al. - 2021 - A Speaker-aware Parallel Hierarchical Attentive En.pdf:application/pdf},
}

@misc{liu_meta_2022,
	title = {Meta {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2202.07940},
	abstract = {Recent studies pointed out that knowledge distillation (KD) suffers from two degradation problems, the teacher-student gap and the incompatibility with strong data augmentations, making it not applicable to training state-of-the-art models, which are trained with advanced augmentations. However, we observe that a key factor, i.e., the temperatures in the softmax functions for generating probabilities of both the teacher and student models, was mostly overlooked in previous methods. With properly tuned temperatures, such degradation problems of KD can be much mitigated. However, instead of relying on a naive grid search, which shows poor transferability, we propose Meta Knowledge Distillation (MKD) to meta-learn the distillation with learnable meta temperature parameters. The meta parameters are adaptively adjusted during training according to the gradients of the learning objective. We validate that MKD is robust to different dataset scales, different teacher/student architectures, and different types of data augmentation. With MKD, we achieve the best performance with popular ViT architectures among compared methods that use only ImageNet-1K as training data, ranging from tiny to large models. With ViT-L, we achieve 86.5\% with 600 epochs of training, 0.6\% better than MAE that trains for 1,650 epochs.},
	language = {en},
	urldate = {2022-06-14},
	publisher = {arXiv},
	author = {Liu, Jihao and Liu, Boxiao and Li, Hongsheng and Liu, Yu},
	month = feb,
	year = {2022},
	note = {Number: arXiv:2202.07940
arXiv:2202.07940 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {1901.07291.pdf:/Users/victor/Zotero/storage/TXBL8D9E/1901.07291.pdf:application/pdf;1909.00105.pdf:/Users/victor/Zotero/storage/QYNSLZMV/1909.00105.pdf:application/pdf;1911.02116.pdf:/Users/victor/Zotero/storage/U55GPQBY/1911.02116.pdf:application/pdf;2001.08210.pdf:/Users/victor/Zotero/storage/ZKE3DRX2/2001.08210.pdf:application/pdf;2020.calcs-1.5.pdf:/Users/victor/Zotero/storage/IY8LXYZ8/2020.calcs-1.5.pdf:application/pdf;2102.01672.pdf .pdf:/Users/victor/Zotero/storage/VAZZEQVV/2102.01672.pdf .pdf:application/pdf;Liu et al. - 2022 - Meta Knowledge Distillation.pdf:/Users/victor/Zotero/storage/KJ363UF8/Liu et al. - 2022 - Meta Knowledge Distillation.pdf:application/pdf},
}

@misc{sengupta_comprehensive_2022,
	title = {A {Comprehensive} {Understanding} of {Code}-mixed {Language} {Semantics} using {Hierarchical} {Transformer}},
	url = {http://arxiv.org/abs/2204.12753},
	abstract = {Being a popular mode of text-based communication in multilingual communities, code-mixing in online social media has became an important subject to study. Learning the semantics and morphology of code-mixed language remains a key challenge, due to scarcity of data and unavailability of robust and language-invariant representation learning technique. Any morphologically-rich language can beneﬁt from character, subword, and word-level embeddings, aiding in learning meaningful correlations. In this paper, we explore a hierarchical transformerbased architecture (HIT) to learn the semantics of code-mixed languages. HIT consists of multi-headed self-attention and outer product attention components to simultaneously comprehend the semantic and syntactic structures of code-mixed texts. We evaluate the proposed method across 6 Indian languages (Bengali, Gujarati, Hindi, Tamil, Telugu and Malayalam) and Spanish for 9 NLP tasks on 17 datasets. The HIT model outperforms stateof-the-art code-mixed representation learning and multilingual language models in all tasks. We further demonstrate the generalizability of the HIT architecture using masked language modeling-based pre-training, zero-shot learning, and transfer learning approaches. Our empirical results show that the pretraining objectives signiﬁcantly improve the performance on downstream tasks.},
	language = {en},
	urldate = {2022-06-14},
	publisher = {arXiv},
	author = {Sengupta, Ayan and Suresh, Tharun and Akhtar, Md Shad and Chakraborty, Tanmoy},
	month = apr,
	year = {2022},
	note = {Number: arXiv:2204.12753
arXiv:2204.12753 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Sengupta et al. - 2022 - A Comprehensive Understanding of Code-mixed Langua.pdf:/Users/victor/Zotero/storage/JYLT4YQE/Sengupta et al. - 2022 - A Comprehensive Understanding of Code-mixed Langua.pdf:application/pdf},
}

@article{liu_filling_nodate,
	title = {Filling the {Gap} of {Utterance}-aware and {Speaker}-aware {Representation} for {Multi}-turn {Dialogue}},
	abstract = {A multi-turn dialogue is composed of multiple utterances from two or more different speaker roles. Thus utterance- and speaker-aware clues are supposed to be well captured in models. However, in the existing retrieval-based multi-turn dialogue modeling, the pre-trained language models (PrLMs) as encoder represent the dialogues coarsely by taking the pairwise dialogue history and candidate response as a whole, the hierarchical information on either utterance interrelation or speaker roles coupled in such representations is not well addressed. In this work, we propose a novel model to ﬁll such a gap by modeling the effective utterance-aware and speakeraware representations entailed in a dialogue history. In detail, we decouple the contextualized word representations by masking mechanisms in Transformer-based PrLM, making each word only focus on the words in current utterance, other utterances, two speaker roles (i.e., utterances of sender and utterances of receiver), respectively. Experimental results show that our method boosts the strong ELECTRA baseline substantially in four public benchmark datasets, and achieves various new state-of-the-art performance over previous methods. A series of ablation studies are conducted to demonstrate the effectiveness of our method.},
	language = {en},
	author = {Liu, Longxiang and Zhang, Zhuosheng and Zhao, Hai and Zhou, Xi and Zhou, Xiang},
	pages = {9},
	file = {Liu et al. - Filling the Gap of Utterance-aware and Speaker-awa.pdf:/Users/victor/Zotero/storage/R783YQYS/Liu et al. - Filling the Gap of Utterance-aware and Speaker-awa.pdf:application/pdf},
}

@inproceedings{pratapa_word_2018,
	address = {Brussels, Belgium},
	title = {Word {Embeddings} for {Code}-{Mixed} {Language} {Processing}},
	url = {http://aclweb.org/anthology/D18-1344},
	doi = {10.18653/v1/D18-1344},
	abstract = {We compare three existing bilingual word embedding approaches, and a novel approach of training skip-grams on synthetic code-mixed text generated through linguistic models of code-mixing, on two tasks - sentiment analysis and POS tagging for code-mixed text. Our results show that while CVM and CCA based embeddings perform as well as the proposed embedding technique on semantic and syntactic tasks respectively, the proposed approach provides the best performance for both tasks overall. Thus, this study demonstrates that existing bilingual embedding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Pratapa, Adithya and Choudhury, Monojit and Sitaram, Sunayana},
	year = {2018},
	pages = {3067--3072},
	file = {language_models_are_unsupervised_multitask_learners.pdf:/Users/victor/Zotero/storage/Z2VAM99T/language_models_are_unsupervised_multitask_learners.pdf:application/pdf;Pratapa et al. - 2018 - Word Embeddings for Code-Mixed Language Processing.pdf:/Users/victor/Zotero/storage/HZLMDTBI/Pratapa et al. - 2018 - Word Embeddings for Code-Mixed Language Processing.pdf:application/pdf},
}

@inproceedings{pratapa_language_2018,
	address = {Melbourne, Australia},
	title = {Language {Modeling} for {Code}-{Mixing}: {The} {Role} of {Linguistic} {Theory} based {Synthetic} {Data}},
	shorttitle = {Language {Modeling} for {Code}-{Mixing}},
	url = {http://aclweb.org/anthology/P18-1143},
	doi = {10.18653/v1/P18-1143},
	abstract = {Training language models for Code-mixed (CM) language is known to be a difﬁcult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artiﬁcial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can signiﬁcantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pratapa, Adithya and Bhat, Gayatri and Choudhury, Monojit and Sitaram, Sunayana and Dandapat, Sandipan and Bali, Kalika},
	year = {2018},
	pages = {1543--1553},
	file = {Pratapa et al. - 2018 - Language Modeling for Code-Mixing The Role of Lin.pdf:/Users/victor/Zotero/storage/3EKAJUNK/Pratapa et al. - 2018 - Language Modeling for Code-Mixing The Role of Lin.pdf:application/pdf},
}

@inproceedings{chandu_my_2019,
	address = {Florence, Italy},
	title = {“{My} {Way} of {Telling} a {Story}”: {Persona} based {Grounded} {Story} {Generation}},
	shorttitle = {“{My} {Way} of {Telling} a {Story}”},
	url = {https://www.aclweb.org/anthology/W19-3402},
	doi = {10.18653/v1/W19-3402},
	abstract = {Visual storytelling is the task of generating stories based on a sequence of images. Inspired by the recent works in neural generation focusing on controlling the form of text, this paper explores the idea of generating these stories in different personas. However, one of the main challenges of performing this task is the lack of a dataset of visual stories in different personas. Having said that, there are independent datasets for both visual storytelling and annotated sentences for various persona. In this paper we describe an approach to overcome this by getting labelled persona data from a different task and leveraging those annotations to perform persona based story generation. We inspect various ways of incorporating personality in both the encoder and the decoder representations to steer the generation in the target direction. To this end, we propose ﬁve models which are incremental extensions to the baseline model to perform the task at hand. In our experiments we use ﬁve different personas to guide the generation process. We ﬁnd that the models based on our hypotheses perform better at capturing words while generating stories in the target persona.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the {Second} {Workshop} on {Storytelling}},
	publisher = {Association for Computational Linguistics},
	author = {Chandu, Khyathi and Prabhumoye, Shrimai and Salakhutdinov, Ruslan and Black, Alan W},
	year = {2019},
	pages = {11--21},
	file = {Chandu et al. - 2019 - “My Way of Telling a Story” Persona based Grounde.pdf:/Users/victor/Zotero/storage/9MATCXRY/Chandu et al. - 2019 - “My Way of Telling a Story” Persona based Grounde.pdf:application/pdf},
}

@inproceedings{zhou_bert_2022,
  title={BERT learns to teach: Knowledge distillation with meta learning},
  author={Zhou, Wangchunshu and Xu, Canwen and McAuley, Julian},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7037--7049},
  year={2022}
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://aclweb.org/anthology/P16-1162},
	doi = {10.18653/v1/P16-1162},
	abstract = {Neural machine translation (NMT) models typically operate with a ﬁxed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year = {2016},
	pages = {1715--1725},
	file = {A Sociolinguistic Study of Linguistic Variation and Code Matrix In Kanpur.pdf:/Users/victor/Zotero/storage/TSYBNSAI/A Sociolinguistic Study of Linguistic Variation and Code Matrix In Kanpur.pdf:application/pdf;Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:/Users/victor/Zotero/storage/NYT5GCVH/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}

@misc{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efﬁcient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efﬁcient by ﬁtting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reﬂected in experimental results.},
	language = {en},
	urldate = {2022-06-14},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {Number: arXiv:1312.6114
arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {1911.00536.pdf:/Users/victor/Zotero/storage/DM5LU9D5/1911.00536.pdf:application/pdf;1911.02116.pdf:/Users/victor/Zotero/storage/8RN6EUWH/1911.02116.pdf:application/pdf;2020.calcs-1.5.pdf:/Users/victor/Zotero/storage/HAZUNT48/2020.calcs-1.5.pdf:application/pdf;Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:/Users/victor/Zotero/storage/5AEU3GDX/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf},
}


@article{radford_language_nodate,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@article{bhopal_school_of_social_sciences___affiliated_barkatullah_university_dialectics_2018,
	title = {The {Dialectics} of {Hinglish}: {A} {Perspective}},
	volume = {2/2018},
	issn = {25449354},
	shorttitle = {The {Dialectics} of {Hinglish}},
	url = {http://alp.uw.edu.pl/wp-content/uploads/sites/315/2018/10/ALP-25_2-4-Nidhi-NEMA-Jagtar-Kaur-CHAWLA.pdf},
	doi = {10.32612/uw.25449354.2018.2.pp.37-51},
	abstract = {In India, the phenomenon of Hinglish has rapidly emerged from being a fashionable style of speech to a significant force instrumental in bringing about a major paradigm shift in social demography. Globalization and economic liberalization has served as catalysts to amplify this uniform communication code, which is currently blurring the linguistic barriers in a country speaking 780 dialects. Hinglish is redefining the cultural conventions in marketing/advertisement, Bollywood, and communication styles present in social media and the Internet. Its claim to be a proper language is substantiated by its acknowledgement on prestigious literary forums. While the concept is welcomed by both the marketplace and the masses as a beneficial symbiotic experience, it has also left the stakeholders of standard language, both Hindi and English, fretting and fuming. Amidst all the celebrations and concerns, the corpus of Hinglish is constantly widening and evolving because it is has been internalized, and not imposed, by the society as its own creation. The language accommodates diversity, lends flexibility, and suits the temperament of modern India. This paper studies how Hinglish has managed to seep into the very fabric of Indian society, restructuring the governing norms and practices. The paper also attempts to reflect how Hinglish is much more than just a language hybrid.},
	language = {en},
	number = {25},
	urldate = {2022-06-15},
	journal = {Applied Linguistics Papers},
	author = {{Bhopal School of Social Sciences \&  Affiliated Barkatullah University} and Nema, Nidhi and Chawla, Jagtar Kaur and {Bhopal School of Social Sciences \&  Affiliated Barkatullah University}},
	month = jun,
	year = {2018},
	pages = {37--51},
	file = {Bhopal School of Social Sciences &  Affiliated Barkatullah University et al. - 2018 - The Dialectics of Hinglish A Perspective.pdf:/Users/victor/Zotero/storage/S8HI53IZ/Bhopal School of Social Sciences &  Affiliated Barkatullah University et al. - 2018 - The Dialectics of Hinglish A Perspective.pdf:application/pdf},
}

@article{chaturvedi_sociolinguistic_2015,
	title = {A {Sociolinguistic} {Study} of {Linguistic} {Variation} and {Code} {Matrix} {In} {Kanpur}},
	volume = {192},
	issn = {18770428},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877042815034886},
	doi = {10.1016/j.sbspro.2015.06.017},
	abstract = {The diversity of languages and social structures in India has created a complex yet sustainable relation between linguistic variations and social classifications. This paper proposes a sociolinguistic study of linguistic variation and code matrix in Kanpur city. It shall be interesting to note the social behaviours mirrored in different language usages, variety of codes and resultant code-mixing and code-switching. The study of code matrix shall define the current status of different languages and dialects in the city. Hence this paper shall attempt to underline the temper, inclination and aspirations of language community of Kanpur through its linguistic variation along with indicating the growth of Kanpuria Hindi and evolving Hinglish in the city of Kanpur.},
	language = {en},
	urldate = {2022-06-15},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {Chaturvedi, Sujata},
	month = jun,
	year = {2015},
	pages = {107--115},
	file = {Chaturvedi - 2015 - A Sociolinguistic Study of Linguistic Variation an.pdf:/Users/victor/Zotero/storage/9MTW29RW/Chaturvedi - 2015 - A Sociolinguistic Study of Linguistic Variation an.pdf:application/pdf},
}

@inproceedings{srivastava_understanding_nodate,
  title={Understanding script-mixing: A case study of Hindi-English bilingual Twitter users},
  author={Srivastava, Abhishek and Bali, Kalika and Choudhury, Monojit},
  booktitle={Proceedings of the The 4th Workshop on Computational Approaches to Code Switching},
  pages={36--44},
  year={2020}
}

@article{ganguly_positive_2019,
	title = {The positive and negative effects of social media in {India}},
	volume = {62},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3345671},
	doi = {10.1145/3345671},
	language = {en},
	number = {11},
	urldate = {2022-06-15},
	journal = {Communications of the ACM},
	author = {Ganguly, Niloy and Kumaraguru, Ponnurangam},
	month = oct,
	year = {2019},
	pages = {98--99},
	file = {Ganguly and Kumaraguru - 2019 - The positive and negative effects of social media .pdf:/Users/victor/Zotero/storage/PS38MPKS/Ganguly and Kumaraguru - 2019 - The positive and negative effects of social media .pdf:application/pdf},
}

@article{parshad_what_2016,
	title = {What is {India} speaking? {Exploring} the “{Hinglish}” invasion},
	volume = {449},
	issn = {03784371},
	shorttitle = {What is {India} speaking?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437116000236},
	doi = {10.1016/j.physa.2016.01.015},
	abstract = {Language competition models help understand language shift dynamics, and have effectively captured how English has outcompeted various local languages, such as Scottish Gaelic in Scotland, and Mandarin in Singapore. India, with a 125 million English speakers boasts the second largest number of English speakers in the world, after the United States. The 1961–2001 Indian censuses report a sharp increase in Hindi/English Bilinguals, suggesting that English is on the rise in India. To the contrary, we claim supported by field evidence, that these statistics are inaccurate, ignoring an emerging class who do not have full bilingual competence and switch between Hindi and English, communicating via a code popularly known as ‘‘Hinglish’’. Since current language competition models occlude hybrid practices and detailed local ecological factors, they are inappropriate to capture the current language dynamics in India. Expanding predator–prey and sociolinguistic theories, we draw on local Indian ecological factors to develop a novel three-species model of interaction between Monolingual Hindi speakers, Hindi/English Bilinguals and Hinglish speakers, and explore the long time dynamics it predicts. The model also exhibits Turing instability, which is the first pattern formation result in language dynamics. These results challenge traditional assumptions of English encroachment in India. More broadly, the three-species model introduced here is a first step towards modeling the dynamics of hybrid language scenarios in other settings across the world.},
	language = {en},
	urldate = {2022-06-15},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Parshad, Rana D. and Bhowmick, Suman and Chand, Vineeta and Kumari, Nitu and Sinha, Neha},
	month = may,
	year = {2016},
	pages = {375--389},
	file = {Parshad et al. - 2016 - What is India speaking Exploring the “Hinglish” i.pdf:/Users/victor/Zotero/storage/7GWMMTQT/Parshad et al. - 2016 - What is India speaking Exploring the “Hinglish” i.pdf:application/pdf},
}

@article{liu_multilingual_2020,
  title={Multilingual Denoising Pre-training for Neural Machine Translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020}
}

@inproceedings{conneau_unsupervised_2020,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}

@inproceedings{majumder_generating_2019,
  title={Generating Personalized Recipes from Historical User Preferences},
  author={Majumder, Bodhisattwa Prasad and Li, Shuyang and Ni, Jianmo and McAuley, Julian},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5976--5982},
  year={2019}
}

@article{lample_cross-lingual_2019,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{gehrmann_gem_2021,
	title = {The {GEM} {Benchmark}: {Natural} {Language} {Generation}, its {Evaluation} and {Metrics}},
	shorttitle = {The {GEM} {Benchmark}},
	url = {http://arxiv.org/abs/2102.01672},
	abstract = {We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.},
	language = {en},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Gehrmann, Sebastian and Adewumi, Tosin and Aggarwal, Karmanya and Ammanamanchi, Pawan Sasanka and Anuoluwapo, Aremu and Bosselut, Antoine and Chandu, Khyathi Raghavi and Clinciu, Miruna and Das, Dipanjan and Dhole, Kaustubh D. and Du, Wanyu and Durmus, Esin and Dušek, Ondřej and Emezue, Chris and Gangal, Varun and Garbacea, Cristina and Hashimoto, Tatsunori and Hou, Yufang and Jernite, Yacine and Jhamtani, Harsh and Ji, Yangfeng and Jolly, Shailza and Kale, Mihir and Kumar, Dhruv and Ladhak, Faisal and Madaan, Aman and Maddela, Mounica and Mahajan, Khyati and Mahamood, Saad and Majumder, Bodhisattwa Prasad and Martins, Pedro Henrique and McMillan-Major, Angelina and Mille, Simon and van Miltenburg, Emiel and Nadeem, Moin and Narayan, Shashi and Nikolaev, Vitaly and Niyongabo, Rubungo Andre and Osei, Salomey and Parikh, Ankur and Perez-Beltrachini, Laura and Rao, Niranjan Ramesh and Raunak, Vikas and Rodriguez, Juan Diego and Santhanam, Sashank and Sedoc, João and Sellam, Thibault and Shaikh, Samira and Shimorina, Anastasia and Cabezudo, Marco Antonio Sobrevilla and Strobelt, Hendrik and Subramani, Nishant and Xu, Wei and Yang, Diyi and Yerukola, Akhila and Zhou, Jiawei},
	month = apr,
	year = {2021},
	note = {Number: arXiv:2102.01672
arXiv:2102.01672 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Gehrmann et al. - 2021 - The GEM Benchmark Natural Language Generation, it.pdf:/Users/victor/Zotero/storage/XJUABSFB/Gehrmann et al. - 2021 - The GEM Benchmark Natural Language Generation, it.pdf:application/pdf},
}

@inproceedings{zhang_adversarial_2017,
  title={Adversarial feature matching for text generation},
  author={Zhang, Yizhe and Gan, Zhe and Fan, Kai and Chen, Zhi and Henao, Ricardo and Shen, Dinghan and Carin, Lawrence},
  booktitle={International Conference on Machine Learning},
  pages={4006--4015},
  year={2017},
  organization={PMLR}
}

@inproceedings{zhang_dialogpt_2020,
  title={DIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
  author={Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, William B},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={270--278},
  year={2020}
}

@inproceedings{singh_twitter_2018,
	address = {Melbourne, Australia},
	title = {A {Twitter} {Corpus} for {Hindi}-{English} {Code} {Mixed} {POS} {Tagging}},
	url = {http://aclweb.org/anthology/W18-3503},
	doi = {10.18653/v1/W18-3503},
	abstract = {Code-mixing is a linguistic phenomenon where multiple languages are used in the same occurrence that is increasingly common in multilingual societies. Codemixed content on social media is also on the rise, prompting the need for tools to automatically understand such content. Automatic Parts-of-Speech (POS) tagging is an essential step in any Natural Language Processing (NLP) pipeline, but there is a lack of annotated data to train such models. In this work, we present a unique language tagged and POS-tagged dataset of code-mixed English-Hindi tweets related to ﬁve incidents in India that led to a lot of Twitter activity. Our dataset is unique in two dimensions: (i) it is larger than previous annotated datasets and (ii) it closely resembles typical real-world tweets. Additionally, we present a POS tagging model that is trained on this dataset to provide an example of how this dataset can be used. The model also shows the efﬁcacy of our dataset in enabling the creation of codemixed social media POS taggers.},
	language = {en},
	urldate = {2022-06-15},
	booktitle = {Proceedings of the {Sixth} {International} {Workshop} on {Natural} {Language} {Processing} for {Social} {Media}},
	publisher = {Association for Computational Linguistics},
	author = {Singh, Kushagra and Sen, Indira and Kumaraguru, Ponnurangam},
	year = {2018},
	pages = {12--17},
	file = {Singh et al. - 2018 - A Twitter Corpus for Hindi-English Code Mixed POS .pdf:/Users/victor/Zotero/storage/4DDFSG63/Singh et al. - 2018 - A Twitter Corpus for Hindi-English Code Mixed POS .pdf:application/pdf},
}

@inproceedings{singh_language_2018,
	address = {Melbourne, Australia},
	title = {Language {Identification} and {Named} {Entity} {Recognition} in {Hinglish} {Code} {Mixed} {Tweets}},
	url = {http://aclweb.org/anthology/P18-3008},
	doi = {10.18653/v1/P18-3008},
	abstract = {While growing code-mixed content on Online Social Networks (OSNs) provides a fertile ground for studying various aspects of code-mixing, the lack of automated text analysis tools render such studies challenging. To meet this challenge, a family of tools for analyzing code-mixed data such as language identiﬁers, partsof-speech (POS) taggers, chunkers have been developed. Named Entity Recognition (NER) is an important text analysis task which is not only informative by itself, but is also needed for downstream NLP tasks such as semantic role labeling. In this work, we present an exploration of automatic NER of code-mixed data. We compare our method with existing off-theshelf NER tools for social media content, and ﬁnd that our systems outperforms the best baseline by 33.18 \% (F1 score).},
	language = {en},
	urldate = {2022-06-15},
	booktitle = {Proceedings of {ACL} 2018, {Student} {Research} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Singh, Kushagra and Sen, Indira and Kumaraguru, Ponnurangam},
	year = {2018},
	pages = {52--58},
	file = {Singh et al. - 2018 - Language Identification and Named Entity Recogniti.pdf:/Users/victor/Zotero/storage/SQLVZ4KY/Singh et al. - 2018 - Language Identification and Named Entity Recogniti.pdf:application/pdf},
}

@inproceedings{rudra_understanding_2016,
	address = {Austin, Texas},
	title = {Understanding {Language} {Preference} for {Expression} of {Opinion} and {Sentiment}: {What} do {Hindi}-{English} {Speakers} do on {Twitter}?},
	shorttitle = {Understanding {Language} {Preference} for {Expression} of {Opinion} and {Sentiment}},
	url = {http://aclweb.org/anthology/D16-1121},
	doi = {10.18653/v1/D16-1121},
	abstract = {Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010). Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers. In this paper, we report a study on 430,000 unique tweets from Indian users, speciﬁcally Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment. To this end, we develop classiﬁers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments. Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing. As an aside, we explore some common pragmatic functions of codeswitching through sentiment detection.},
	language = {en},
	urldate = {2022-06-15},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rudra, Koustav and Rijhwani, Shruti and Begum, Rafiya and Bali, Kalika and Choudhury, Monojit and Ganguly, Niloy},
	year = {2016},
	pages = {1131--1141},
	file = {Rudra et al. - 2016 - Understanding Language Preference for Expression o.pdf:/Users/victor/Zotero/storage/Z9X56YQK/Rudra et al. - 2016 - Understanding Language Preference for Expression o.pdf:application/pdf},
}

@misc{zheng_pre-training_2019,
	title = {A {Pre}-training {Based} {Personalized} {Dialogue} {Generation} {Model} with {Persona}-sparse {Data}},
	url = {http://arxiv.org/abs/1911.04700},
	abstract = {Endowing dialogue systems with personas is essential to deliver more human-like conversations. However, this problem is still far from well explored due to the difﬁculties of both embodying personalities in natural languages and the persona sparsity issue observed in most dialogue corpora. This paper proposes a pre-training based personalized dialogue model that can generate coherent responses using persona-sparse dialogue data. In this method, a pre-trained language model is used to initialize an encoder and decoder, and personal attribute embeddings are devised to model richer dialogue contexts by encoding speakers’ personas together with dialogue histories. Further, to incorporate the target persona in the decoding process and to balance its contribution, an attention routing structure is devised in the decoder to merge features extracted from the target persona and dialogue contexts using dynamically predicted weights. Our model can utilize persona-sparse dialogues in a uniﬁed manner during the training process, and can also control the amount of personarelated features to exhibit during the inference process. Both automatic and manual evaluation demonstrates that the proposed model outperforms state-of-the-art methods for generating more coherent and persona consistent responses with persona-sparse data.},
	language = {en},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Zheng, Yinhe and Zhang, Rongsheng and Mao, Xiaoxi and Huang, Minlie},
	month = nov,
	year = {2019},
	note = {Number: arXiv:1911.04700
arXiv:1911.04700 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zheng et al. - 2019 - A Pre-training Based Personalized Dialogue Generat.pdf:/Users/victor/Zotero/storage/L5XBHF97/Zheng et al. - 2019 - A Pre-training Based Personalized Dialogue Generat.pdf:application/pdf},
}

@inproceedings{gamback_comparing_nodate,
    title = "Identifying Languages at the Word Level in Code-Mixed {I}ndian Social Media Text",
    author = {Das, Amitava  and
      Gamb{\"a}ck, Bj{\"o}rn},
    booktitle = "Proceedings of the 11th International Conference on Natural Language Processing",
    month = dec,
    year = "2014",
    address = "Goa, India",
    publisher = "NLP Association of India",
    url = "https://aclanthology.org/W14-5152",
    pages = "378--387",
}

@article{goodfellow_generative_2014,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{garg_code-switched_2018,
	address = {Brussels, Belgium},
	title = {Code-switched {Language} {Models} {Using} {Dual} {RNNs} and {Same}-{Source} {Pretraining}},
	url = {http://aclweb.org/anthology/D18-1346},
	doi = {10.18653/v1/D18-1346},
	abstract = {This work focuses on building language models (LMs) for code-switched text. We propose two techniques that signiﬁcantly improve these LMs: 1) A novel recurrent neural network unit with dual components that focus on each language in the code-switched text separately 2) Pretraining the LM using synthetic text from a generative model estimated using the training data. We demonstrate the effectiveness of our proposed techniques by reporting perplexities on a Mandarin-English task and derive signiﬁcant reductions in perplexity.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Garg, Saurabh and Parekh, Tanmay and Jyothi, Preethi},
	year = {2018},
	pages = {3078--3083},
	file = {Garg et al. - 2018 - Code-switched Language Models Using Dual RNNs and .pdf:/Users/victor/Zotero/storage/KYKP2M4G/Garg et al. - 2018 - Code-switched Language Models Using Dual RNNs and .pdf:application/pdf},
}

@inproceedings{guzman_metrics_2017,
	title = {Metrics for {Modeling} {Code}-{Switching} {Across} {Corpora}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/guzman17_interspeech.html},
	doi = {10.21437/Interspeech.2017-1429},
	abstract = {In developing technologies for code-switched speech, it would be desirable to be able to predict how much language mixing might be expected in the signal and the regularity with which it might occur. In this work, we offer various metrics that allow for the classiﬁcation and visualization of multilingual corpora according to the ratio of languages represented, the probability of switching between them, and the time-course of switching. Applying these metrics to corpora of different languages and genres, we ﬁnd that they display distinct probabilities and periodicities of switching, information useful for speech processing of mixed-language data.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Guzmán, Gualberto and Ricard, Joseph and Serigos, Jacqueline and Bullock, Barbara E. and Toribio, Almeida Jacqueline},
	month = aug,
	year = {2017},
	pages = {67--71},
	file = {Guzmán et al. - 2017 - Metrics for Modeling Code-Switching Across Corpora.pdf:/Users/victor/Zotero/storage/3QQ3ILHK/Guzmán et al. - 2017 - Metrics for Modeling Code-Switching Across Corpora.pdf:application/pdf},
}

@article{barnett2000,
author = {Ruthanna Barnett and Eva Codó and Eva Eppler and Montse Forcadell and Penelope Gardner-Chloros and Roeland van Hout and Melissa Moyer and Maria Carme Torras and Maria Teresa Turell and Mark Sebba and Marianne Starren and Sietse Wensing},
title ={The LIDES Coding Manual: A document for preparing and analyzing language interaction data Version 1.1—July, 1999},
journal = {International Journal of Bilingualism},
volume = {4},
number = {2},
pages = {131-132},
year = {2000},
doi = {10.1177/13670069000040020101},

URL = { 
        https://doi.org/10.1177/13670069000040020101
    
},
eprint = { 
        https://doi.org/10.1177/13670069000040020101
    
}

}

@misc{luong_effective_2015,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1508.04025},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	language = {en},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	month = sep,
	year = {2015},
	note = {Number: arXiv:1508.04025
arXiv:1508.04025 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:/Users/victor/Zotero/storage/79RQQPUE/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf},
}

@inproceedings{bansal-etal-2020-code,
    title = "Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream {NLP} Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection",
    author = "Bansal, Srijan  and
      Garimella, Vishal  and
      Suhane, Ayush  and
      Patro, Jasabanta  and
      Mukherjee, Animesh",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.96",
    doi = "10.18653/v1/2020.acl-main.96",
    pages = "1018--1023",
}

@inproceedings{strubell-etal-2019-energy,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
}

@article{sengupta_does_2022,
	title = {Does aggression lead to hate? {Detecting} and reasoning offensive traits in hinglish code-mixed texts},
	volume = {488},
	issn = {0925-2312},
	shorttitle = {Does aggression lead to hate?},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221017306},
	doi = {10.1016/j.neucom.2021.11.053},
	abstract = {Aggression is a prominent trait of human beings that can affect social harmony in a negative way. The hate mongers misuse the freedom of speech in social media platforms to flood with their venomous comments in many forms. Identifying different traits of online offense is thus inevitable and the need of the hour. Existing studies usually handle one or two offense traits at a time, mainly due to the lack of a combined annotated dataset and a scientific study that provides insights into the relationship among the traits. In this paper, we study the relationship among five offense traits – aggression, hate, sarcasm, humor, and stance in Hinglish (Hindi-English) social media code-mixed texts. We employ various state-of-the-art deep learning systems at different morphological granularities for the classification across five offense traits. Our evaluation of the unified framework suggests ∼90\% performance across all major traits. Furthermore, we propose a novel notion of causal importance score to quantify the effect of different abusive keywords and the overall context on the offensiveness of the texts.},
	language = {en},
	urldate = {2022-06-21},
	journal = {Neurocomputing},
	author = {Sengupta, Ayan and Bhattacharjee, Sourabh Kumar and Akhtar, Md. Shad and Chakraborty, Tanmoy},
	month = jun,
	year = {2022},
	keywords = {Aggression detection, Causal analysis, Code-Mixed language, Hate speech detection, Offensive texts, Semi-supervised learning, Stance detection, Transformer},
	pages = {598--617},
	file = {ScienceDirect Snapshot:/Users/victor/Zotero/storage/6TP4BTY3/S0925231221017306.html:text/html},
}

@article{fleiss1971measuring,
  title={Measuring nominal scale agreement among many raters.},
  author={Fleiss, Joseph L},
  journal={Psychological bulletin},
  volume={76},
  number={5},
  pages={378},
  year={1971},
  publisher={American Psychological Association}
}

@article{jawahar2021exploring,
  title={Exploring text-to-text transformers for English to Hinglish machine translation with synthetic code-mixing},
  author={Jawahar, Ganesh and Nagoudi, El Moatez Billah and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS},
  journal={arXiv preprint arXiv:2105.08807},
  year={2021}
}

@inproceedings{yong2023prompting,
  title={Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages},
  author={Yong, Zheng Xin and Zhang, Ruochen and Forde, Jessica and Wang, Skyler and Subramonian, Arjun and Lovenia, Holy and Cahyawijaya, Samuel and Winata, Genta and Sutawika, Lintang and Cruz, Jan Christian Blaise and others},
  booktitle={Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching},
  pages={43--63},
  year={2023}
}

@inproceedings{sravani2023enhancing,
  title={Enhancing Code-mixed Text Generation Using Synthetic Data Filtering in Neural Machine Translation},
  author={Sravani, Dama and Mamidi, Radhika},
  booktitle={Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)},
  pages={211--220},
  year={2023}
}

@article{winata2019code,
  title={Code-switched language models using neural based synthetic data from parallel sentences},
  author={Winata, Genta Indra and Madotto, Andrea and Wu, Chien-Sheng and Fung, Pascale},
  journal={arXiv preprint arXiv:1909.08582},
  year={2019}
}

@article{tan2021code,
  title={Code-mixing on sesame street: Dawn of the adversarial polyglots},
  author={Tan, Samson and Joty, Shafiq},
  journal={arXiv preprint arXiv:2103.09593},
  year={2021}
}

@inproceedings{santy2021bertologicomix,
  title={BERTologiCoMix: How does code-mixing interact with multilingual BERT?},
  author={Santy, Sebastin and Srinivasan, Anirudh and Choudhury, Monojit},
  booktitle={Proceedings of the Second Workshop on Domain Adaptation for NLP},
  pages={111--121},
  year={2021}
}

@inproceedings{dowlagar2021gated,
  title={Gated Convolutional Sequence to Sequence Based Learning for English-Hingilsh Code-Switched Machine Translation.},
  author={Dowlagar, Suman and Mamidi, Radhika},
  booktitle={Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching},
  pages={26--30},
  year={2021}
}

@inproceedings{gautam2021comet,
  title={Comet: Towards code-mixed translation using parallel monolingual sentences},
  author={Gautam, Devansh and Kodali, Prashant and Gupta, Kshitij and Goel, Anmol and Shrivastava, Manish and Kumaraguru, Ponnurangam},
  booktitle={Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching},
  pages={47--55},
  year={2021}
}

@article{amin2023marathi,
  title={Marathi-English Code-mixed Text Generation},
  author={Amin, Dhiraj and Govilkar, Sharvari and Kulkarni, Sagar and Lalit, Yash Shashikant and Khwaja, Arshi Ajaz and Xavier, Daries and Gupta, Sahil Girijashankar},
  journal={arXiv preprint arXiv:2309.16202},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{reichelt2014talk,
  title={Talk to me personally: Personalization of language style in computer-based learning},
  author={Reichelt, Maria and K{\"a}mmerer, Frauke and Niegemann, Helmut M and Zander, Steffi},
  journal={Computers in Human behavior},
  volume={35},
  pages={199--210},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{king-cook-2020-evaluating,
    title = "Evaluating Approaches to Personalizing Language Models",
    author = "King, Milton  and
      Cook, Paul",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.299",
    pages = "2461--2469",
    abstract = "In this work, we consider the problem of personalizing language models, that is, building language models that are tailored to the writing style of an individual. Because training language models requires a large amount of text, and individuals do not necessarily possess a large corpus of their writing that could be used for training, approaches to personalizing language models must be able to rely on only a small amount of text from any one user. In this work, we compare three approaches to personalizing a language model that was trained on a large background corpus using a relatively small amount of text from an individual user. We evaluate these approaches using perplexity, as well as two measures based on next word prediction for smartphone soft keyboards. Our results show that when only a small amount of user-specific text is available, an approach based on priming gives the most improvement, while when larger amounts of user-specific text are available, an approach based on language model interpolation performs best. We carry out further experiments to show that these approaches to personalization outperform language model adaptation based on demographic factors.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{sengupta2024social,
  title={Social, economic, and demographic factors drive the emergence of Hinglish code-mixing on social media},
  author={Sengupta, Ayan and Das, Soham and Akhtar, Md Shad and Chakraborty, Tanmoy},
  journal={Humanities and Social Sciences Communications},
  volume={11},
  number={1},
  pages={1--12},
  year={2024},
  publisher={Palgrave}
}

@article{workshop2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@misc{workshop2023bloom,
      title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo González Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and Gérard Dupont and Germán Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jörg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Muñoz and Maraim Masoud and María Grandury and Mario Šaško and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis López and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Taşar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre François Lavallée and Rémi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and Stéphane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aurélie Névéol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdeněk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Muñoz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Clémentine Fourrier and Daniel León Periñán and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc Pàmies and Maria A Castillo and Marianna Nezhurina and Mario Sänger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Théo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
      year={2023},
      eprint={2211.05100},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}