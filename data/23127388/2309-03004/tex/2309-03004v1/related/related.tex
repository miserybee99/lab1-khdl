\section{Related Works}

In this section, we list works on the same topic as ours. \cref{sec:preliminary} contains works on different topics that our explanation depends on, we omit their details for simplicity here.

In our point of view, the research of activation sparsity in MLP modules starts from the discovery of the relation between MLP and knowledge gained during training. \citet{mlp_as_database} first rewrite MLPs in Transformers into an unnormalized attention mechanism where queries are inputs to the MLP block while keys and values are provided by the first and second weight matrices instead of inputs. So MLP blocks are key-value memories. 
\citet{knowledge_neurons} push forward by detecting how each key-value pair is related to each question exploiting activation magnitudes as well as their gradients, and providing a method to surgically manipulate answers for individual questions in Q\&A tasks. These works reorient research attention back to MLPs, which are previously shadowed by self-attention.

Recently, comprehensive experiments conducted by \citet{observation} demonstrate activation sparsity in MLPs is a prevailing phenomenon in various architectures and on various CV and NLP tasks. 
\citet{observation} also eliminate alternative explanations and attribute activation sparsity solely to training dynamics. 
The authors explain the sparsity theoretically with initialization and by calculating gradients, but their explanation is restricted to the last layer and the first step because in later steps the independence between weights and samples required by the explanation is broken. 
They also discover that some activation functions, such as $\tanh$, hinder the sparsity \citep[see][Fig B.3(c)]{observation}, but did not elaborate on it. 
Compared to their explanations, our explanation applies to all layers and large steps, and accounts for the activation functions' critical role in activation sparsity.

Following empirical discoveries by \citet{observation}, \citet{sharpness_aware} show that sharpness-aware (SA) optimization has a stronger bias toward activation sparsity. 
They explain theoretically by calculating gradients and finding that SA optimization imposes in gradients a component toward reducing norms of activations. However, their explanation is still conducted on shallow 2-layer pure MLPs and requires SA optimization, which is not included in standard training practice. Nevertheless, this explanation hints at the role of flatness in the emergence of activation sparsity. Inspired by them, we explain \emph{deep} networks trained by standard SGD or other stochastic trainers by substituting flat minima for SA optimization.

A more recent work by \citet{from_noises} holds a point that sparsity is a resistance to noises. However, noises are manually imposed and not included in standard data augmentations. We substitute gradient noise from SGD or other stochastic optimizers for them.
\citet{large_step} prove sparsity on 2-layer diagonal MLPs and conjecture similar things to happen in more general networks. Both works hint at the relation between noises (Gaussian sample noises and stochastic gradient noises) and activation sparsity, also leading to the flatness bias of stochastic optimization.

\citet{adversarial_of_moe} study the adversarial robustness of Mixture of Experts (MoE) models brought by architecture-imposed sparsity. They inspire us to relate sparsity with adversarial robustness, although we do it reversely. It is the major inspiration for our results.

To sum up, existing discoveries hint at the relation between activation sparsity and noises, flatness and activation functions but they are still restricted to shallow layers, small steps and special training. Inspired by them and filling their gaps, our explanation applies to deep networks and large training steps, and sticks to standard training procedures.

Although not devoting much to explaining the emergence of activation sparsity in CNNs, \citet{exploit_sparsity_in_CNN} boost activation sparsity through Hoyer regularization\citep{hoyer} and a new activation function FATReLU that uses dynamic thresholds between activation and deactivation. They also design algorithms to exploit this sparsity, leading to $\ge 1.75\mathrm{x}$ speedup in CNN's inference. Compared to their sparsity encouragement method that requires well-designed procedures to select thresholds, hyperparameters for our theoretically induced modifications can be easily selected. The discontinuity of FATReLU also bothers training from scratch\citep{exploit_sparsity_in_CNN}, while we recommend applying our modifications from scratch to enjoy better sparsity and additionally smaller \emph{training} costs. Regarding exploitation, we consider it out of the manuscript's scope.
\citet{L1_sparsity} encourages activation sparsity in CNN by explicit $L_1$ regularization. We intend to investigate the emergence of activation sparsity from implicit regularization as demonstrated by \citet{observation}, so we solely rely on implicit regularization boosted by modifications. Nevertheless, our methods are architecturally orthogonal and we believe applying both together can further boost activation sparsity.

There are other works that are not devoted to activation sparsity but are related. \citet{sparse_symbol} formulate, with Shapley value, and prove that there are sparse ``symbols'' as groups of patches that are the only major contributors to the output of any well-trained and masking-robust AIs. They provide a sparsity independent of training dynamics. Their theory focuses on symbols and sparsity in inputs, which is inherently different from ours.

In Primer \citep{primer}, several architectural changes given by architecture searching include a new activation function Squared-ReLU. In this work, we induce a similar squared $\relu$ activation but with the non-zero part shifted left and use it to guide the search for flat minima and gradient/activation sparsity. \cite{primer} demonstrate impressive improvements of Squared-ReLU in both ablation and addition experiments, and our work provides a potential explanation for this improvement.