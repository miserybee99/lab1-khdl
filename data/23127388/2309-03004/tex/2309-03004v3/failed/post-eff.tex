To exploit the eigenvalue concentration, we define effective sparsity in \cref{def:effective_gradient_sparsity} following \cref{lemma:rewrite_gradient_sparsity}.
\begin{lemma}\label{lemma:rewrite_gradient_sparsity}
    Let $g^l \defeq \left(J_{\ce}(f) J_f(\dbmlp^l)\right)^\transpose \in \reals^{n}$ be the (column) gradient w.r.t. activations of $\mlp^l$ or $\dbmlp^l$, then there is $M^l = g^l \left(g^l\right)^\transpose$. Furthermore, we have
    \begin{align}
        \left(\gamma^l\right)^\transpose \left(\left(K^l \left(K^l\right)^\transpose\right) \hadamard M^l\right) \gamma^l
        =& \left(g^l \hadamard \gamma^l\right)^\transpose \kkT (g^l \hadamard \gamma^l).
    \end{align}
    \begin{proof}
        $M^l = g^l \left(g^l\right)^\transpose$ is straightforward by definition. For the other equality, combining $\diag{\gamma}$ in a way other than what is done in \cref{lemma:adversarial_and_sparsity} gives
        \begin{align}
            &   \left(\gamma^l\right)^\transpose \left(\left(K^l \left(K^l\right)^\transpose\right) \hadamard M^l\right) \gamma^l\\
            =&  \left(\gamma^l\right)^\transpose \diag{g^l} \kkT \diag{g^l} \gamma^l\\
            =& \left(g^l \hadamard \gamma^l\right)^\transpose \kkT (g^l \hadamard \gamma^l).
        \end{align}
    \end{proof}
\end{lemma}
