@inproceedings{observation,
	title = {The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers},
	url = {https://openreview.net/forum?id=TJ2nxciYCk-},
	shorttitle = {The Lazy Neuron Phenomenon},
	abstract = {This paper studies a curious phenomenon that machine learning model with Transformer architectures have sparse activation maps. By activation map we refer to the intermediate output of the multi-layer perceptrons ({MLPs}) after a {ReLU} activation function, and by "sparse" we mean that on average very few entries (e.g., 3.0\% for T5-Base and 6.3\% for {ViT}-B16) are nonzero for each input to {MLP}. Moreover, larger Transformers with more layers and wider {MLP} hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels. We discuss how sparsity immediately implies a way to significantly reduce the {FLOP} count and improve efficiency for Transformers. Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small k brings a collection of desired properties, namely less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence.},
	eventtitle = {The Eleventh International Conference on Learning Representations},
	author = {Li, Zonglin and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, Ankit Singh and Reddi, Sashank J. and Ye, Ke and Chern, Felix and Yu, Felix and Guo, Ruiqi and Kumar, Sanjiv},
	urldate = {2023-08-17},
	date = {2022-09-29},
	year = {2022},
	month = {09},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\78758\\Zotero\\storage\\X2KJ95NW\\Li 等 - 2022 - The Lazy Neuron Phenomenon On Emergence of Activa.pdf:application/pdf},
}


@misc{sharpness_aware,
	title = {Sharpness-Aware Minimization Leads to Low-Rank Features},
	url = {http://arxiv.org/abs/2305.16292},
	abstract = {Sharpness-aware minimization ({SAM}) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is wellknown and is the primary motivation, we uncover an additional intriguing effect of {SAM}: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by {SAM} which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank reduction mechanism can be more complex, especially for deep networks with pre-activation skip connections and self-attention layers. We make our code available at https://github.com/tml-epfl/sam-low-rank-features.},
	number = {{arXiv}:2305.16292},
	publisher = {{arXiv}},
	author = {Andriushchenko, Maksym and Bahri, Dara and Mobahi, Hossein and Flammarion, Nicolas},
	urldate = {2023-06-02},
	date = {2023-05-25},
	year = {2023},
	month = {05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.16292 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Andriushchenko 等 - 2023 - Sharpness-Aware Minimization Leads to Low-Rank Fea.pdf:C\:\\Users\\78758\\Zotero\\storage\\BVFYRP87\\Andriushchenko 等 - 2023 - Sharpness-Aware Minimization Leads to Low-Rank Fea.pdf:application/pdf},
}

@inproceedings{escape,
	title = {Towards Theoretically Understanding Why Sgd Generalizes Better Than Adam in Deep Learning},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f3f27a324736617f20abbf2ffd806f6d-Abstract.html},
	abstract = {It is not clear yet why {ADAM}-alike adaptive gradient algorithms suffer from worse generalization performance than  {SGD} despite their faster training speed.   This work aims to provide understandings on this generalization gap by analyzing their local convergence behaviors. Specifically,  we observe the heavy tails of gradient noise in these algorithms. This motivates us to analyze these algorithms through their  Levy-driven stochastic differential equations ({SDEs})  because of the similar convergence behaviors of an algorithm and its {SDE}. Then we establish the escaping time of these {SDEs} from a local basin. The result shows that (1) the escaping time of both {SGD} and {ADAM}{\textasciitilde}depends on the  Radon measure of the basin positively and the heaviness of gradient noise negatively;  (2) for the same basin, {SGD} enjoys smaller escaping time than {ADAM}, mainly because  (a) the geometry adaptation in {ADAM}{\textasciitilde}via adaptively scaling each gradient coordinate well diminishes the anisotropic structure in gradient noise and results in larger Radon measure of a basin; (b)  the exponential gradient average in {ADAM}{\textasciitilde}smooths its gradient and leads to lighter gradient noise tails than {SGD}.  So {SGD} is more locally unstable than {ADAM}{\textasciitilde}at sharp minima defined as the minima whose local basins have small Radon measure, and can better escape from them to flatter ones with larger Radon measure.   As flat minima here which often refer to the minima at flat or asymmetric basins/valleys often generalize better than sharp ones{\textasciitilde}{\textbackslash}cite\{keskar2016large,he2019asymmetric\}, our result explains the better generalization performance of {SGD} over {ADAM}.  Finally, experimental results confirm our heavy-tailed gradient noise assumption and theoretical affirmation.},
	pages = {21285--21296},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven Chu Hong and E, Weinan},
	urldate = {2023-08-17},
	date = {2020},
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{chaudhari_stochastic,
	title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
	doi = {10.1109/ITA.2018.8503224},
	abstract = {Stochastic gradient descent ({SGD}) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that {SGD} minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So {SGD} does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, {SGD} does not even converge in the classical sense: we show that the most likely trajectories of {SGD} for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such “out-of-equilibrium” behavior is a consequence of highly nonisotropic gradient noise in {SGD}; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims. This article summarizes the findings in [1]. See the longer version for background, detailed results and proofs.},
	eventtitle = {2018 Information Theory and Applications Workshop ({ITA})},
	pages = {1--10},
	booktitle = {2018 Information Theory and Applications Workshop ({ITA})},
	author = {Chaudhari, Pratik and Soatto, Stefano},
	date = {2018-02},
	year = {2018},
	month = {02},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics, Mathematics - Optimization and Control, theory, information bottleneck, implicit regularizer, Entropy, Force, Limit-cycles, Mathematical model, Steady-state, Temperature, Trajectory},
}

@misc{weight_information_bottleneck,
	title = {Where is the Information in a Deep Neural Network?},
	url = {http://arxiv.org/abs/1905.12213},
	abstract = {Whatever information a deep neural network has gleaned from training data is encoded in its weights. How this information affects the response of the network to future data remains largely an open question. Indeed, even deﬁning and measuring information entails some subtleties, since a trained network is a deterministic map, so standard information measures can be degenerate. We measure information in a neural network via the optimal trade-off between accuracy of the response and complexity of the weights, measured by their coding length. Depending on the choice of code, the deﬁnition can reduce to standard measures such as Shannon Mutual Information and Fisher Information. However, the more general deﬁnition allows us to relate information to generalization and invariance, through a novel notion of effective information in the activations of a deep network. We establish a novel relation between the information in the weights and the effective information in the activations, and use this result to show that models with low (information) complexity not only generalize better, but are bound to learn invariant representations of future inputs. These relations hinge not only on the architecture of the model, but also on how it is trained, highlighting the complex inter-dependency between the class of functions implemented by deep neural networks, the loss function used for training them from ﬁnite data, and the inductive bias implicit in the optimization.},
	number = {{arXiv}:1905.12213},
	publisher = {{arXiv}},
	author = {Achille, Alessandro and Paolini, Giovanni and Soatto, Stefano},
	urldate = {2023-03-24},
	date = {2020-06-21},
	year = {2020},
	month = {06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.12213 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory, theory, information bottleneck},
}

@misc{sparsity_handbook,
	title = {Ten Lessons We Have Learned in the New ``Sparseland'': A Short Handbook for Sparse Neural Network Researchers},
	url = {http://arxiv.org/abs/2302.02596},
	doi = {10.48550/arXiv.2302.02596},
	shorttitle = {Ten Lessons We Have Learned in the New ``Sparseland''},
	abstract = {This article does not propose any novel algorithm or new hardware for sparsity. Instead, it aims to serve the "common good" for the increasingly prosperous Sparse Neural Network ({SNN}) research community. We attempt to summarize some most common confusions in {SNNs}, that one may come across in various scenarios such as paper review/rebuttal and talks - many drawn from the authors' own bittersweet experiences! We feel that doing so is meaningful and timely, since the focus of {SNN} research is notably shifting from traditional pruning to more diverse and profound forms of sparsity before, during, and after training. The intricate relationships between their scopes, assumptions, and approaches lead to misunderstandings, for non-experts or even experts in {SNNs}. In response, we summarize ten Q{\textbackslash}\&As of {SNNs} from many key aspects, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs. sparse training, dense-to-sparse training vs. sparse-to-sparse training, static sparsity vs. dynamic sparsity, before-training/during-training vs. post-training sparsity, and many more. We strive to provide proper and generically applicable answers to clarify those confusions to the best extent possible. We hope our summary provides useful general knowledge for people who want to enter and engage with this exciting community; and also provides some "mind of ease" convenience for {SNN} researchers to explain their work in the right contexts. At the very least (and perhaps as this article's most insignificant target functionality), if you are writing/planning to write a paper or rebuttal in the field of {SNNs}, we hope some of our answers could help you!},
	number = {{arXiv}:2302.02596},
	publisher = {{arXiv}},
	author = {Liu, Shiwei and Wang, Zhangyang},
	urldate = {2023-07-03},
	date = {2023-06-24},
	year = {2023},
	month = {06},
	eprinttype = {arxiv},
	eprint = {2302.02596 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{stochastic_collapse,
	title = {Stochastic Collapse: How Gradient Noise Attracts {SGD} Dynamics Towards Simpler Subnetworks},
	url = {http://arxiv.org/abs/2306.04251},
	doi = {10.48550/arXiv.2306.04251},
	shorttitle = {Stochastic Collapse},
	abstract = {In this work, we reveal a strong implicit bias of stochastic gradient descent ({SGD}) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by {SGD}. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that {SGD} exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss. We observe empirically the existence of attractive invariant sets in trained deep neural networks, implying that {SGD} dynamics often collapses to simple subnetworks with either vanishing or redundant neurons. We further demonstrate how this simplifying process of stochastic collapse benefits generalization in a linear teacher-student framework. Finally, through this analysis, we mechanistically explain why early training with large learning rates for extended periods benefits subsequent generalization.},
	number = {{arXiv}:2306.04251},
	publisher = {{arXiv}},
	author = {Chen, Feng and Kunin, Daniel and Yamamura, Atsushi and Ganguli, Surya},
	urldate = {2023-07-11},
	date = {2023-06-07},
	year = {2023},
	month = {06},
	eprinttype = {arxiv},
	eprint = {2306.04251 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}


@inproceedings{adversarial_and_flat_minima,
	title = {Relating Adversarially Robust Generalization to Flat Minima},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Stutz_Relating_Adversarially_Robust_Generalization_to_Flat_Minima_ICCV_2021_paper.html},
	booktitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	pages = {7807--7817},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	urldate = {2023-08-17},
	date = {2021},
	year = {2021},
	langid = {english},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}


@inproceedings{knowledge_neurons,
	location = {Dublin, Ireland},
	title = {Knowledge Neurons in Pretrained Transformers},
	url = {https://aclanthology.org/2022.acl-long.581},
	doi = {10.18653/v1/2022.acl-long.581},
	abstract = {Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus (Petroni et al., 2019; Jiang et al., 2020b). In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Speciﬁcally, we examine the ﬁll-in-the-blank cloze task for {BERT}. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We ﬁnd that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) speciﬁc factual knowledge without ﬁne-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers. The code is available at https://github.com/ Hunter-{DDM}/knowledge-neurons.},
	eventtitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {8493--8502},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
	urldate = {2023-03-24},
	date = {2022},
	year = {2022},
	langid = {english},
	keywords = {compositionality, {NLP}, transformer, interpretability},
}
@inproceedings{mlp_as_database,
	location = {Online and Punta Cana, Dominican Republic},
	title = {Transformer Feed-Forward Layers Are Key-Value Memories},
	url = {https://aclanthology.org/2021.emnlp-main.446},
	doi = {10.18653/v1/2021.emnlp-main.446},
	eventtitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	pages = {5484--5495},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	urldate = {2023-08-17},
	date = {2021},
	year = {2021},
	langid = {english},
	keywords = {Computer Science - Computation and Language},
}


@inproceedings{large_step,
	title = {{SGD} with Large Step Sizes Learns Sparse Features},
	url = {https://proceedings.mlr.press/v202/andriushchenko23b.html},
	abstract = {We showcase important features of the dynamics of the Stochastic Gradient Descent ({SGD}) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) may lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics that biases it implicitly toward simple predictors. Furthermore, we show empirically that the longer large step sizes keep {SGD} high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used: the regularization effect comes solely from the {SGD} dynamics influenced by the large step sizes schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the {SGD} dynamics through the loss landscape of neural networks. We justify these findings theoretically through the study of simple neural network models as well as qualitative arguments inspired from stochastic processes. This analysis allows us to shed new light on some common practices and observed phenomena when training deep networks.},
	eventtitle = {International Conference on Machine Learning},
	pages = {903--925},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Andriushchenko, Maksym and Varre, Aditya Vardhan and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
	urldate = {2023-07-23},
	date = {2023-07-03},
	year = {2023},
	month = {07},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{from_noises,
	title = {Emergence of Sparse Representations from Noise},
	url = {https://openreview.net/forum?id=cxYaBAXVKg},
	abstract = {A hallmark of biological neural networks, which distinguishes them from their artificial counterparts, is the high degree of sparsity in their activations. This discrepancy raises three questions our work helps to answer: (i) Why are biological networks so sparse? (ii) What are the benefits of this sparsity? (iii) How can these benefits be utilized by deep learning models? Our answers to all of these questions center around training networks to handle random noise. Surprisingly, we discover that noisy training introduces three implicit loss terms that result in sparsely firing neurons specializing to high variance features of the dataset. When trained to reconstruct noisy-{CIFAR}10, neurons learn biological receptive fields. More broadly, noisy training presents a new approach to potentially increase model interpretability with additional benefits to robustness and computational efficiency.},
	author = {Bricken, Trenton and Schaeffer, Rylan and Olshausen, Bruno and Kreiman, Gabriel},
	urldate = {2023-07-23},
	date = {2023-06-15},
	year = {2023},
	month = {06},
	langid = {english},
	eventtitle = {International Conference on Machine Learning},
	publisher = {{PMLR}},
}


@misc{sparse_symbol,
	title = {Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in {AI} Models},
	url = {http://arxiv.org/abs/2305.01939},
	doi = {10.48550/arXiv.2305.01939},
	abstract = {This paper aims to prove the emergence of symbolic concepts in well-trained {AI} models. We prove that if (1) the high-order derivatives of the model output w.r.t. the input variables are all zero, (2) the {AI} model can be used on occluded samples and will yield higher confidence when the input sample is less occluded, and (3) the confidence of the {AI} model does not significantly degrade on occluded samples, then the {AI} model will encode sparse interactive concepts. Each interactive concept represents an interaction between a specific set of input variables, and has a certain numerical effect on the inference score of the model. Specifically, it is proved that the inference score of the model can always be represented as the sum of the interaction effects of all interactive concepts. In fact, we hope to prove that conditions for the emergence of symbolic concepts are quite common. It means that for most {AI} models, we can usually use a small number of interactive concepts to mimic the model outputs on any arbitrarily masked samples.},
	number = {{arXiv}:2305.01939},
	publisher = {{arXiv}},
	author = {Ren, Qihan and Gao, Jiayang and Shen, Wen and Zhang, Quanshi},
	urldate = {2023-07-08},
	date = {2023-05-03},
	year = {2023},
	month = {05},
	eprinttype = {arxiv},
	eprint = {2305.01939 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}


@inproceedings{primer,
	title = {Searching for Efficient Transformers for Language Modeling},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/2f3c6a4cd8af177f6456e7e51a916ff3-Abstract.html},
	abstract = {Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer {TensorFlow} program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer’s improvements can be mostly attributed to two simple modifications: squaring {ReLU} activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention.Experiments show Primer’s gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to {GPT}-3 {XL}, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.},
	pages = {6010--6022},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {So, David and Mańke, Wojciech and Liu, Hanxiao and Dai, Zihang and Shazeer, Noam and Le, Quoc V},
	urldate = {2023-08-17},
	date = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@book{chi_squared,
  title={Statistics for experimenters},
  author={Box, George EP and Hunter, William H and Hunter, Stuart and others},
  volume={664},
  year={1978},
  publisher={John Wiley and sons New York}
}

@misc {asymptotic_incomplete_gamma,
    title = {About the asymptotics of the incomplete gamma function $\gamma(a,z)$},
    author = {Antonio Vargas (https://math.stackexchange.com/users/5531/antonio-vargas)},
    howpublished = {Mathematics Stack Exchange},
    note = {URL:https://math.stackexchange.com/q/2181644 (version: 2017-04-13)},
    eprint = {https://math.stackexchange.com/q/2181644},
    url = {https://math.stackexchange.com/q/2181644}
}
@misc {product_of_gaussian,
    title = {Is the product of two Gaussian random variables also a Gaussian?},
    author = {Ulisses Braga-Neto (https://math.stackexchange.com/users/78702/ulisses-braga-neto)},
    howpublished = {Mathematics Stack Exchange},
    note = {URL:https://math.stackexchange.com/q/397716 (version: 2013-05-21)},
    eprint = {https://math.stackexchange.com/q/397716},
    url = {https://math.stackexchange.com/q/397716}
}

@inreference{hadamard_and_eigenvector,
	title = {Schur product theorem},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Schur_product_theorem&oldid=1108728042#Proof_using_eigendecomposition},
	booktitle = {Wikipedia},
	urldate = {2023-08-04},
	date = {2022-09-05},
	year = {2022},
	month = {09},
	langid = {english},
	note = {Page Version {ID}: 1108728042},
	file = {Snapshot:C\:\\Users\\78758\\Zotero\\storage\\AJN6ZMLT\\Schur_product_theorem.html:text/html},
}2 

@article{freq_bias,
	title = {How Much does Initialization Affect Generalization?},
	abstract = {Characterizing the remarkable generalization properties of over-parameterized neural networks remains an open problem. A growing body of recent literature shows that the bias of stochastic gradient descent ({SGD}) and architecture choice implicitly leads to better generalization. In this paper, we show on the contrary that, independently of architecture, {SGD} can itself be the cause of poor generalization if one does not ensure good initialization. Specifically, we prove that any differentiably parameterized model, trained under gradient flow, obeys a weak spectral bias law which states that sufficiently high frequencies train arbitrarily slowly. This implies that very high frequencies present at initialization will remain after training, and hamper generalization. Further, we empirically test the developed theoretical insights using practical, deep networks. Finally, we contrast our framework with that supplied by the flat-minima conjecture and show that Fourier analysis grants a more reliable framework for understanding the generalization of neural networks.},
	author = {Ramasinghe, Sameera and {MacDonald}, Lachlan and Farazi, Moshiur and Saratchandran, Hemanth and Lucey, Simon},
	langid = {english},
	file = {Ramasinghe 等 - How Much does Initialization Affect Generalization.pdf:C\:\\Users\\78758\\Zotero\\storage\\VFYUD2V4\\Ramasinghe 等 - How Much does Initialization Affect Generalization.pdf:application/pdf},
}

@inproceedings{vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{zhang_three_2018,
  title={Three Mechanisms of Weight Decay Regularization},
  author={Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{cifar10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@INPROCEEDINGS{imagenet1k,

  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},

  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 

  title={ImageNet: A large-scale hierarchical image database}, 

  year={2009},

  volume={},

  number={},

  pages={248-255},

  doi={10.1109/CVPR.2009.5206848}}


@online{pytorch_recipe,
	title = {vision/references/classification at main · pytorch/vision},
	author = {PyTorch},
	url = {https://github.com/pytorch/vision/tree/main/references/classification},
	abstract = {Datasets, Transforms and Models specific to Computer Vision - pytorch/vision},
	titleaddon = {{GitHub}},
	urldate = {2023-08-05},
	year = {2023},
	langid = {english},
	file = {Snapshot:C\:\\Users\\78758\\Zotero\\storage\\K6PPAY39\\classification.html:text/html},
}

@misc{zipit,
	title = {{ZipIt}! Merging Models from Different Tasks without Training},
	url = {http://arxiv.org/abs/2305.03053},
	doi = {10.48550/arXiv.2305.03053},
	abstract = {Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "{ZipIt}!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for a staggering 20-60\% improvement over prior work, making the merging of models trained on disjoint tasks feasible.},
	number = {{arXiv}:2305.03053},
	publisher = {{arXiv}},
	author = {Stoica, George and Bolya, Daniel and Bjorner, Jakob and Hearn, Taylor and Hoffman, Judy},
	urldate = {2023-08-08},
	date = {2023-05-04},
	year = {2023},
	month = {05},
	eprinttype = {arxiv},
	eprint = {2305.03053 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\78758\\Zotero\\storage\\BWLKZ44Z\\Stoica 等 - 2023 - ZipIt! Merging Models from Different Tasks without.pdf:application/pdf},
}

@inproceedings{flat_minima,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{switch_transformer,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{moe_vit,
	title = {Scaling Vision with Sparse Mixture of Experts},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html},
	abstract = {Sparsely-gated Mixture of Experts networks ({MoEs}) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are "dense", that is, every input is processed by every parameter. We present a Vision {MoE} (V-{MoE}), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-{MoE} matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-{MoE} to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-{MoE} to scale vision models, and train a 15B parameter model that attains 90.35\% on {ImageNet}.},
	pages = {8583--8595},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, André and Keysers, Daniel and Houlsby, Neil},
	urldate = {2023-08-17},
	date = {2021},
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\78758\\Zotero\\storage\\ZTHIKR7N\\Riquelme 等 - 2021 - Scaling Vision with Sparse Mixture of Experts.pdf:application/pdf},
}

@inproceedings{moe_DG,
  title={Sparse Mixture-of-Experts are Domain Generalizable Learners},
  author={Li, Bo and Shen, Yifei and Yang, Jingkang and Wang, Yezhen and Ren, Jiawei and Che, Tong and Zhang, Jun and Liu, Ziwei},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{adversarial_of_moe,
	title = {On the Adversarial Robustness of Mixture of Experts},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/3effb91593c4fb42b1da1528328eff49-Paper-Conference.pdf},
	pages = {9660--9671},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Puigcerver, Joan and Jenatton, Rodolphe and Riquelme, Carlos and Awasthi, Pranjal and Bhojanapalli, Srinadh},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	date = {2022},
	year = {2022},
	file = {10986_on_the_adversarial_robustness_-Supplementary Material.pdf:C\:\\Users\\78758\\Zotero\\storage\\546ESTQB\\10986_on_the_adversarial_robustness_-Supplementary Material.pdf:application/pdf;Puigcerver 等 - On the Adversarial Robustness of Mixture of Expert.pdf:C\:\\Users\\78758\\Zotero\\storage\\XZXXHYHB\\Puigcerver 等 - On the Adversarial Robustness of Mixture of Expert.pdf:application/pdf},
}

@inproceedings{attention_sparsity_1,
	location = {Hong Kong, China},
	title = {Adaptively Sparse Transformers},
	url = {https://aclanthology.org/D19-1223},
	doi = {10.18653/v1/D19-1223},
	abstract = {Attention mechanisms have become ubiquitous in {NLP}. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter – which controls the shape and sparsity of alpha-entmax – allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.},
	eventtitle = {{EMNLP}-{IJCNLP} 2019},
	pages = {2174--2184},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Correia, Gonçalo M. and Niculae, Vlad and Martins, André F. T.},
	urldate = {2023-08-17},
	date = {2019-11},
	year = {2019},
	month = {11},
	keywords = {Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\78758\\Zotero\\storage\\9HRZZ3W5\\Correia 等 - 2019 - Adaptively Sparse Transformers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\78758\\Zotero\\storage\\TP7CCGDA\\1909.html:text/html;Full Text PDF:C\:\\Users\\78758\\Zotero\\storage\\RERALL6G\\Correia 等 - 2019 - Adaptively Sparse Transformers.pdf:application/pdf},
}

@inproceedings{attention_sparsity_2,
	title = {{SwinBERT}: End-to-End Transformers With Sparse Attention for Video Captioning},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Lin_SwinBERT_End-to-End_Transformers_With_Sparse_Attention_for_Video_Captioning_CVPR_2022_paper.html},
	shorttitle = {{SwinBERT}},
	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {17949--17958},
	author = {Lin, Kevin and Li, Linjie and Lin, Chung-Ching and Ahmed, Faisal and Gan, Zhe and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
	urldate = {2023-08-09},
	year = {2022},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\78758\\Zotero\\storage\\KBS2NVCF\\Lin 等 - 2022 - SwinBERT End-to-End Transformers With Sparse Atte.pdf:application/pdf},
}

@inproceedings{attention_sparsity_3,
	location = {Cham},
	title = {{DynaST}: Dynamic Sparse Transformer for Exemplar-Guided Image Generation},
	isbn = {978-3-031-19787-1},
	doi = {10.1007/978-3-031-19787-1_5},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{DynaST}},
	abstract = {One key challenge of exemplar-guided image generation lies in establishing fine-grained correspondences between input and guided images. Prior approaches, despite the promising results, have relied on either estimating dense attention to compute per-point matching, which is limited to only coarse scales due to the quadratic memory cost, or fixing the number of correspondences to achieve linear complexity, which lacks flexibility. In this paper, we propose a dynamic sparse attention based Transformer model, termed Dynamic Sparse Transformer ({DynaST}), to achieve fine-level matching with favorable efficiency. The heart of our approach is a novel dynamic-attention unit, dedicated to covering the variation on the optimal number of tokens one position should focus on. Specifically, {DynaST} leverages the multi-layer nature of Transformer structure, and performs the dynamic attention scheme in a cascaded manner to refine matching results and synthesize visually-pleasing outputs. In addition, we introduce a unified training objective for {DynaST}, making it a versatile reference-based image translation framework for both supervised and unsupervised scenarios. Extensive experiments on three applications, pose-guided person image generation, edge-based face synthesis, and undistorted image style transfer, demonstrate that {DynaST} achieves superior performance in local details, outperforming the state of the art while reducing the computational cost significantly. Our code is available here.},
	pages = {72--90},
	booktitle = {Computer Vision – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Liu, Songhua and Ye, Jingwen and Ren, Sucheng and Wang, Xinchao},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	langid = {english},
	keywords = {Dynamic sparse attention, Exemplar-guided image generation, Transformer},
}

@article{mp_distribution,
  title={Rate of convergence in probability to the Marchenko-Pastur law},
  author={G{\"o}tze, Friedrich and Tikhomirov, Alexander},
  journal={Bernoulli},
  volume={10},
  number={3},
  pages={503--548},
  year={2004},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@inproceedings{parameter_growth,
	location = {Online and Punta Cana, Dominican Republic},
	title = {Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent},
	url = {https://aclanthology.org/2021.emnlp-main.133},
	doi = {10.18653/v1/2021.emnlp-main.133},
	shorttitle = {Effects of Parameter Norm Growth During Transformer Training},
	abstract = {The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent ({GD}). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude ({\textbackslash}ell\_2 norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such “saturated” networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in {GD} of particular interest for {NLP}. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.},
	eventtitle = {{EMNLP} 2021},
	pages = {1766--1781},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Merrill, William and Ramanujan, Vivek and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A.},
	urldate = {2023-08-17},
	date = {2021-11},
	year={2021},
	month={11},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\78758\\Zotero\\storage\\UELPRJ8X\\Merrill 等 - 2023 - Effects of Parameter Norm Growth During Transforme.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\78758\\Zotero\\storage\\KFHGWNK3\\2010.html:text/html;Full Text PDF:C\:\\Users\\78758\\Zotero\\storage\\VZVADG3T\\Merrill 等 - 2021 - Effects of Parameter Norm Growth During Transforme.pdf:application/pdf},
}

@inproceedings{alpha_stable,
	title = {A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks},
	url = {https://proceedings.mlr.press/v97/simsekli19a.html},
	abstract = {The gradient noise ({GN}) in the stochastic gradient descent ({SGD}) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem ({CLT}) kicks in. This assumption is often made for mathematical convenience, since it enables {SGD} to be analyzed as a stochastic differential equation ({SDE}) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the {GN} in a more general context and invoke the generalized {CLT} ({GCLT}), which suggests that the {GN} converges to a heavy-tailed αα{\textbackslash}alpha-stable random variable. Accordingly, we propose to analyze {SGD} as an {SDE} driven by a Lévy motion. Such {SDEs} can incur ‘jumps’, which force the {SDE} transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the αα{\textbackslash}alpha-stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the {GN} is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that {SGD} prefers wide minima.},
	eventtitle = {International Conference on Machine Learning},
	pages = {5827--5837},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
	urldate = {2023-08-17},
	date = {2019-05-24},
	year = {2019},
	month = {05},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\78758\\Zotero\\storage\\IQ7JM46X\\Simsekli 等 - 2019 - A Tail-Index Analysis of Stochastic Gradient Noise.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\78758\\Zotero\\storage\\KEG5AQ8H\\1901.html:text/html;Full Text PDF:C\:\\Users\\78758\\Zotero\\storage\\UXKZZMXM\\Simsekli 等 - 2019 - A Tail-Index Analysis of Stochastic Gradient Noise.pdf:application/pdf},
}

@misc{relax_mp_distribution,
	title = {Marchenko-Pastur law with relaxed independence conditions},
	url = {http://arxiv.org/abs/1912.12724},
	doi = {10.48550/arXiv.1912.12724},
	abstract = {We prove the Marchenko-Pastur law for the eigenvalues of \$p {\textbackslash}times p\$ sample covariance matrices in two new situations where the data does not have independent coordinates. In the first scenario - the block-independent model - the \$p\$ coordinates of the data are partitioned into blocks in such a way that the entries in different blocks are independent, but the entries from the same block may be dependent. In the second scenario - the random tensor model - the data is the homogeneous random tensor of order \$d\$, i.e. the coordinates of the data are all \${\textbackslash}binom\{n\}\{d\}\$ different products of \$d\$ variables chosen from a set of \$n\$ independent random variables. We show that Marchenko-Pastur law holds for the block-independent model as long as the size of the largest block is \$o(p)\$ and for the random tensor model as long as \$d = o(n{\textasciicircum}\{1/3\})\$. Our main technical tools are new concentration inequalities for quadratic forms in random variables with block-independent coordinates, and for random tensors.},
	number = {{arXiv}:1912.12724},
	publisher = {{arXiv}},
	author = {Bryson, Jennifer and Vershynin, Roman and Zhao, Hongkai},
	urldate = {2023-08-14},
	date = {2021-02-02},
	year = {2021},
	month = {02},
	eprinttype = {arxiv},
	eprint = {1912.12724 [math]},
	keywords = {60B20, Mathematics - Probability},
	file = {arXiv Fulltext PDF:C\:\\Users\\78758\\Zotero\\storage\\8VT6DAMB\\Bryson 等 - 2021 - Marchenko-Pastur law with relaxed independence con.pdf:application/pdf},
}



@inproceedings{ib_disentangle,
	location = {San Diego, {CA}},
	title = {Emergence of Invariance and Disentanglement in Deep Representations},
	isbn = {978-1-72810-124-8},
	url = {https://ieeexplore.ieee.org/document/8503149/},
	doi = {10.1109/ITA.2018.8503149},
	abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overﬁtting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a {PAC}-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underﬁtting and overﬁtting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
	eventtitle = {2018 Information Theory and Applications Workshop ({ITA})},
	pages = {1--9},
	booktitle = {2018 Information Theory and Applications Workshop ({ITA})},
	publisher = {{IEEE}},
	author = {Achille, Alessandro and Soatto, Stefano},
	urldate = {2023-10-08},
	date = {2018-02},
	year = {2018},
	month = {02},
	langid = {english},
	file = {Achille 和 Soatto - 2018 - Emergence of Invariance and Disentanglement in Dee.pdf:C\:\\Users\\78758\\Zotero\\storage\\FKFLEPKX\\Achille 和 Soatto - 2018 - Emergence of Invariance and Disentanglement in Dee.pdf:application/pdf},
}


@article{mp_quadratic_form,
  title={A short proof of the Marchenko--Pastur theorem},
  author={Yaskov, Pavel},
  journal={Comptes Rendus Mathematique},
  volume={354},
  number={3},
  pages={319--322},
  year={2016},
  publisher={Elsevier}
}

@article{mp_linear_time_series1,
  title={A note on a Mar{\v{c}}enko--Pastur type theorem for time series},
  author={Yao, Jianfeng},
  journal={Statistics \& probability letters},
  volume={82},
  number={1},
  pages={22--28},
  year={2012},
  publisher={Elsevier}
}

@article{mp_linear_time_series2,
	title = {On the Mar\v{c}enko-Pastur law for linear time series},
	volume = {43},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1310.7270},
	doi = {10.1214/14-AOS1294},
	abstract = {This paper is concerned with extensions of the classical Mar{\textbackslash}v\{c\}enko-Pastur law to time series. Specifically, \$p\$-dimensional linear processes are considered which are built from innovation vectors with independent, identically distributed (real- or complex-valued) entries possessing zero mean, unit variance and finite fourth moments. The coefficient matrices of the linear process are assumed to be simultaneously diagonalizable. In this setting, the limiting behavior of the empirical spectral distribution of both sample covariance and symmetrized sample autocovariance matrices is determined in the high-dimensional setting \$p/n{\textbackslash}to c{\textbackslash}in (0,{\textbackslash}infty)\$ for which dimension \$p\$ and sample size \$n\$ diverge to infinity at the same rate. The results extend existing contributions available in the literature for the covariance case and are one of the first of their kind for the autocovariance case.},
	number = {2},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Liu, Haoyang and Aue, Alexander and Paul, Debashis},
	urldate = {2023-08-22},
	date = {2015-04-01},
	year = {2015},
	month = {04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1310.7270 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
	file = {Liu 等 - 2015 - On the Marv c enko-Pastur law for linear time ser.pdf:C\:\\Users\\78758\\Zotero\\storage\\G934EH5W\\Liu 等 - 2015 - On the Marv c enko-Pastur law for linear time ser.pdf:application/pdf},
}

@article{mp_martingale,
	title = {Limit Theorems for Spectra of Positive Random Matrices under Dependence},
	volume = {133},
	issn = {1072-3374, 1573-8795},
	url = {http://link.springer.com/10.1007/s10958-006-0035-8},
	doi = {10.1007/s10958-006-0035-8},
	pages = {1257--1276},
	number = {3},
	journaltitle = {Journal of Mathematical Sciences},
	shortjournal = {J Math Sci},
	author = {Gotze, F. and Tikhomirov, A.},
	urldate = {2023-08-22},
	date = {2006-03},
	year = {2006},
	month = {03},
	langid = {english},
	file = {Gotze 和 Tikhomirov - 2006 - Limit Theorems for Spectra of Positive Random Matr.pdf:C\:\\Users\\78758\\Zotero\\storage\\BNMSK4KQ\\Gotze 和 Tikhomirov - 2006 - Limit Theorems for Spectra of Positive Random Matr.pdf:application/pdf},
}

@book{RMT_book,
	location = {New York, {NY}},
	title = {Spectral Analysis of Large Dimensional Random Matrices},
	isbn = {978-1-4419-0660-1 978-1-4419-0661-8},
	url = {https://link.springer.com/10.1007/978-1-4419-0661-8},
	series = {Springer Series in Statistics},
	publisher = {Springer},
	author = {Bai, Zhidong and Silverstein, Jack W.},
	urldate = {2023-08-23},
	date = {2010},
	year = {2010},
	langid = {english},
	doi = {10.1007/978-1-4419-0661-8},
	keywords = {Eigenvalue, Eigenvector, Fitting, Matrix, Matrix Theory, Random variable},
}


@article{mp_proof_sketch,
	title = {{LARGE} {SAMPLE} {COVARIANCE} {MATRICES} {WITHOUT} {INDEPENDENCE} {STRUCTURES} {IN} {COLUMNS}},
	abstract = {The limiting spectral distribution of large sample covariance matrices is derived under dependence conditions. As applications, we obtain the limiting spectral distributions of Spearman’s rank correlation matrices, sample correlation matrices, sample covariance matrices from ﬁnite populations, and sample covariance matrices from causal {AR}(1) models.},
	author = {Bai, Zhidong and Zhou, Wang},
	langid = {english},
	file = {Bai 和 Zhou - LARGE SAMPLE COVARIANCE MATRICES WITHOUT INDEPENDE.pdf:C\:\\Users\\78758\\Zotero\\storage\\QKRAKAJU\\Bai 和 Zhou - LARGE SAMPLE COVARIANCE MATRICES WITHOUT INDEPENDE.pdf:application/pdf},
}

@article{mp_original,
	title = {{DISTRIBUTION} {OF} {EIGENVALUES} {FOR} {SOME} {SETS} {OF} {RANDOM} {MATRICES}},
	volume = {1},
	issn = {0025-5734},
	url = {https://iopscience.iop.org/article/10.1070/SM1967v001n04ABEH001994},
	doi = {10.1070/SM1967v001n04ABEH001994},
	pages = {457--483},
	number = {4},
	journaltitle = {Mathematics of the {USSR}-Sbornik},
	shortjournal = {Math. {USSR} Sb.},
	author = {Marčenko, V A and Pastur, L A},
	urldate = {2023-08-24},
	date = {1967-04-30},
	year = {1967},
	month = {04},
	langid = {english},
	file = {Marčenko 和 Pastur - 1967 - DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDO.pdf:C\:\\Users\\78758\\Zotero\\storage\\L5IW4M4M\\Marčenko 和 Pastur - 1967 - DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDO.pdf:application/pdf},
}

@article{exponential_decay_to_as,
  title={Introduction to probability, statistics, and random processes},
  author={Pishro-Nik, Hossein},
  year={2016}
}

@MISC {sum_eigenvalue_and_singular_value,
    TITLE = {Sum of eigenvalues and singular values},
    AUTHOR = {23rd (https://math.stackexchange.com/users/46120/23rd)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/386872 (version: 2013-05-09)},
    EPRINT = {https://math.stackexchange.com/q/386872},
    URL = {https://math.stackexchange.com/q/386872}
}


@misc{scaling_law,
	title = {Scaling Laws for Neural Language Models},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overﬁtting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sampleefﬁcient, such that optimally compute-efﬁcient training involves training very large models on a relatively modest amount of data and stopping signiﬁcantly before convergence.},
	number = {{arXiv}:2001.08361},
	publisher = {{arXiv}},
	author = {Kaplan, Jared and {McCandlish}, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	urldate = {2023-03-24},
	date = {2020-01-22},
	year = {2020},
	month = {01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, large model},
	file = {Kaplan 等 - 2020 - Scaling Laws for Neural Language Models.pdf:C\:\\Users\\78758\\Zotero\\storage\\PP87E72Z\\Kaplan 等 - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf},
}
@article{mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{synaptic_noise_1,
  title={Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training},
  author={Murray, Alan F and Edwards, Peter J},
  journal={IEEE Transactions on neural networks},
  volume={5},
  number={5},
  pages={792--802},
  year={1994},
  publisher={IEEE}
} 

@article{synaptic_noise_2,
  title={Synaptic weight noise during multilayer perceptron training: fault tolerance and training improvements},
  author={Murray, Alan F and Edwards, Peter J},
  journal={IEEE Transactions on Neural Networks},
  volume={4},
  number={4},
  pages={722--725},
  year={1993},
  publisher={IEEE}
}


@inproceedings{sparsity_and_scalability,
	title = {Sparse is Enough in Scaling Transformers},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/51f15efdd170e6043fa02a74882f0470-Abstract.html},
	abstract = {Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.},
	pages = {9895--9907},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and {KAISER}, {LUKASZ} and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
	urldate = {2023-09-21},
	date = {2021},
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\78758\\Zotero\\storage\\H3RQDSBP\\Jaszczur 等 - 2021 - Sparse is Enough in Scaling Transformers.pdf:application/pdf},
}

@article{empirical_hessian,
	title = {Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
	url = {https://openreview.net/forum?id=rJO1_M0Lf},
	abstract = {We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the *flatness* of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create *large* connected components at the bottom of the landscape. Second, the dependence of a small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light on the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.},
	author = {Sagun, Levent and Evci, Utku and Guney, V. Ugur and Dauphin, Yann and Bottou, Leon},
	urldate = {2023-10-01},
	date = {2018-02-11},
	year = {2018},
	mon = {02},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\78758\\Zotero\\storage\\G2L75K2N\\Sagun 等 - 2018 - Empirical Analysis of the Hessian of Over-Parametr.pdf:application/pdf},
}



@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}



@online{t5_recipe,
	title = {T5-like span-masked language modeling},
	url = {https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling},
	abstract = {🤗 Transformers: State-of-the-art Machine Learning for Pytorch, {TensorFlow}, and {JAX}. - huggingface/transformers},
	author = {Huggingface},
	titleaddon = {{GitHub}},
	urldate = {2023-10-07},
	year={2023},
	langid = {english},
}

@article{mixer,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@inproceedings{exploit_sparsity_in_CNN,
	title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks},
	url = {https://proceedings.mlr.press/v119/kurtz20a.html},
	abstract = {Optimizing convolutional neural networks for fast inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network. Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear ({ReLU}) activation functions. In this paper, we present an in-depth analysis of methods for maximizing the sparsity of the activations in a trained neural network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains. To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit ({FATReLU}). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss. Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale {ImageNet} image classification task on modern Intel {CPUs}, with little or no retraining cost.},
	eventtitle = {International Conference on Machine Learning},
	pages = {5533--5543},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Shavit, Nir and Alistarh, Dan},
	urldate = {2023-10-25},
	year = {2020},
	month = {11},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Kurtz 等 - 2020 - Inducing and Exploiting Activation Sparsity for Fa.pdf:C\:\\Users\\78758\\Zotero\\storage\\V9E9XEDH\\Kurtz 等 - 2020 - Inducing and Exploiting Activation Sparsity for Fa.pdf:application/pdf},
}

@inproceedings{exploit_input_sparsity2,
  title={Exploiting the input sparsity to accelerate deep neural networks: poster},
  author={Dong, Xiao and Liu, Lei and Li, Guangli and Li, Jiansong and Zhao, Peng and Wang, Xueying and Feng, Xiaobing},
  booktitle={Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
  pages={401--402},
  year={2019}
}

@article{hoyer,
  title={Non-negative matrix factorization with sparseness constraints.},
  author={Hoyer, Patrik O},
  journal={Journal of machine learning research},
  volume={5},
  number={9},
  year={2004}
}

@inproceedings{L1_sparsity,
  title={Accelerating convolutional neural networks via activation map compression},
  author={Georgiadis, Georgios},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7085--7095},
  year={2019}
}



@inproceedings{anti_PGD,
	title = {Anticorrelated Noise Injection for Improved Generalization},
	url = {https://proceedings.mlr.press/v162/orvieto22a.html},
	abstract = {Injecting artificial noise into gradient descent ({GD}) is commonly employed to improve the performance of machine learning models. Usually, uncorrelated noise is used in such perturbed gradient descent ({PGD}) methods. It is, however, not known if this is optimal or whether other types of noise could provide better generalization performance. In this paper, we zoom in on the problem of correlating the perturbations of consecutive {PGD} steps. We consider a variety of objective functions for which we find that {GD} with anticorrelated perturbations ("Anti-{PGD}") generalizes significantly better than {GD} and standard (uncorrelated) {PGD}. To support these experimental findings, we also derive a theoretical analysis that demonstrates that Anti-{PGD} moves to wider minima, while {GD} and {PGD} remain stuck in suboptimal regions or even diverge. This new connection between anticorrelated noise and generalization opens the field to novel ways to exploit noise for training machine learning models.},
	eventtitle = {International Conference on Machine Learning},
	pages = {17094--17116},
	booktitle = {Proceedings of the 39th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Orvieto, Antonio and Kersting, Hans and Proske, Frank and Bach, Francis and Lucchi, Aurelien},
	urldate = {2023-10-16},
	year = {2022},
	month = {06},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Orvieto 等 - 2022 - Anticorrelated Noise Injection for Improved Genera.pdf:C\:\\Users\\78758\\Zotero\\storage\\KMPX52MH\\Orvieto 等 - 2022 - Anticorrelated Noise Injection for Improved Genera.pdf:application/pdf},
}
