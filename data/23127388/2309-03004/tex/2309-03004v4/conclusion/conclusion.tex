\section{Conclusion}

In this work, we explain the activation sparsity observed by \citet{observation} with gradient sparsity and effective gradient sparsity, which emerges from flatness and implicit adversarial robustness. 
Proofs are done for pure MLPs and other architectures including Transformers under hypothetical massively perturbed training. LayerNorm plays a critical role in them.
To argue the effectiveness of zeroth biases, we analyze the phenomenon of spectral concentration in weight matrices by introducing random matrix theory to the research of training dynamics. 
We propose two sparsity-oriented architectural plug-and-play modifications and one radical modification. 
Experiments for verification, where our theory predicts well, rule out some potential explanations and provide support for our emphasis on gradient sparsity. 
We test our modification on ImageNet-1K and C4 to demonstrate its great practical effectiveness. 
Finetuning for sparsity demonstrates a cheaper way toward better sparsity for existing models. Assumptions made during analyses are empirically validated.

Having demonstrated how sparsity emerges from noises and the robustness against them, we wildly conjecture that a similar explanation applies to sparsity, and thus energy efficiency, in human brains and cognition, which face enormous environmental, sensory and synaptic noises as well as small-batch learning every day and exhibit great robustness against them. Our work also demonstrates that some architectural designs may greatly ease theoretical analyses in deep neural networks, and more theoretically oriented architectures can be proposed in the future. We introduce random matrix theory as a powerful tool in analyzing training dynamics, hoping for its broader application in the machine learning community.
