\section{Experimental Details}\label{appendix:experimental_details}

In experiments for validation (\cref{sec:v_experiments}), ViT-Base/16 is trained on CIFAR-10 using Adam. Hyperparameters include learning rate of $10^{-4}$, batch size of $64$, no dropout, weight decay, learning rate decay or gradient clipping. Random cropping and random horizontal flipping is used as data augmentation. The training lasts for 100 epochs with the first 5 epochs of linear warmup. Automatic mixed precision (AMP) is not used. Note that only $\weird$ and unrefined $\dbmlp$ are used. $\dbmlp$ is \emph{not} restricted and there are \emph{no} restricted LayerNorm layers as well. The ViT implementation is based on that of Torch Vision. Logging happens every 10 steps.

During training ViT-Base/16 from scratch on ImageNet-1K, the recipe is adapted from \citet{pytorch_recipe}. Specifically, the optimizer is AdamW with learning rate $3 \times 10^{-3}$, $(\beta_1, \beta_2)=(0.9, 0.999)$, weight decay $0.3$, dropout $0.0$ and gradient clipping of global norm $1.0$. Model EMA is \emph{removed} to ease experiment tracking. Learning rate is scheduled with cosine annealing without warmup. Batch size is $2048$, distributed on 4 GPUs for 300 epochs. Other tricks include mixup of alpha $0.2$, cutmix of alpha $1.0$, auto augmentation of policy \texttt{ra}, label smoothing of $0.11$. AMP with \texttt{fp16} is used to accelerate training. Images are resized to $256 \times 256$ to save storage. Logging happens every 100 steps.
During finetuning, the differences include modifications for sparsity described in \cref{sec:refine} and \cref{sec:p_experiments}, LoRA of rank $192$, and epoch number reduced to $15$, among which LayerNorm uplifting takes $5$ epochs.

During training T5-Base from scratch on C4, the recipe is adapted from \citet{t5_recipe} and \citet{observation}. Specifically, the optimizer is AdamW with learning rate $0.01$, $(\beta_1, \beta_2)=(0.9, 0.999)$, weight decay $0.001$, dropout $0.0$ and gradient clipping of global norm $1.0$. Learning rate is scheduled with inverse square root scheduling with linear warmup of $10,000$ steps, while the full training lasts for $100,000$ steps. Batch size is $256$. To avoid downloading the entire data set, we use the streaming functionality of Huggingface \texttt{datasets} to download the first $25,600,000$ training samples without shuffling and the full validation split. Recipe from \citet{t5_recipe} includes data preparation that concatenates all raw sentences and re-splits them into samples. This step consumes too much storage so we confine the concatenation within a batch and empties are filled with padding tokens that are ignored during computing the loss. This compromise reduces the number of usable samples during training, but there are still approximately at least $50$ non-empty samples for every 64 samples. The generation of spans is left unchanged, where 15\% of the input is corrupted without any mixture. The encoder receives $512$ tokens while $114$ tokens are used on the decoder side. Logging happens every 25 steps. When training modified T5, the rented GPUs expired at step $95,000$. Nevertheless, the effectiveness of modification is already obvious at that step.
During finetuning, LoRA of rank $192$ and modifications described in \cref{sec:refine} and \cref{sec:p_experiments} are equipped. The training step reduces to $10,000$, among which learning rate warmup takes $1,000$ steps and LayerNorm uplifting and activation function mixing last for $3,000$ steps. Logging happens every $50$ steps during finetuning T5.


For experiments in \cref{sec:t_exp:anisotropy}, the tiny MLPs have an input layer of with $32 \times 32$, five hidden layers of width $p$ (or equivalently four hidden linear layers whose matrices are of size $p \times p$) and one output layer of width $10$. Skip connection is placed across each of the 4 hidden linear layers. LayerNorm layers with affine parameters \emph{turned off} are also placed before hidden layers. The optimizer is SGD with learning rate $10^{-3}$. Weight decay is altered as a part of the experiment. Batch size is $32$, the number of epochs is $100$. No learning rate warmup is used because it alters the weight decay after multiplying learning rate during training and bothers the computation of $r$. Data augmentation includes random cropping and random rotation of at most 30 degrees. Logging happens every 20 steps.