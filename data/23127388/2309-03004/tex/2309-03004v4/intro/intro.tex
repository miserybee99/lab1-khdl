\section{Introduction}

Despite the success of overparameterized deep neural networks, how they learn, work and generalize well is not fully understood. 
Although more than half of parameters and computation are devoted to them \citep{knowledge_neurons}, even in Transformers, MLP blocks have remained black box for years, blocking interpretation, manipulation and pruning in them.
Recently, some works \citep{mlp_as_database,knowledge_neurons} have tried to dive into MLP blocks and reveal its relation with learned knowledge by rewriting MLP blocks of Transformers into an attention mechanism where keys and values are provided by the first and second linear layers, and form a key-value memory that stores learned knowledge. 

A more recent work \citep{observation} further discovers activation sparsity in MLP blocks, i.e., only a small portion of neurons are activated during inference, within MLP blocks of pure MLP, ResNet, T5, ($\relu$) ViT and other architectures on various tasks, \emph{without} explicit regularization. This discovery not only brings research attention back to MLP blocks when it comes to understanding Transformers but also leads to potential aggressive but unstructured and dynamic neuron pruning during inference and thus large reduction in inference cost. Specifically, if pruning is ideally implemented to skip non-activated neurons, theoretically in T5 the inference costs happening in MLP blocks can be astonishingly reduced by about $90\%$. There is also an increasing tendency of activation sparsity, measured in percentage of zero activations, when models become larger \citep{observation}, hinting at potentially great cost reduction for large models. 
Activation sparsity in CNNs, although weaker \citep{observation,exploit_sparsity_in_CNN}, has been exploited to provide on-the-fly pruning or compression during inference \citep{exploit_sparsity_in_CNN}. 

However, the emergence of sparsity is not yet fully understood. Several works \citep{observation,sharpness_aware,large_step,from_noises} have been proposed to explain the emergence of sparsity from training dynamics. 
\citet{observation} explain activation sparsity of the last layer during the first step by computing gradients and exploiting properties of initialization methods. 
When sharpness-aware optimization is used, \citet{sharpness_aware} find positive components in gradients that point towards norm reduction of activations. However, only a shallow 2-layer MLP is studied by \citet{sharpness_aware}. \citet{large_step} consider second-order behaviors of SGD and proves sparsity on diagonal 2-layer MLPs, but for general networks it is only conjectured. 
\citet{from_noises} find that noises added to samples improve sparsity. However, the noises are manually imposed and are not included in standard augmentations. 
Although having achieved better understanding on sparsity and training dynamics and hinting at the roles of noises and flatness in activation sparsity, these works are still restricted to shallow networks, small steps, additional regularization or augmentations that cannot be found in ubiquitous training protocols. Nevertheless, they point out the role of flatness or noises in the emergence of sparsity.

Filling the gap between experiments and these theoretical results, we propose a new theoretical explanation that applies to deep networks, large training steps and standard training practices, by further emphasizing flatness and noises.
In particular, we first rewrite the flatness bias of SGD into a tendency to improve implicit adversarial robustness w.r.t. hidden features and parameters. 
Gradient sparsity and effective gradient sparsity are proposed as causes of activation sparsity. 
To support this, we prove a theorem stating that these kinds of sparsity can be one of the sources of implicit adversarial robustness. Since flat minima bias puts constraints on all layers, basing our explanation on this inductive bias allows us to reason about deep layers.
To eliminate other potential sources of implicit adversarial robustness, we exploit LayerNorm layers, refer to an already discovered inductive bias called parameter growth \citep{parameter_growth} and empirically discover a new phenomenon of spectral concentration, i.e., the fraction between the largest and smallest non-zero singular values of the first weight matrices in MLP blocks is not very large.
We prove the emergence of spectral concentration at initialization. We also theoretically discuss its re-emergence and maintenance during later training using random matrix theory (RMT) by extracting from stochastic gradients two large random matrices, indicating that training stochasticity's contribution to sparsity is two-folded. Notably, thanks to RMT our formulation of stochastic gradient noises is very direct, without assuming Gaussian or $\sas$ distributions on them but uniform and independent sampling \emph{within a batch} and bounds on anisotropy and gradient norms that can be estimated empirically.
Following these theoretical insights, we propose two plug-and-play and orthogonal architectural modifications, which brings relatively approximately $50\%$ training sparsity improvements and at least $36\%$ on testing sparsity compared to existing naturally emergent sparsity of $10\%$ or $20\%$ non-zero activations. 
The structure of this work is as follows:
\begin{itemize}
    \item In \cref{sec:preliminary}, we introduce preliminaries and background information. But in \cref{sec:preliminary:sparsity}, we propose gradient sparsity, distinguish it from activation sparsity and argue its importance over activation sparsity;
    \item In \cref{sec:illustration}, we build an intuitive framework of our explanation from flat minima and implicit adversarial robustness. Two almost plug-and-play architectural modifications, $\dbmlp$ and $\jrelu$, are proposed to further improve sparsity and ease formal analyses;
    \item In \cref{sec:gradients}, gradients of MLP blocks are computed, laying the basis for later formal analyses. Effective gradient sparsity is defined in this subsection, and its connection with training updates as well as gradient sparsity is discussed. Specifically, \cref{lemma:eff_and_sparsity} connects effective gradient sparsity measure in $L_2$ norms to activation sparsity directly measured in $L_0$ norms for $\relu$ networks, while the similar connection under $\jrelu$ is approximately discussed;
    \item In \cref{sec:three_elements}, we prove \cref{theorem:main} that relates flat minima and implicit adversarial robustness to effective gradient sparsity and activation sparsity by proving relatively tight chained upperbounds among them, demonstrating that sparsity can be the source of implicit adversarial robustness imposed by flat minima;
    \item In \cref{sec:discussion}, we instantiate \cref{theorem:main} in several specific settings involving pure MLPs and Transformers.
    Among them, \cref{theorem:main_with_hidden_vectors_and_layernorm} proves the tendency toward effective gradient sparsity on pure MLPs with LayerNorms. We argue that effective gradient sparsity is more stable and powerful during the entire training than direct activation sparsity.
    \cref{theorem:main_with_effective_duplication} deals with Transformers and other architectures by assuming perturbation training like dropout or tokenwise synapse noises. \arxivonly{Another under-testing module called $\magic$ is immediate after \cref{theorem:main_with_effective_duplication} and is elaborated on in \cref{appendix:magic}.}
        We discuss the effectiveness of zeroth biases in the rest of this subsection.
        Aside from effective gradient sparsity, implicit adversarial robustness and flatness can potentially be achieved by reducing norms of a matrix or misaligning gradients with that matrix in the term brought by our modification.
        To eliminate the first, the already discovered phenomenon of parameter growth is exploited. The latter is handled in later subsections.
    \item Eliminating the latter, we discover another phenomenon in ViT and T5 that most non-zero eigenvalues of the matrix differ by at most 100 times for most of the time, leaving only two possibilities: adversarial robustness is achieved only by (effective) gradient sparsity, or back-propagated gradients are totally lost. In \cref{sec:spectral_init}, we prove the emergence of this spectral concentration at initialization, exploiting modern initialization techniques. A drastic architectural modification, wide MLPs, is proposed to fill a gap of theories;
    \item In \cref{sec:spectral_training} we discuss spectral concentration's maintenance and re-emergence in latter stochastic training, applying random matrix theory by extracting two large random matrices from the updates to the weight matrices;
    \item In \cref{sec:refine}, $\dbmlp$ is refined following theoretical insights of \cref{sec:theory};
    \item In \cref{sec:v_experiments}, we conduct experiments 1) to show that activation sparsity can be lost but gradient sparsity is stable, and 2) to verify our explanation;
    \item In \cref{sec:p_experiments}, we train modified ViT-Base/16 on ImageNet-1K as well as T5-Base on C4 from scratch to examine the effectiveness of our modifications in the sense of sparsity and further verify our explanation. We also finetune trained weights for sparsity after plugging the two modifications to demonstrate a cheaper way to become sparser;
    \item In \cref{sec:t_experiments}, assumptions made in \cref{sec:three_elements} and \cref{sec:spectral_training} are examined empirically.
\end{itemize}
To summarize our contribution, we propose the notions of gradient sparsity, effective gradient sparsity and implicit adversarial robustness. 
We explain activation sparsity with flat minima and implicit adversarial robustness, and propose two architectural theoretically guided modifications to improve sparsity.
The benefits of LayerNorm layers and the practice of excluding their parameters from weight decay are emphasized in theories and experiments.
To our knowledge, we are the first to utilize random matrix theory to reason about inductive biases of stochastic training. As a result, the modeling of stochastic gradient noises (SGN) is very direct and avoids any debatable SGN modeling like Gaussian or $\sas$ models.
Experiments show that our explanation is more applicable than other potential ones and our modification further improves activation sparsity by a large margin.

\jmlronly{Codes of experiments can be found on GitHub\footnote{\coderepo{}} and raw logs recorded by TensorBoard are available on Huggingface\footnote{\logrepo{}}.}