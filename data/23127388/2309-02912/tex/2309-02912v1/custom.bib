% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{kirk2023semeval,
  title={SemEval-2023 Task 10: Explainable Detection of Online Sexism},
  author={Kirk, Hannah Rose and Yin, Wenjie and Vidgen, Bertie and R{\"o}ttger, Paul},
  journal={arXiv preprint arXiv:2303.04222},
  year={2023}
}

@article{mathew2021hatexplain, title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17745}, DOI={10.1609/aaai.v35i17.17745}, abstractNote={Hate speech is a challenging issue plaguing the online social media. While better models for hate speech detection are continuously being developed, there is little research on the bias and interpretability aspects of hate speech. In this paper, we introduce HateXplain, the first benchmark hate speech dataset covering multiple aspects of the issue. Each post in our dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based. We utilize existing state-of-the-art models and observe that even models that perform very well in classification do not score high on explainability metrics like model plausibility and faithfulness. We also observe that models, which utilize the human rationales for training, perform better in reducing unintended bias towards target communities. We have made our code and dataset public for other researchers.}, number={17}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh}, year={2021}, month={May}, pages={14867-14875} }

@inproceedings{wijesiriwardene2020alone,
author = {Wijesiriwardene, Thilini and Inan, Hale and Kursuncu, Ugur and Gaur, Manas and Shalin, Valerie L. and Thirunarayan, Krishnaprasad and Sheth, Amit and Arpinar, I. Budak},
title = {ALONE: A Dataset for Toxic Behavior Among Adolescents on Twitter},
year = {2020},
isbn = {978-3-030-60974-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60975-7_31},
doi = {10.1007/978-3-030-60975-7_31},
booktitle = {Social Informatics: 12th International Conference, SocInfo 2020, Pisa, Italy, October 6–9, 2020, Proceedings},
pages = {427–439},
numpages = {13},
keywords = {Toxicity, Harassment, Social media, Dataset, Resource},
location = {Pisa, Italy}
}

@misc{kennedy_atari_davani_yeh_omrani_kim_coombs_havaldar_portillo,
 title={Introducing the Gab Hate Corpus: Defining and applying hate-based rhetoric to social media posts at scale},
 url={psyarxiv.com/hqjxn},
 DOI={10.1007/s10579-021-09569-x},
 publisher={PsyArXiv},
 author={Kennedy, Brendan and Atari, Mohammad and Davani, Aida M and Yeh, Leigh and Omrani, Ali and Kim, Yehsong and Coombs, Kris and Havaldar, Shreya and Portillo-Wightman, Gwenyth and Gonzalez, Elaine and et al.},
 year={2018},
 month={Jul}
}

@article{Sarkar_KhudaBukhsh_2021, title={Are Chess Discussions Racist? An Adversarial Hate Speech Data Set (Student Abstract)}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17937}, DOI={10.1609/aaai.v35i18.17937}, abstractNote={On June 28, 2020, while presenting a chess podcast on Grandmaster Hikaru Nakamura, Antonio Radic’s YouTube handle got blocked because it contained ``harmful and dangerous’’ content. YouTube did not give further specific reason, and the channel got reinstated within 24 hours. However, Radic speculated that given the current political situation, a referral to ``black against white’’, albeit in the context of chess, earned him this temporary ban. In this paper, via a substantial corpus of 681,995 comments, on 8,818 YouTube videos hosted by five highly popular chess-focused YouTube channels, we ask the following research question: \emph{how robust are off-the-shelf hate-speech classifiers to out-of-domain adversarial examples?} We release a data set of 1,000 annotated comments where existing hate speech classifiers misclassified benign chess discussions as hate speech. We conclude with an intriguing analogy result on racial bias with our findings pointing out to the broader challenge of color polysemy.}, number={18}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sarkar, Rupak and KhudaBukhsh, Ashiqur R.}, year={2021}, month={May}, pages={15881-15882} }

@article{Mollas_2022,
  title={Ethos: an online hate speech detection dataset},
  author={Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios},
  journal={arXiv preprint arXiv:2006.08328},
  year={2020}
}

@article{10.1145/3583562,
author = {Sarker, Jaydeb and Turzo, Asif Kamal and Dong, Ming and Bosu, Amiangshu},
title = {Automated Identification of Toxic Code Reviews Using ToxiCR},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3583562},
doi = {10.1145/3583562},
abstract = {Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community to maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on Software Engineering (SE) dataset, such as one curated from code review comments. To encounter this challenge, we present ToxiCR, a supervised learning-based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the ten supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are SE domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8\% accuracy and 88.9\% F11 score. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pretrained models, evaluation results, and source code publicly available at: https://github.com/WSU-SEAL/ToxiCR).},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
keywords = {natural language processing, code review, toxicity, tool development, sentiment analysis}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Ribeiro_Calais_Santos_Almeida_Meira, title={Characterizing and Detecting Hateful Users on Twitter}, volume={12}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/15057}, DOI={10.1609/icwsm.v12i1.15057}, abstractNote={ &lt;p&gt; Current approaches to characterize and detect hate speech focus on content posted in Online Social Networks (OSNs). They face shortcomings to get the full picture of hate speech due to its subjectivity and the noisiness of OSN text. This work partially addresses these issues by shifting the focus towards users. We obtain a sample of Twitter’s retweet graph with 100,386 users and annotate 4,972 as hateful or normal, and also find 668 users suspended after 4 months. Our analysis shows that hateful/suspended users differ from normal/active ones in terms of their activity patterns, word usage and network structure. Exploiting Twitter’s network of connections, we find that a node embedding algorithm outperforms content-based approaches for detecting both hateful and suspended users. Overall, we present a user-centric view of hate speech, paving the way for better detection and understanding of this relevant and challenging issue. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Ribeiro, Manoel and Calais, Pedro and Santos, Yuri and Almeida, Virgílio and Meira Jr., Wagner}, year={2018}, month={Jun.} }

@article{Founta_Djouvas_Chatzakou_Leontiadis_Blackburn_Stringhini_Vakali_Sirivianos_Kourtellis_2018, title={Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior}, volume={12}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14991}, DOI={10.1609/icwsm.v12i1.14991}, abstractNote={ &lt;p&gt; In recent years online social networks have suffered an increase in sexism, racism, and other types of aggressive and cyberbullying behavior, often manifesting itself through offensive, abusive, or hateful language. Past scientific work focused on studying these forms of abusive activity in popular online social networks, such as Facebook and Twitter. Building on such work, we present an eight month study of the various forms of abusive behavior on Twitter, in a holistic fashion. Departing from past work, we examine a wide variety of labeling schemes, which cover different forms of abusive behavior. We propose an incremental and iterative methodology that leverages the power of crowdsourcing to annotate a large collection of tweets with a set of abuse-related labels. By applying our methodology and performing statistical analysis for label merging or elimination, we identify a reduced but robust set of labels to characterize abuse-related tweets. Finally, we offer a characterization of our annotated dataset of 80 thousand tweets, which we make publicly available for further scientific exploration. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Founta, Antigoni and Djouvas, Constantinos and Chatzakou, Despoina and Leontiadis, Ilias and Blackburn, Jeremy and Stringhini, Gianluca and Vakali, Athena and Sirivianos, Michael and Kourtellis, Nicolas}, year={2018}, month={Jun.} }

@misc{kennedy2020constructing,
      title={Constructing interval variables via faceted Rasch measurement and multitask deep learning: a hate speech application}, 
      author={Chris J. Kennedy and Geoff Bacon and Alexander Sahn and Claudia von Vacano},
      year={2020},
      eprint={2009.10277},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{doccano,
  title={{doccano}: Text Annotation Tool for Human},
  url={https://github.com/doccano/doccano},
  note={Software available from https://github.com/doccano/doccano},
  author={
    Hiroki Nakayama and
    Takahiro Kubo and
    Junya Kamura and
    Yasufumi Taniguchi and
    Xu Liang},
  year={2018},
}

@inproceedings{pei-etal-2022-potato,
    title = "{POTATO}: The Portable Text Annotation Tool",
    author = "Pei, Jiaxin  and
      Ananthasubramaniam, Aparna  and
      Wang, Xingyao  and
      Zhou, Naitian  and
      Dedeloudis, Apostolos  and
      Sargent, Jackson  and
      Jurgens, David",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-demos.33",
    pages = "327--337",
    abstract = "We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common ML/NLP tasks, active learning, keypress shortcuts, keyword highlights, tooltips); and 3) supports a high degree of customization (editable UI, inserting pre-screening questions, attention and qualification tests). Experiments over two annotation tasks suggest that POTATO improves labeling speed through its specially-designed productivity features, especially for long documents and complex tasks. POTATO is available at https://github.com/davidjurgens/potato and will continue to be updated.",
}

@article{Kim_Razi_Stringhini_Wisniewski_De_Choudhury_2021, title={You Don’t Know How I Feel: Insider-Outsider Perspective Gaps in Cyberbullying Risk Detection}, volume={15}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/18061}, DOI={10.1609/icwsm.v15i1.18061}, abstractNote={Cyberbullying is a prevalent concern within social computing research that has led to the development of several supervised machine learning (ML) algorithms for automated risk detection. A critical aspect of ML algorithm development is how to establish ground truth that is representative of the phenomenon of interest in the real world. Often, ground truth is determined by third-party annotators (i.e., “outsiders”) who are removed from the situational context of the interaction; therefore, they cannot fully understand the perspective of the individuals involved (i.e., “insiders”). To understand the extent of this problem, we compare “outsider” versus “insider” perspectives when annotating 2,000 posts from an online peer-support platform. We interpolate this analysis to a corpus containing over 2.3 million posts on bullying and related topics, and reveal significant gaps in ML models that use third-party annotators to detect bullying incidents. Our results indicate that models based on the insiders’ perspectives yield a significantly higher recall in identifying bullying posts and are able to capture a range of explicit and implicit references and linguistic framings, including person-specific impressions of the incidents. Our study highlights the importance of incorporating the victim’s point of view in establishing effective tools for cyberbullying risk detection. As such, we advocate for the adoption of human-centered and value-sensitive approaches for algorithm development that bridge insider-outsider perspective gaps in a way that empowers the most vulnerable.}, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Kim, Seunghyun and Razi, Afsaneh and Stringhini, Gianluca and Wisniewski, Pamela J. and De Choudhury, Munmun}, year={2021}, month={May}, pages={290-302} }

@article{vidgen2020directions,
  title={Directions in abusive language training data, a systematic review: Garbage in, garbage out},
  author={Vidgen, Bertie and Derczynski, Leon},
  journal={Plos one},
  volume={15},
  number={12},
  pages={e0243300},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA},
  url={https://doi.org/10.1007/s10579-020-09502-8}
}

@inproceedings{Mitchell_jury,
author = {Gordon, Mitchell L. and Lam, Michelle S. and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S.},
title = {Jury Learning: Integrating Dissenting Voices into Machine Learning Models},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502004},
doi = {10.1145/3491102.3502004},
abstract = {Whose labels should a machine learning (ML) algorithm learn to emulate? For ML tasks ranging from online comment toxicity to misinformation detection to medical diagnosis, different groups in society may have irreconcilable disagreements about ground truth labels. Supervised ML today resolves these label disagreements implicitly using majority vote, which overrides minority groups’ labels. We introduce jury learning, a supervised ML approach that resolves these disagreements explicitly through the metaphor of a jury: defining which people or groups, in what proportion, determine the classifier’s prediction. For example, a jury learning model for online toxicity might centrally feature women and Black jurors, who are commonly targets of online harassment. To enable jury learning, we contribute a deep learning architecture that models every annotator in a dataset, samples from annotators’ models to populate the jury, then runs inference to classify. Our architecture enables juries that dynamically adapt their composition, explore counterfactuals, and visualize dissent. A field evaluation finds that practitioners construct diverse juries that alter 14\% of classification outcomes.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {115},
numpages = {19},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{parmar-etal-2023-dont,
    title = "Don{'}t Blame the Annotator: Bias Already Starts in the Annotation Instructions",
    author = "Parmar, Mihir  and
      Mishra, Swaroop  and
      Geva, Mor  and
      Baral, Chitta",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.130",
    pages = "1779--1789",
    abstract = "In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator{'}s instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks.",
}

@inproceedings{sandri-etal-2023-dont,
    title = "Why Don{'}t You Do It Right? Analysing Annotators{'} Disagreement in Subjective Tasks",
    author = "Sandri, Marta  and
      Leonardelli, Elisa  and
      Tonelli, Sara  and
      Jezek, Elisabetta",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.178",
    pages = "2428--2441",
    abstract = "Annotators{'} disagreement in linguistic data has been recently the focus of multiple initiatives aimed at raising awareness on issues related to {`}majority voting{'} when aggregating diverging annotations. Disagreement can indeed reflect different aspects of linguistic annotation, from annotators{'} subjectivity to sloppiness or lack of enough context to interpret a text. In this work we first propose a taxonomy of possible reasons leading to annotators{'} disagreement in subjective tasks. Then, we manually label part of a Twitter dataset for offensive language detection in English following this taxonomy, identifying how the different categories are distributed. Finally we run a set of experiments aimed at assessing the impact of the different types of disagreement on classification performance. In particular, we investigate how accurately tweets belonging to different categories of disagreement can be classified as offensive or not, and how injecting data with different types of disagreement in the training set affects performance. We also perform offensive language detection as a multi-task framework, using disagreement classification as an auxiliary task.",
}

@article{Poletto2020ResourcesAB,
  title={Resources and benchmark corpora for hate speech detection: a systematic review},
  author={Fabio Poletto and Valerio Basile and Manuela Sanguinetti and Cristina Bosco and Viviana Patti},
  journal={Language Resources and Evaluation},
  year={2020},
  volume={55},
  pages={477 - 523},
  url="https://doi.org/10.1007/s10579-020-09502-8"
}

@article{10.1145/3232676,
author = {Fortuna, Paula and Nunes, S\'{e}rgio},
title = {A Survey on Automatic Detection of Hate Speech in Text},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3232676},
doi = {10.1145/3232676},
abstract = {The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {85},
numpages = {30},
keywords = {opinion mining, literature review, natural language processing, text mining, Hate speech}
}

@article{Kovcs2021ChallengesOH,
  title={Challenges of Hate Speech Detection in Social Media},
  author={Gy{\"o}rgy Kov{\'a}cs and Pedro Alonso and Rajkumar Saini},
  journal={SN Computer Science},
  year={2021},
  volume={2},
  url="https://doi.org/10.1007/s42979-021-00457-3"
}



































@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}
