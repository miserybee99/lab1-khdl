{
  "title": "Prompt-based Node Feature Extractor for Few-shot Learning on Text-Attributed Graphs",
  "authors": [
    "Xuanwen Huang",
    "Kaiqiao Han",
    "Dezheng Bao",
    "Quanjin Tao",
    "Zhisheng Zhang",
    "Yang Yang",
    "Qi Zhu"
  ],
  "submission_date": "2023-09-06T09:12:52+00:00",
  "revised_dates": [
    "2023-09-06T09:12:52+00:00"
  ],
  "publication_venue": null,
  "abstract": "Text-attributed Graphs (TAGs) are commonly found in the real world, such as\nsocial networks and citation networks, and consist of nodes represented by\ntextual descriptions. Currently, mainstream machine learning methods on TAGs\ninvolve a two-stage modeling approach: (1) unsupervised node feature extraction\nwith pre-trained language models (PLMs); and (2) supervised learning using\nGraph Neural Networks (GNNs). However, we observe that these representations,\nwhich have undergone large-scale pre-training, do not significantly improve\nperformance with a limited amount of training samples. The main issue is that\nexisting methods have not effectively integrated information from the graph and\ndownstream tasks simultaneously. In this paper, we propose a novel framework\ncalled G-Prompt, which combines a graph adapter and task-specific prompts to\nextract node features. First, G-Prompt introduces a learnable GNN layer\n(\\emph{i.e.,} adaptor) at the end of PLMs, which is fine-tuned to better\ncapture the masked tokens considering graph neighborhood information. After the\nadapter is trained, G-Prompt incorporates task-specific prompts to obtain\n\\emph{interpretable} node representations for the downstream task. Our\nexperiment results demonstrate that our proposed method outperforms current\nstate-of-the-art (SOTA) methods on few-shot node classification. More\nimportantly, in zero-shot settings, the G-Prompt embeddings can not only\nprovide better task interpretability than vanilla PLMs but also achieve\ncomparable performance with fully-supervised baselines.",
  "categories": [
    "cs.SI"
  ],
  "arxiv_id": "2309.02848"
}