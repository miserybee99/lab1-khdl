\section{Related work}
Modeling TAGs involves numerous works related to the NLP domain and Graph domain. Currently, pre-trained language models are the primary method for modeling the textual information in text-as-graphs \cite{mikolov2013efficient}. Presently, pre-trained language models are mainly based on transformer structures\cite{vaswani2017attention}, with a variety of pre-training methods, such as fill-mask \cite{devlin2018bert}, paragraph prediction\cite{devlin2018bert}, adversarial learning\cite{he2020deberta}, and auto-regressive learning\cite{radford2018improving}. Based on these tasks, many excellent pre-trained models have emerged, including BERT\cite{devlin2018bert}, RoBERTa\cite{liu2019roberta}, and GPT3\cite{brown2020language}. PLMs contain an amount of knowledge acquired through extensive pre-training data\cite{wei2022emergent}. Recently, using prompts has been proposed to better utilize the performance of pre-trained language models\cite{brown2020language}. Based on this finding, prompt learning\cite{liu2021gpt,gu2021ppt,lester2021power} has achieved impressive results in few-shot and zero-shot learning and has been widely applied by other domains.
Currently, the structural information in modeling TAGs is primarily modeled through GNNs, such as GraphSAGE\cite{hamilton2017inductive}, GAT\cite{velivckovic2017graph}, APPNP\cite{gasteiger2018predict,dong2021equivalence} and RevGAT\cite{li2021training}, and there are also many pre-training tasks on graphs such as GAE\cite{kipf2016variational}, GraphCL\cite{you2020graph} that can be extended to TAGs. Recently, many methods explore better utilizing the knowledge of PLMs to model TAGs more effectively, such as pre-training language models through graph-related tasks \cite{chien2021node} and finetuning PLMs together with GNNs via knowledge distillation\cite{mavromatis2023train} or variational inference \cite{zhao2022learning}.