{
  "title": "MAD: Modality Agnostic Distance Measure for Image Registration",
  "authors": [
    "Vasiliki Sideri-Lampretsa",
    "Veronika A. Zimmer",
    "Huaqi Qiu",
    "Georgios Kaissis",
    "Daniel Rueckert"
  ],
  "submission_date": "2023-09-06T09:59:58+00:00",
  "revised_dates": [
    "2023-09-06T09:59:58+00:00"
  ],
  "publication_venue": null,
  "abstract": "Multi-modal image registration is a crucial pre-processing step in many\nmedical applications. However, it is a challenging task due to the complex\nintensity relationships between different imaging modalities, which can result\nin large discrepancy in image appearance. The success of multi-modal image\nregistration, whether it is conventional or learning based, is predicated upon\nthe choice of an appropriate distance (or similarity) measure. Particularly,\ndeep learning registration algorithms lack in accuracy or even fail completely\nwhen attempting to register data from an \"unseen\" modality. In this work, we\npresent Modality Agnostic Distance (MAD), a deep image distance}] measure that\nutilises random convolutions to learn the inherent geometry of the images while\nbeing robust to large appearance changes. Random convolutions are\ngeometry-preserving modules which we use to simulate an infinite number of\nsynthetic modalities alleviating the need for aligned paired data during\ntraining. We can therefore train MAD on a mono-modal dataset and successfully\napply it to a multi-modal dataset. We demonstrate that not only can MAD\naffinely register multi-modal images successfully, but it has also a larger\ncapture range than traditional measures such as Mutual Information and\nNormalised Gradient Fields.",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "arxiv_id": "2309.02875"
}