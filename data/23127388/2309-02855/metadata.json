{
  "title": "Bandwidth-efficient Inference for Neural Image Compression",
  "authors": [
    "Shanzhi Yin",
    "Tongda Xu",
    "Yongsheng Liang",
    "Yuanyuan Wang",
    "Yanghao Li",
    "Yan Wang",
    "Jingjing Liu"
  ],
  "submission_date": "2023-09-06T09:31:37+00:00",
  "revised_dates": [
    "2023-09-07T03:25:02+00:00"
  ],
  "publication_venue": null,
  "abstract": "With neural networks growing deeper and feature maps growing larger, limited\ncommunication bandwidth with external memory (or DRAM) and power constraints\nbecome a bottleneck in implementing network inference on mobile and edge\ndevices. In this paper, we propose an end-to-end differentiable bandwidth\nefficient neural inference method with the activation compressed by neural data\ncompression method. Specifically, we propose a transform-quantization-entropy\ncoding pipeline for activation compression with symmetric exponential Golomb\ncoding and a data-dependent Gaussian entropy model for arithmetic coding.\nOptimized with existing model quantization methods, low-level task of image\ncompression can achieve up to 19x bandwidth reduction with 6.21x energy saving.",
  "categories": [
    "cs.CV",
    "eess.IV",
    "68U10(primary), 94A08 68T07(secondary)",
    "I.2.6; I.4.2"
  ],
  "arxiv_id": "2309.02855"
}