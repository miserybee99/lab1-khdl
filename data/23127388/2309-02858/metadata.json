{
  "title": "Generalised Mutual Information: a Framework for Discriminative Clustering",
  "authors": [
    "Louis Ohl",
    "Pierre-Alexandre Mattei",
    "Charles Bouveyron",
    "Warith Harchaoui",
    "Mickaël Leclercq",
    "Arnaud Droit",
    "Frédéric Precioso"
  ],
  "submission_date": "2023-09-06T09:39:33+00:00",
  "revised_dates": [
    "2023-09-06T09:39:33+00:00"
  ],
  "publication_venue": null,
  "abstract": "In the last decade, recent successes in deep clustering majorly involved the\nMutual Information (MI) as an unsupervised objective for training neural\nnetworks with increasing regularisations. While the quality of the\nregularisations have been largely discussed for improvements, little attention\nhas been dedicated to the relevance of MI as a clustering objective. In this\npaper, we first highlight how the maximisation of MI does not lead to\nsatisfying clusters. We identified the Kullback-Leibler divergence as the main\nreason of this behaviour. Hence, we generalise the mutual information by\nchanging its core distance, introducing the Generalised Mutual Information\n(GEMINI): a set of metrics for unsupervised neural network training. Unlike MI,\nsome GEMINIs do not require regularisations when training as they are\ngeometry-aware thanks to distances or kernels in the data space. Finally, we\nhighlight that GEMINIs can automatically select a relevant number of clusters,\na property that has been little studied in deep discriminative clustering\ncontext where the number of clusters is a priori unknown.",
  "categories": [
    "stat.ML",
    "cs.AI",
    "cs.IT",
    "cs.LG",
    "math.IT",
    "stat.ME",
    "62H30",
    "G.3"
  ],
  "arxiv_id": "2309.02858"
}