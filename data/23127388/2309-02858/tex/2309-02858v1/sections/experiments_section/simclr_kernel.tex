\input{figs_tex/cifar10/tab_simclr_kernel}

%To further illustrate the benefits of \gls*{mmd}-\glspl*{gemini} through the kernel, resp. Wasserstein-\gls*{gemini} through the distance, we continue the same experiment as in section~\ref{ssec:exp_mnist}. However, we focus this time on the CIFAR10 dataset in which a linear kernel is less efficient, i.e. trying to separate images using the euclidean distance between each. As improved distance, we chose a linear kernel between features extracted from a pretrained SIMCLR model~\cite{chen_simple_2020}, resp. norm 2 between these features for the Wasserstein-\gls*{gemini}. We provide results for two different architectures: LeNet-5 model and ResNet-18, the latter being a common choice of models in deep clustering literature~\cite{van_gansbeke_scan_2020,tao_clustering-friendly_2021}. We report the results in Table~\ref{tab:simclr_kernel} and provide the baseline of \gls*{mi}. We also write the baselines from related works when not using data augmentations to make a fair comparison. Indeed, models trained with \glspl*{gemini} do not use data augmentation: only the architecture and the kernel or distance function in the data space plays a role. We observe here that the choice of kernel or distance can be critical in \glspl*{gemini}. First of all, although the scores of the \gls*{mmd} and Wasserstein variants are greater than the \gls*{mi} when using the linear kernel or Euclidean norm, they remain close to the performances of K-Means on the images of CIFAR10. However, the introduction of SIMCLR features to provide a kernel function or distance improves the scores for the 3 \glspl*{gemini}. Interestingly, we observe that for the Resnet-18 the introduction of such features to guide the \glspl*{gemini} was not as successful as it was on the LeNet-5, even though it is a more complex model. By changing the kernel for a more insightful function via SIMCLR, we achieve good performances on the non-data-augmented CIFAR10.

%K-Means: 0.041  ARI train et 0.041 ARI test

%Rajouter insights, explication. + . Phrase bien sentie. Insister, CIFAR10 images raw, froms cracth sans augmentation

To further illustrate the benefits of the kernel or distance provided to GEMINIs, we continue the same experiment as in section~\ref{ssec:exp_mnist}. However, we focus this time on the CIFAR10 dataset. As improved distance, we chose a linear kernel and $\ell_2$ norm between features extracted from a pretrained SIMCLR model~\cite{chen_simple_2020}. We provide results for two different architectures: LeNet-5 and ResNet-18 both trained from scratch on raw images, the latter being a common choice of models in deep clustering literature~\cite{van_gansbeke_scan_2020,tao_clustering-friendly_2021}. We report the results in Table~\ref{tab:simclr_kernel} and provide the baseline of MI. We also write the baselines from related works when not using data augmentations to make a fair comparison. Indeed, models trained with GEMINIs do not use data augmentation: only the architecture and the kernel or distance function in the data space plays a role. We observe here that the choice of kernel or distance can be critical in GEMINIs. Indeed, while the Euclidean norm between images does not provide insights on how images of cats and dogs are far as shown by K-Means, features derived from SIMCLR carry much more insight on the proximity of images. This shows that the performances of GEMINIs depend on the quality of distance functions. Interestingly, we observe that for the Resnet-18 using SIMCLR features to guide GEMINIs was not as successful as it has been on the LeNet-5. We believe that the ability of this network to draw any decision boundary makes it equivalent to a categorical distribution model as in Sec.~\ref{ssec:exp_categorical}. Finally, to the best of our knowledge, we are the first to train from scratch a standard discriminative neural network on CIFAR raw images without using labels or direct data augmentations, while getting sensible clustering results. However, other recent methods achieve best scores using data augmentations which we do not~\cite{park_improving_2021}.