We focus in this experiment on the clustering of nodes in a graph representing email interaction between individuals in the Enron dataset (available at \url{https://www.cs.cmu.edu/~./enron/}. This famous company was filed for bankruptcy on the 2nd of December 2001 following an investigation for fraud by the Securities and Exchange Commision (SEC). Following Bouveyron et al.~\cite{bouveyron_2018_stochastic}, we focus on exchange of emails in the Enron corporation between the 1st of September 2021 and the 31st of December 2001, which corresponds to the peak period at which the company collapsed. We represent the dataset as a graph where each node corresponds to an employee and each edge represents the sending of at least one email. We choose to let the edges unweighted because the number of emails sent between two persons is not necessarily reflexive of how two persons can be close to each other. After filtering nodes that do not communicate between September and December, the graph comprises 141 nodes and 872 symmetric edges. In order to use the Wasserstein metric, we choose as a distance the all-pairs-shortest path as already used in Section~\ref{ssec:exp_moons} and Appendix~\ref{app:fw_distance}. Due to the necessity of using a \emph{symmetric} distance for the Wasserstein metric, we let the graph undirected. Thus, an edge between two persons represent the exchange of at least one email anyhow.

To cluster the nodes of the graph, we adopt a simple Graph Convolution Network (GCN)~\cite{kipf_2016_variational} inspired by Kipf et al.~\cite{kipf_2016_variational} and Liang et al.\cite{liang_2022_clustering} with a hidden size of dimension 64 and 10 clusters to find at best~\cite{bouveyron_2018_stochastic}. We vary the number of hidden layers from 1 to 3. All models were run 30 times. We perform clustering by maximising the OvO Wasserstein during 1000 epochs with a learning rate of $2\times10^{-3}$ for the Adam optimiser. Indeed, with a long training time, we allow the model to find more clusters despite a stabilised GEMINI. We also experimented using the MI with the same models and hyperparameters. We eventually select the final model using the highest GEMINI value for each different depth of GCN.

Similarly, we run 30 times as well two competitors with a number of clusters to find ranging from 2 to 10: the LPM model~\cite{hoff_2002_latent} and the Deep LPM model~\cite{liang_2022_clustering} which are generative methods based on the assumption of a latent position of the nodes in the graph determining their interaction. Their respective best models were selected according to the lowest bayesian information criterion and the highest evidence lower bound. For Deep LPM, we used the architecture proposed by the authors in~\cite{liang_2022_clustering} with a one-hidden-layer network.

\input{figs_tex/enron/fig_graph_cluster}
\input{figs_tex/enron/fig_interaction_matrices}
\input{figs_tex/enron/fig_pac_scores}
%\input{figs_tex/enron/fig_cmatrix_gemini_deep_lpm}
\input{figs_tex/enron/tab_enron_ari}
%\input{figs_tex/enron/fig_employee_status_distribution}

We start by showing the highest Wasserstein-GEMINI clustering in Figure~\ref{fig:projection_clustering} where the graph nodes are positioned according to the Fruchterman Reingold algorithm~\cite{fruchterman_graph_1991}.

When we look at the interaction matrices of the best models in Figure~\ref{fig:interaction_matrices}, we can observe that the GEMINI clustering applied to graph using the all pair shortest path distance yields dense clusters where nodes intensively connect with each others (Figure~\ref{sfig:enron_gemini_interaction}). The LPM model found fewer clusters that are as well densely connected while the Deep LPM found 6 clusters including one densely connected and another one which contains multiple nodes sparsely connected. Interestingly, GEMINI managed to isolate graph nodes that act as hub e.g. the 9th cluster in Fig.~\ref{sfig:enron_gemini_interaction} with star-shaped interactions while Deep LPM mixes hubs in its 3rd cluster in Figure~\ref{sfig:enron_dlpm_interaction}. A potential explanation for this difference is that LPM models seek to cluster nodes based on supposedly close latent representations whereas the GEMINI clustering uses the Wasserstein distance, hence taking into account the flow of information passing through each graph node. In Table~\ref{tab:ari_enron}, we compared the average ARI between our 30 GEMINI models with the best selected LPM and Deep LPM clustering. It appears indeed that GEMINI models have a stronger ARI with the LPM model than with the Deep LPM, as the former concentrated on dense clusters. Interestingly, we can notice in Table~\ref{tab:ari_enron} that the MI models with one hidden layer are able to produce also satisfying results with an ARI close to or better than the ARI between GEMINI and LPM methods. However, when the models turn deeper, the performances of MI drop whereas the ARI between the Wasserstein-GEMINI decreases more slowly.

To further compare the performances of these models, we chose to evaluate their proportions of ambiguous clusters (PAC score)~\cite{senbabaouglu_2014_critical}. The PAC score is evaluated using two steps: we first build a matrix where each entry $i,j$ contains the proportion of times two samples $i$ and $j$ where clustered together by some model, then arrange these proportions to build a cdf which highlights for given quantiles the proportions of paired samples that can be clustered together. The PAC score corresponds to the difference of cdf between two given quantiles, commonly 10\% and 90\%. Through Figure~\ref{fig:pac_scores}, we can see that the models trained with the Wasserstein GEMINI have a lower PAC score compared to MI-trained models which highlights a more consistent clustering assignment through all 30 runs. We can further observe that for both objective functions, a deeper model leads to higher PAC score which we can explain by the difficulty to repeat the same decision boundary with deep models. Eventually, we may conclude that while the MI may have competitive results with shallow models compared to GEMINI and Deep LPM, it is unable to repeat a consistent clustering as illustrated by a high PAC score.

%When looking at the distribution of employee status according to the clustering, we can observe in Figure~\ref{sfig:status_distribution_dlpm} that the cluster 8 is a dense connection from vice-president to employees. This same distribution can be observed in \gls*{gemini}'s 8th cluster. However, the clusters 1 and 3 in Figure~\ref{sfig:status_distribution_gemini} focus either on the distribution of directors, of the distribution of employees. Hence, they share and take some of the persons that were all part of the Deep LPM's 8th cluster. Some other clusters, such as the 4th (GEMINI) is representative of the board of presidents in the company, in possible communication with few employees. Overall, both clusterings of the two algorithms describe the hierarchical communication between persons in the Enron company.
