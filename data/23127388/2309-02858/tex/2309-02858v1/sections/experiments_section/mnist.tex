\input{figs_tex/mnist/tab_mnist}

We trained a neural network using either MI or GEMINIs. Following Hu et al.~\cite{hu_learning_2017}, we first tried with a MLP with one single hidden layer of dimension 1200. To further illustrate the robustness of the method and its adaptability to other architectures, we also experimented using a LeNet-5 architecture~\cite{lecun_gradient_1998} since it is adequate to the MNIST dataset. We report our results in Table~\ref{tab:mnist_experiment}. Since we are dealing with a clustering method, we may not know the number of clusters a priori in a dataset. The only thing that can be said about MNIST is that there are \emph{at least} 10 clusters, one per digit. Indeed, writings of digits could differ leading to more clusters than the number of classes. That is why we further tested the same method with 15 clusters to find in Table~\ref{tab:mnist_experiment}. We first see that the scores of the MMD and Wasserstein GEMINIs are greater than the MI. We also observe that no $f$-divergence-GEMINI always yield best ARIs. Nonetheless, we observe better performances in the case of the TV GEMINIs owing to its bounded gradient. This results in controlled stepsize when doing gradient descent contrarily to KL- and squared Hellinger-GEMINIs. Notice that the change of architecture from a MLP to a LeNet-5 unexpectedly halves the scores for the $f$-divergences. We believe this drop is due to the change of notion of neighborhood implied by the network architecture.