\input{figs_tex/gstm/tab_gstm_num_clusters}
\input{figs_tex/gstm/fig_entropy_maps}

%  When searching for 8 clusters, outliers are put in the extra clusters, allowing the models to leverage a good ARI nonetheless. Meanwhile, GEMINIs perform better ARIs for both 4 and 8 clusters, whatever the degree of freedom. Note that the \gls*{mmd} and Wasserstein-\gls*{gemini} present lower standard deviation for high scores compared to $f$-divergence GEMINIs. Moreover, as mentioned in section~\ref{ssec:local_maxima}, the usual MI is best maximised when its decision boundary presents little entropy $\mathcal{H}(\pyx)$. As neural networks can be overconfident~\cite{guo_calibration_2017}, MI is likely to yield overconfident clustering by minimizing the conditional entropy. We highlight such behaviour in Figure~\ref{fig:gstm_entropy_maps} where the Rényi entropy~\cite{renyi_measures_1961} associated to each sample in the MI (Figure~\ref{sfig:mi_entropy_map}) is much lower, if not 0, compared to \gls*{mmd}-\gls*{ovo} and Wasserstein-\gls*{ovo} (figures~\ref{sfig:mmd_entropy_map} and~\ref{sfig:wasserstein_entropy_map}).

To prove the strength of using neural networks for clustering trained with GEMINI, we introduced extreme samples in Gaussian mixtures by replacing a Gaussian distribution with a Student-t distribution for which the degree of freedom $\rho$ is small. We fixed $K=4$ clusters, 3 being drawn from multivariate Gaussian distributions and the last one from a multivariate Student-t distribution in 2 dimensions for visualisation purposes with 1 degree of freedom or 2. Thus, the Student-t distribution produces samples that can be perceived as outliers regarding a Gaussian mixture owing to its heavy tail.

Each cluster distribution is centered around a mean $\mu_i$ which proximity is controlled by a scalar $\alpha$. For simplicity, all covariance matrices are the identity scaled by a scalar $\sigma$. We define:
\begin{align*}
    \mu_1 &= [\alpha, \alpha],&\mu_2 &= [\alpha, -\alpha],\\
\mu_3 &= [-\alpha, \alpha],&\mu_4 &= [-\alpha, -\alpha].
\end{align*}

To sample from a multivariate Student-t distribution, we first draw samples $\vec{x}$ from a centered multivariate Gaussian distribution. We then sample another variable $u$ from a $\chi^2$-distribution using the degrees of freedom $\rho$ as parameter. Finally, $\vec{x}$ is multiplied by $\sqrt{\frac{\rho}{u}}$, yielding samples from the Student-t distribution.

We report the ARIs of Multi-Layered Perceptron (MLP) trained 20 times with GEMINIs in Table~\ref{tab:gstm_experiment_ari}. The presence of "outliers" leads K-Means and Gaussian Mixture models to fail at grasping the 4 distributions when tring to find 4 clusters. Meanwhile, GEMINIs perform better. Note that the MMD and Wasserstein-GEMINI present lower standard deviation for high scores compared to $f$-divergence GEMINIs. We attribute these performances to both the MLP that tries to find separating hyperplanes in the data space and the absence of hypotheses regarding the data. Moreover, as mentioned in section~\ref{ssec:local_maxima}, the usual MI is best maximised when its decision boundary presents little entropy $\mathcal{H}(\pyx)$. As neural networks can be overconfident~\cite{guo_calibration_2017}, MI is likely to yield overconfident clustering by minimizing the conditional entropy. We highlight such behaviour in Figure~\ref{fig:gstm_entropy_maps} where the Rényi entropy~\cite{renyi_measures_1961} associated to each sample in the MI (Figure~\ref{sfig:mi_entropy_map}) is much lower, if not 0, compared to OvO MMD and OvO Wasserstein (figures~\ref{sfig:mmd_entropy_map} and~\ref{sfig:wasserstein_entropy_map}). We conclude that Wasserstein- and MMD-GEMINIs train neural networks not to be overconfident, hence yielding more moderate distributions $\pyx$.