\input{figs_tex/moons/fig_moons_boundary}

We highlighted that MI can be maximised without requiring to find the suitable decision boundary. Here, we show how the provided distance to the OvO Wasserstein-GEMINI can leverage appropriate clustering when we have a good a priori on the data.

The cluster assignment is drawn according to a Bernoulli distribution with parameter 0.5. The points are then sampled using a uniformly distributed angle in $[0,\pi]$. Axis symmetry is applied depending on the clusters. All points are distributed at the same radius $\rho$ from some circle centre before adding some Gaussian noise. The samples then undergo some offset so that both moons are not linearly separable.

We generated a dataset consisting of two facing moons on which we trained a MLP using either the MI or the OvO Wasserstein-GEMINI. To construct a distance $c$ for the Wasserstein distance, we derived a distance from the Floyd-Warshall algorithm~\cite{warshall_theorem_1962,roy_transitivite_1959} on a sparse graph describing neighborhoods of samples. This distance describes how many neighbors are in between two samples, further details are provided in appendix~\ref{app:fw_distance}. We report the different decision boundaries in Figure~\ref{fig:moons_decision_boundary}. We observe that the insight on the neighborhood provided by our distance $c$ helped the MLP to converge to the correct solution with an appropriate decision boundary unlike the MI.  Note that the usual Euclidean distance in the Wasserstein metric would have converged to a solution similar to the MI. Indeed for 2 clusters, the optimal transport plan has a larger value, using a distribution similar to Figure~\ref{sfig:moons_mi_2clusters}, than in Figure~\ref{sfig:moons_gemini_2clusters}. This toy example shows how an insightful metric provided to the Wasserstein distance in GEMINIs can lead to correct decision boundaries while only designing a discriminative distribution $\pyx$ and a distance $c$.

In addition, we highlight an interesting behaviour of all GEMINIs. During optimisation, it is possible that the model converges to using fewer clusters than the number to find. For example in Figure~\ref{fig:moons_decision_boundary}, for 5 clusters, the model can converge to 4 balanced clusters and 1 empty cluster (Figure~\ref{sfig:moons_gemini_5clusters}) unlike MI that produced 5 misplaced clusters (Figure~\ref{sfig:moons_mi_5clusters}). Indeed, the entropy on the cluster proportion in the MI forces to use the maximum number of clusters.