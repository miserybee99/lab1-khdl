We consider in this section a dataset consisting in $N$ unlabelled samples $\mathcal{D}=\{\vec{x}_i\}_{i=1}^N$. We distinguish two major use cases of the mutual information: one where we measure the dependence between two continuous variables, as is the case in representation learning, and one where the random variable is discrete. In representation learning, the goal is to construct a continuous representation $\vec{z}$ extracted from the data $\vec{x}$ using a learnable distribution of parameters $\theta$. In clustering, samples $\vec{x}$ are assigned to the discrete variable $y$ through another learnable distribution.


%Each sample is associated to a cluster $y$ using a learnable distribution $\pyx$ of parameters $\theta$. The variable $y$ is discrete. For continuous case, e.g. representation learning, we write representations $\vec{z} \sim p_\theta(\vec{z}|\vec{x})$.
%\begin{equation*}\label{eq:mi_representation_learning}
%    \mathcal{I}(\vec{x},\vec{z})=\int\int  \log{\frac{p(\vec{x},\vec{z})}{p(\vec{x})p(\vec{z})}}d\vec{x}d\vec{z}\quad,
%\end{equation*}
%\begin{equation*}\label{eq:mi_clustering}
%    \mathcal{I}(\vec{x},y) = \sum_{i=1}^K \int p(y_i,\vec{x})\log{\frac{p(y_i,\vec{x})}{p(y_i)p(\vec{x})}}d\vec{x}
%\end{equation*}

\subsection{Representation learning}
\label{ssec:mi_representation}

Representation learning consists in finding high-level features $\vec{z}_i$ extracted from the data $\vec{x}_i$ in order to perform a \emph{downstream task}, e.g. clustering or classification. MI between $\vec{x}$ and $\vec{z}$ is a common choice for learning features\cite{hjelm_learning_2019}. However, estimating correctly MI between two random variables in continuous domains is often intractable when $p(\vec{x}|\vec{z})$ or $p(\vec{z}|\vec{x})$ is unknown, thus lower bounds are preferred, e.g. variational estimators such as MINE~\cite{belghazi_mine_2018}, $\I_\text{NCE}$\cite{van_den_oord_representation_2018},$\I_\text{BA}$\cite{barber_im_2003},$\I_\text{DV}$\cite{donsker_asymptotic_1983}. These bounds require more parameters: additional discriminator networks are also trained to make a distinction between data and features issued from the joint distribution or product of marginals. Most of these lower bounds often present high-variance such as $\I_\text{NJW}$\cite{nguyen_estimating_2010}. Poole et al.~\cite{poole_variational_2019} eventually bridged the gap between this high-variance estimators and the high-bias low-variance $\I_\text{NCE}$ by introducing $\I_\alpha$, an interpolated lower bound. Another common choice of loss function to train features are contrastive losses such as NT-XENT~\cite{chen_simple_2020} where the similarity between the features $\vec{z}_i$ from data $\vec{x}_i$ is maximised with the features $\tilde{\vec{z}}$ from a data-augmented $\tilde{\vec{x}}_i$ against any other features $\vec{z}_j$. Recently, Do et al.~\cite{do_clustering_2021} achieved excellent performances in single-stage methods by highlighting the link between the $\I_\text{NCE}$ estimator~\cite{van_den_oord_representation_2018} and contrastive learning losses. Representation learning therefore comes at the cost of a complex lower bound estimator on MI, which often requires data augmentation.
%While this often improves the performance of deep learning models, it can be a pitfall as it may lead to data overfitting in classification. Overall, data augmentation needs to be carefully designed when dealing with data where augmentations have a chance to change the class of the samples, such as medical data. {\bf TODO ref manquante} Thus, clustering tasks are even riskier when clusters are not known a priori and data augmentations can change the assignments.
Moreover, it was noticed that the MI is hardly predictive of downstream tasks~\cite{tschannen_mutual_2019} when the variable $y$ is continuous, i.e. a high value of MI does not clarify whether the discovered representations are insightful with regards to the target of the downstream task.

\subsection{Discriminative clustering}
\label{sssec:mi_clustering}

The MI has been first used as an objective for learning discriminative clustering models~\cite{bridle_unsupervised_1992}. Associated architectures went from simple logistic regression~\cite{krause_discriminative_2010} to deeper architectures~\cite{hu_learning_2017,ji_invariant_2019}. Beyond architecture improvement, the MI maximisation was also carried with several regularisations. These regularisations include penalty terms such as weight decay~\cite{krause_discriminative_2010} or Virtual Adversarial Training (VAT)~\cite{hu_learning_2017,miyato_virtual_2018}). Data augmentation was further used to provide invariances in clustering, as well as specific architecture designs like auxiliary clustering heads~\cite{ji_invariant_2019}. Rewriting the MI in terms of entropies:
\begin{equation}
\label{eq:mi_kl_entropies}
\I (\vec{x};y) = \mathcal{H}(y) - \mathcal{H}(y|\vec{x})
\end{equation}
highlights a requirement for balanced clusters, through the cluster entropy term $\mathcal{H}(y)$. Indeed, a uniform distribution maximises the entropy. This hints that an unregularised discrete mutual information for clustering can possibly produce uniformly distributed clusters among samples, regardless of how close they could be. We highlight this claim in section~\ref{ssec:local_maxima}. As an example of regularisation impact: maximising the MI with $\ell_2$ constraint can be equivalent to a soft and regularised K-Means in a feature space~\cite{jabi_deep_2019}. In clustering, the number of clusters to find is usually not known in advance. Therefore, an interesting clustering algorithm should be able to find a relevant number of clusters, i.e. perform model selection. However, model selection for parametric deep clustering models is expensive~\cite{ronen_deepdpm_2022}. Cluster selection through MI maximisation has been little studied in related works, since experiments usually tasked models to find the (supervised) classes of datasets. Furthermore, the literature diverged towards deep learning methods focusing mainly on images, yet rarely on other type of data such as tabular data~\cite{min_survey_2018}.

\subsection{Maximising the MI can lead to bad decision boundaries}
\label{ssec:local_maxima}

Maximising the MI directly can be a poor objective: a high MI value is not necessarily predictive of the quality of the features regarding downstream tasks~\cite{tschannen_mutual_2019} when $y$ is continuous. We support a similar argument for the case where the data $\vec{x}$ is a continuous random variable and the cluster assignment $y$ a categorical variable. Indeed, the MI can be maximised by setting appropriately a sharp decision boundary which partitions evenly the data, i.e. when the distribution $\pyx$ converges to a Dirac distribution. This reasoning can be seen in the entropy-based formulation of the MI (Eq.~\ref{eq:mi_kl_entropies}): any sharp decision boundary minimises the negative conditional entropy, while ensuring balanced clusters maximises the entropy of cluster proportions. Consider for example Figure~\ref{fig:example_good_odd_mi}, where a mixture of Gaussian distributions with equal variances is separated by a sharp decision boundary. We highlight that both models will have the same mutual information on condition that the misplaced decision boundary of Figure~\ref{sfig:odd_decision_boundary} splits evenly the dataset (see Appendix~\ref{app:mi_convergence}).

\input{figs_tex/mi_clustering_objective/fig_example_bad_mi}

Globally, MI misses the idea in clustering that any two points close to one another may be in the same cluster according to some chosen distance. Hence regularisations are required to ensure this constraint. An early sketch of these insights was mentioned by Bridle et al.~\cite{bridle_unsupervised_1992} or Corduneanu et al.~\cite{corduneanu_information_2012}. The non-predictiveness of MI was as well recently empirically highlighted by Zhang and Boykov~\cite{zhang_revisiting_2023} in discrete cases. This can be also be seen as a problem of invariance of the conditional distribution in low density areas~\cite{corduneanu_information_2012}.