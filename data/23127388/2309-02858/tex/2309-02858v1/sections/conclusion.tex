We highlighted that the choice of distance at the core of MI can alter the performances of deep learning models when used as an objective for a deep discriminative clustering. We first showed that MI maximisation does not necessarily reflect the best decision boundary in clustering when the clustering model converges to a Dirac distribution. We introduced the GEMINI, a method which only needs the specification of a neural network and a kernel or distance in the data space. Moreover, we showed how the notion of neighborhood built by the neural network can affect the clustering, especially for MI. To the best of our knowledge, this is the first method that trains single-stage neural networks from scratch using neither data augmentations nor regularisations, yet achieving good clustering performances. We emphasised that GEMINIs are only searching for a maximum number of clusters: after convergence some may be empty. However, we do not have insights to explain this convergence which is part of future work. Finally, we introduced several versions of GEMINIs and would encourage the OvA MMD or OvA Wasserstein as a default choice, since it proves to both incorporate knowledge from the data using a kernel or distance while remaining the less complex than OvO MMD and OvO Wasserstein in time and memory. OvO versions could be privileged for fine-tuning steps. As follow-up, we focused on the introduction of sparsity in GEMINI clustering for feature selection~\cite{ohl_sparse_2023}. Future works could focus on the joint learning of distances or kernels~\cite{wu_deep_2020} while maximising the the GEMINI to get both meaningful clustering and metrics in the data space. We will also investigate why sometimes after convergence some clusters end up empty.