\newcommand{\nmuA}{\mathcal{N}(x|\mu_0,\sigma^2)}
\newcommand{\nmuB}{\mathcal{N}(x|\mu_1,\sigma^2)}

\label{app:mi_boundaries}
\subsection{Models definition}
Let us consider a mixture of two Gaussian distributions, both with different means $\mu_0$ and $\mu_1$, s.t. $\mu_0<\mu_1$ and of same standard deviation $\sigma$:

\begin{equation*}
p(x|y=0) = \nmuA, p(x|y=1) = \nmuB ,
\end{equation*}

where $y$ is the cluster assignment. We take balanced clusters proportions, i.e. $p(y=0)=p(y=1)=\frac{1}{2}$. This first model is the basis that generated the complete dataset $p(x)$. When performing clustering with our discriminative model, we are not aware of the distribution. Consequently: we create other models. We want to compute the difference of mutual information between two decision boundaries that discriminative models $p_\theta(y|x)$ may yield.

We define two decision boundaries: one which splits evenly the data space called $p_A$ and another which splits it on a closed set $p_B$:

\begin{equation}\label{eq:p_a}
p_A(y=1|x) = \left\{ \begin{array}{c c}
1-\epsilon&x>\frac{\mu_1-\mu_0}{2}\\
\epsilon&\text{otherwise}
\end{array}\right. ,
\end{equation}

\begin{equation}\label{eq:p_b}
p_B(y=1|x) = \left\{ \begin{array}{c c}
1-\epsilon&x \in [\mu_0, \mu_1]\\
\epsilon&\text{otherwise}
\end{array}\right. .
\end{equation}

Our goal is to show that both models $p_A$ and $p_B$ will converge to the same value of mutual information as $\epsilon$ converges to 0.

\subsection{Computing cluster proportions}
\subsubsection{Cluster proportion of the correct decision boundary}

To compute the cluster proportions, we estimate with samples $x$ from the distribution $p_\text{data}(x)$. Since we are aware for this demonstration of the true nature of the data distribution, we can use $p(x)$ for sampling. Consequently, we can compute the two marginals:

\begin{align*}
p_A(y=1) &= \int_\mathcal{X} p(x) p_A(y=1|x)dx ,\\
&= \int_{-\infty}^{\frac{\mu_1-\mu_0}{2}}p(x)\epsilon dx + \int_{\frac{\mu_1-\mu_0}{2}}^{+\infty}p(x)(1-\epsilon)dx ,\\
&=\epsilon \left(\int_{-\infty}^{\frac{\mu_1-\mu_0}{2}}p(x)dx \right) + (1-\epsilon) \left( \int_{\frac{\mu_1-\mu_0}{2}}^{+\infty}p(x)dx\right) ,\\
&= \frac{1}{2} .
\end{align*}

\subsubsection{Cluster proportion of the misplaced decision boundary}
For the misplaced decision boundary, the marginal is different:

\begin{equation*}\begin{split}
p_B(y=1) &= \int_\mathcal{X} p(x) p_B(y=1|x)dx ,\\
&= \epsilon\left(\int_{-\infty}^{\mu_0}p(x)dx + \int_{\mu_1}^{+\infty}p(x)dx\right) + (1-\epsilon) \int_{\mu_0}^{\mu_1}p(x)dx ,\\
&=\epsilon \left(1-\int_{\mu_0}^{\mu_1}p(x)dx\right) + (1-\epsilon)\int_{\mu_0}^{\mu_1}p(x)dx .
\end{split}\end{equation*}

Here, we simply introduce a new variable named $\beta$ that will be a shortcut for noting the proportion of data between $\mu_0$ and $\mu_1$:

\begin{equation*}%\label{eq:beta_definition}
\beta = \int_{\mu_0}^{\mu_1}p(x)dx .
\end{equation*}

And so can we simply write the cluster proportion of decision boundary model B as:

\begin{equation*}\label{eq:py_b}
p_B(y=1) = \epsilon (1-\beta) + (1-\epsilon)\beta ,
\end{equation*}

Leading to the summary of proportions in Table~\ref{tab:proportions}. For convenience, we will write the proportions of model B using the shortcuts:

\begin{equation*}\label{eq:pi_b}
\pi_B = p_B(y=1) = \epsilon + \beta(1-2\epsilon) ,
\end{equation*}
\begin{equation*}%\label{eq:pibar_b}
\bar{\pi}_B = p_B(y=0) = 1-\epsilon - \beta(1-2\epsilon) .
\end{equation*}

\input{figs_tex/appendix/tab_clustering_proportions}

\subsection{Computing the KL divergences}
\subsubsection{Correct decision boundary}

We first start by computing the Kullback-Leibler divergence for some arbitrary value of $x\in\mathbb{R}$:

\begin{equation*}
D_\text{KL}(p_A(y|x)||p_A(y))= \sum_{i=0}^1 p_A(y=i|x) \log{\frac{p_A(y=i|x)}{p_A(y=i)}} .
\end{equation*}

We now need to detail the specific cases, for the value of $p(y=i|x)$ is dependent on $x$. We start $\forall x <\frac{\mu_1-\mu_0}{2}$:

\begin{align*}
D_\text{KL}(p_A(y|x)||p_A(y))&= p_A(y=0|x)\log{\frac{p_A(y=0|x)}{\frac{1}{2}}} + p_A(y=1|x)\log{\frac{p_A(y=1|x)}{\frac{1}{2}}} ,\\
&=(1-\epsilon)\log{2(1-\epsilon)} + \epsilon \log{2\epsilon} .
\end{align*}

The opposite case, $\forall x\geq \frac{\mu_1-\mu_0}{2}$ yields:
\begin{align*}
D_\text{KL}(p_A(y|x)||p_A(y))&= p_A(y=0|x)\log{\frac{p_A(y=0|x)}{\frac{1}{2}}} + p_A(y=1|x)\log{\frac{p_A(y=1|x)}{\frac{1}{2}}} ,\\
&=\epsilon\log{2\epsilon} + (1-\epsilon) \log{2(1-\epsilon)} .
\end{align*}

Since both cases are equal, we can write down:

\begin{equation}\label{eq:correct_kl_div}
D_\text{KL}(p_A(y|x)||p_A(y)) = \epsilon\log{2\epsilon} + (1-\epsilon)\log{2(1-\epsilon)} ,\forall x\in\mathbb{R} .
\end{equation}

\subsubsection{Misplaced boundary}

We proceed to the same detailing of the Kullback-Leibler divergence computation for the misplaced decision boundary. We start with the set $x\in [\mu_0,\mu_1]$:

\begin{align*}
D_\text{KL}(p_B(y|x)||p_B(y))&= p_B(y=0|x) \log{\frac{p_B(y=0|x)}{p_B(y=0)}} + p_B(y=1|x)\log{\frac{p_B(y=1|x)}{p_B(y=1)}} ,\\
&=\epsilon \log{\frac{\epsilon}{\bar{\pi}_B}} + (1-\epsilon) \log{\frac{1-\epsilon}{\pi_B}} .
\end{align*}

When $x$ is out of this set, the divergence becomes:

\begin{align*}
D_\text{KL}(p_B(y|x)||p_B(y))&=p_B(y=0|x) \log{\frac{p_B(y=0|x)}{p_B(y=0)}} + p_B(y=1|x)\log{\frac{p_B(y=1|x)}{p_B(y=1)}} ,\\
&=(1-\epsilon)\log{\frac{1-\epsilon}{\bar{\pi}_B}} + \epsilon \log{\frac{\epsilon}{\pi_B}} .
\end{align*}

To fuse the two results, we will write the KL divergence as such:

\begin{equation}\label{eq:odd_kl_div}
D_\text{KL}(p_B(y|x)||p_B(y)) = \epsilon\log{\epsilon}+ (1-\epsilon)\log(1-\epsilon) - C(x) ,\forall x\in\mathbb{R} ,
\end{equation}

where $C(x)$ is a constant term depending on $x$ defined by:

\begin{equation*}\label{eq:odd_constant}
C(x)=\left\{\begin{array}{c c}
\epsilon\log{\bar{\pi}_B} + (1-\epsilon)\log{\pi_B}&x\in[\mu_0,\mu_1]\\
\epsilon\log{\pi_B} + (1-\epsilon)\log{\bar{\pi}_B}&x\in\mathbb{R}\setminus[\mu_0,\mu_1]\\
\end{array}  .\right.
\end{equation*}

For simplicity of later writings, we will shorten the notations by:

\begin{equation*}
C(x)=\left\{ \begin{array}{c c}
\alpha_1&x\in[\mu_0,\mu_1]\\
\alpha_0&x\in\mathbb{R}\setminus[\mu_0,\mu_1]
\end{array}\right. .
\end{equation*}

\subsection{Evaluating the mutual information}

\subsubsection{Correct decision boundary}

We inject the value of the Kullback-Leibler divergence from Eq.~(\ref{eq:correct_kl_div}) inside an expectation performed over the data distribution $p_\text{data}(x)$:

\begin{align}
\mathcal{I}_A(x;y) &= \mathbb{E}_{x\sim p_\text{data}(x)}\left[ D_\text{KL}(p_A(y|x)||p_A(y))\right] ,\\
&=\int_\mathcal{X}p(x) \left(\epsilon \log(2\epsilon) + (1-\epsilon)\log(2(1-\epsilon)) \right)dx ,\\
&=\epsilon \log(2\epsilon) + (1-\epsilon)\log(2(1-\epsilon)) .\label{eq:mi_a}
\end{align}

Since the KL divergence was independent of $x$, we could leave the constant outside of the integral which is equal to 1.

We can assess the coherence of Eq.~(\ref{eq:mi_a}) since its limit as $\epsilon$ approaches 0 is $\log 2$. In terms of bits, this is the same as saying that the information on $X$ directly gives us information on the $Y$ of the cluster.

\subsubsection{Odd decision boundary}

We inject the value of the KL divergence from Eq.~(\ref{eq:odd_kl_div}) inside the expectation of the mutual information:

\begin{align*}
\mathcal{I}_B(x;y) &= \mathbb{E}_{x\sim p_\text{data}(x)}\left[ D_\text{KL}(p_B(y|x)||p_B(y))\right] ,\\
&=\int_{\mathcal{X}}p(x)\left(\epsilon\log\epsilon +(1-\epsilon)\log(1-\epsilon) -C(x)\right) ,dx\\
&=\epsilon\log\epsilon +(1-\epsilon)\log(1-\epsilon) - \int_{\mathcal{X}}p(x)C(x)dx .
\end{align*}

The first terms are constant with respect to $x$ and the integral of $p(x)$ over $\mathcal{X}$ adds up to 1. We finally need to detail the expectation of the constant $C(x)$ from Eq.~(\ref{eq:odd_constant}):

\begin{align*}
\mathbb{E}_x[C(x)] &= \int_{-\infty}^{\mu_0}C(x)p(x)dx + \int_{\mu_0}^{\mu_1}C(x)p(x)dx + \int_{\mu_1}^{+\infty}C(x)p(x)dx ,\\
&= \alpha_0\left(\int_{-\infty}^{\mu_0}p(x)dx + \int_{\mu_1}^{+\infty}p(x)dx \right) + \alpha_1\int_{\mu_0}^{\mu_1}p(x)dx ,\\
&=\alpha_0(1-\beta) + \alpha_1\beta .
\end{align*}

This can be further improved by unfolding the description of $\alpha_0$ and $\alpha_1$ from Eq.~(\ref{eq:odd_constant}):

\begin{align*}
\alpha_0(1-\beta)+\beta\alpha_1&= \alpha_0 +\beta(\alpha_1-\alpha_0),\\
&=\epsilon\log{\pi_B}+(1-\epsilon)\log{\bar{\pi}_B} + \beta\left[\epsilon\log{\bar{\pi}_B}+(1-\epsilon)\log{\pi_B}\right.\\&\quad\left.-\epsilon\log{\pi_B}- (1-\epsilon)\log{\bar{\pi}_B}\right],\\
&= \left[1-\epsilon+\beta\epsilon-\beta+\beta\epsilon\right]\log{\bar{\pi}_B} + \left[\epsilon+\beta-\beta\epsilon-\beta\epsilon\right]\log{\pi_B},\\
&=\log{\bar{\pi}_B} + \left[2\beta\epsilon-\beta-\epsilon\right]\log{\frac{\bar{\pi}_B}{\pi_B}} .
\end{align*}

We can finally write down the mutual information for the model B:

\begin{equation*}\label{eq:mi_b}
\mathcal{I}_B(x;y) = \epsilon\log{\epsilon} +(1-\epsilon)\log(1-\epsilon) -\log{\bar{\pi}_B} - \left[2\beta\epsilon-\beta-\epsilon\right]\log{\frac{\bar{\pi}_B}{\pi_B}} .
\end{equation*}

\subsection{Differences of mutual information}

Now that we have the exact value of both mutual informations, we can compute their differences:

\begin{align*}
\Delta_\mathcal{I} &= \mathcal{I}_A(x;y) - \mathcal{I}_B(x;y),\\
&= \epsilon\log(2\epsilon) + (1-\epsilon)\log(2(1-\epsilon)) - \epsilon\log{\epsilon} -(1-\epsilon)\log(1-\epsilon)\\&\quad+\log{\bar{\pi}_B} + \left[2\beta\epsilon-\beta-\epsilon\right]\log{\frac{\bar{\pi}_B}{\pi_B}} ,\\
&=\epsilon\log{2}+(1-\epsilon)\log2 +\log{\bar{\pi}_B} + \left[2\beta\epsilon-\beta-\epsilon\right]\log{\frac{\bar{\pi}_B}{\pi_B}} .
\end{align*}

We then deduce how the difference of mutual information evolves as the decision boundary becomes sharper, i.e. when $\epsilon$ approaches 0:

\begin{equation*}
\lim_{\epsilon\rightarrow 0}\Delta_\mathcal{I} = \log{2} + \log{\bar{\pi}_B} -\beta\log{\frac{\bar{\pi}_B}{\pi_B}} .
\end{equation*}

However, the cluster proportions by B $\pi_B$ also take a different value as $\epsilon$ approaches 0. Recalling Eq.~(\ref{eq:p_b}):

\begin{equation*}
\lim_{\epsilon\rightarrow 0} p_B(y=1) = \beta , \lim_{\epsilon\rightarrow 0} p_B(y=0) = 1-\beta .
\end{equation*}

And finally can we write that:

\begin{equation*}\begin{split}
\lim_{\epsilon\rightarrow 0} \Delta_\mathcal{I} &= \log{2} + \log(1-\beta) - \beta \log\frac{1-\beta}{\beta} ,\\
&=\log{2} + (1-\beta)\log(1-\beta) + \beta \log{\beta} ,\\
&=\log{2} -\mathcal{H}(\beta) .
\end{split}\end{equation*}

\input{figs_tex/appendix/fig_convergence_differences}

To conclude, as the decision boundaries turn sharper, i.e. when $\epsilon$ approaches 0, the difference of mutual information between the two models is controlled by the entropy of proportion of data $\beta$ between the two means $\mu_0$ and $\mu_1$. We know that for binary entropies, the optimum is reached for $\beta=0.5$. In other words having $\mu_0$ and $\mu_1$ distant enough to ensure balance of proportions between the two clusters of model B leads to a difference of mutual information equal to 0. We experimentally highlight this convergence in Figure~\ref{fig:mi_convergence_differences} where we compute the mutual information of models A and B as the distance between the two means $\mu_0$ and $\mu_1$ increases in the Gaussian distribution mixture.