The complexity of GEMINI increases with the distances previously mentionned depending on the number of clusters $K$ and the number of samples per batch $N$. It ranges from $\mathcal{O}(NK)$ for the usual MI to $\mathcal{O}(K^2N^3\log{N})$ for the OvO Wasserstein-GEMINI. As an example, we show in Figure~\ref{fig:time_performances} the average time of GEMINI as the number of tasked clusters increases for both 10 samples per batch (Figure~\ref{sfig:time_performances_batch10}) and 500 samples (Figure~\ref{sfig:time_performances_batch500}). The batches consists in randomly generated prediction and distances or kernel between randomly generated data.

\input{figs_tex/appendix/fig_time_performances}

The OvO Wasserstein is the most complex, and so its usage should remain for 10 clusters or less overall. The second most time-consuming loss is the OvA Wasserstein, however its tendency in optimisation to only find 2 clusters makes it a inappropriate. The main difference also to notice between the following MMD is regarding their memory complexity. The OvA MMD requires only $\mathcal{O}(KN^2)$ while the OvO MMD requires $\mathcal{O}(K^2N^2)$. This memory complexity should be the major guide to choosing one MMD-GEMINI or the other. Thus, the minimal time-consuming and resource-demanding GEMINI is the OvA MMD if we consider GEMINIs that incorporates knowledge of data through kernels and distances. Other versions involving $f$-divergences have in fact the same complexity as MI in our implementations, apart from the OvO TV which reaches $\mathcal{O}(K^2N)$ in our implementation.