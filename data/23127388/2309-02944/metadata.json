{
  "title": "The case for and against fixed step-size: Stochastic approximation algorithms in optimization and machine learning",
  "authors": [
    "Caio Kalil Lauand",
    "Ioannis Kontoyiannis",
    "Sean Meyn"
  ],
  "submission_date": "2023-09-06T12:22:32+00:00",
  "revised_dates": [
    "2025-09-03T02:47:11+00:00"
  ],
  "publication_venue": null,
  "abstract": "Theory and application of stochastic approximation (SA) have become\nincreasingly relevant due in part to applications in optimization and\nreinforcement learning. This paper takes a new look at SA with constant\nstep-size $\\alpha>0$, defined by the recursion, $$\\theta_{n+1} = \\theta_{n}+\n\\alpha f(\\theta_n,\\Phi_{n+1})$$ in which $\\theta_n\\in\\mathbb{R}^d$ and\n$\\{\\Phi_{n}\\}$ is a Markov chain. The goal is to approximately solve root\nfinding problem $\\bar{f}(\\theta^*)=0$, where\n$\\bar{f}(\\theta)=\\mathbb{E}[f(\\theta,\\Phi)]$ and $\\Phi$ has the steady-state\ndistribution of $\\{\\Phi_{n}\\}$.\n  The following conclusions are obtained under an ergodicity assumption on the\nMarkov chain, compatible assumptions on $f$, and for $\\alpha>0$ sufficiently\nsmall:\n  $\\textbf{1.}$ The pair process $\\{(\\theta_n,\\Phi_n)\\}$ is geometrically\nergodic in a topological sense.\n  $\\textbf{2.}$ For every $1\\le p\\le 4$, there is a constant $b_p$ such that\n$\\limsup_{n\\to\\infty}\\mathbb{E}[\\|\\theta_n-\\theta^*\\|^p]\\le b_p \\alpha^{p/2}$\nfor each initial condition.\n  $\\textbf{3.}$ The Polyak-Ruppert-style averaged estimates\n$\\theta^{\\text{PR}}_n=n^{-1}\\sum_{k=1}^{n}\\theta_k$ converge to a limit\n$\\theta^{\\text{PR}}_\\infty$ almost surely and in mean square, which satisfies\n$\\theta^{\\text{PR}}_\\infty=\\theta^*+\\alpha \\bar{\\Upsilon}^*+O(\\alpha^2)$ for an\nidentified non-random $\\bar{\\Upsilon}^*\\in\\mathbb{R}^d$. Moreover, the\ncovariance is approximately optimal: The limiting covariance matrix of\n$\\theta{\\text PR}_n$ is approximately minimal in a matricial sense.\n  The two main take-aways for practitioners are application-dependent. It is\nargued that, in applications to optimization, constant gain algorithms may be\npreferable even when the objective has multiple local minima; while a vanishing\ngain algorithm is preferable in applications to reinforcement learning due to\nthe presence of bias.",
  "categories": [
    "math.ST",
    "stat.ML",
    "stat.TH",
    "62L20, 68T05"
  ],
  "arxiv_id": "2309.02944"
}