\section{Discussion}

Real-time adaptive algorithms can respond quickly to optimize experiences for individual students, and their expressivity for personalizing experiences increases with each additional type of student information they are given. In this paper, we have shown that this expressivity may be worthwhile under two circumstances: when it is \textit{necessary}  for expressing the best policy to improve student outcomes, as shown by the simulation results in Section~\ref{section:basicSimResults}, and in certain cases when including those characteristics magnifies differences between conditions for subgroups of students, as shown by the results in Section~\ref{section:assistmentsResults}. For the scenarios we considered, in which there are only two interventions and binary rewards, this magnification in differences can only occur on average if the optimal policy varies across students (or if there is a ceiling or floor effect for rewards for some subgroup of students), but it is possible this situation could occur in other circumstances with more complex environments.

Our results also show the potential ethical concerns inherent in choosing whether to include student characteristics when these characteristics are not uniformly distributed. If these characteristics matter, then failing to include them may lead to a learned policy that systematically optimizes for the majority but not for a minority group. However, when this expressivity is not necessary, it increases variability across students and also increases the time for identifying the correct policy, thus significantly decreasing the number of students assigned the best version of the technology and slightly decreasing their average outcomes. In these cases, a minority group may disproportionately bear the cost of the need for the algorithm to learn additional parameters.

% It is also especially helpful in cases where student characteristics are not uniformly distributed. In that case, an algorithm without the extra information may instead learn a policy that systematically optimizes for the majority but not for a minority group. However, when this expressivity is not necessary, it increases variability across students and also increases the time for identifying the correct policy, thus significantly decreasing the number of students assigned the best version of the technology and slightly decreasing their average outcomes. Despite this, the results based on the real-world experimental data clarify the potential benefits of personalization by demonstrating that having extra information about students can sometimes make learning easier, outweighing the negative impact of learning additional parameters.


There are several limitations to our results. First, we have focused only on discrete student features and discrete outcomes but continuous parameters are also common. For example, we might measure student scores rather than homework completion or model prior knowledge as an estimated ability parameter. If one wanted to extend these analyses to real-valued student features, one could easily incorporate them into the current modeling framework with versions of Thompson sampling for real-valued outcomes~\cite{agrawal2013thompson}, and there exist metrics from a large literature for assessing whether students are treated fairly (e.g.,~\cite{binns2018fairness}). Using real-valued parameters is unlikely to significantly impact trends in results, except that defining student groups for analyzing equitable outcomes is more difficult. Our results from our universal optimal action scenarios show that, with binary rewards, knowledge of the student features is not beneficial if it is unnecessary for expressing the best policy. However, these results may not translate to the real-valued rewards case, where the latent student features will add to the variability in the distributions observed by the non-contextual bandit, and exploring these scenarios is an important step for future work. 
%However,  In contrast to binary outcomes, it is possible that there are cases with real-valued outcomes where knowledge of the student features is beneficial even if that knowledge is unnecessary to express the best policy. In the real-valued case, the latent student features will add to the variability in the distributions observed by the non-contextual bandit, and exploring these universal optimal action scenarios is an important step for future work. 
A second limitation is that our simulations comprise only a single student feature that influences the outcome, though in actual deployments multiple features may influence the best policy. Still, we believe that our results can guide system designers when thinking about such scenarios, especially in weighing the costs and benefits of including each possible variable.

%The real-world scenarios demonstrate that the MAB algorithms' achieve 

% - How different are rewards for MAB algortihms in real word versus a 50-50 split? 
% - How close are the MAB results to the best arm?
% - Why should we be concerned anyway? (variance, systematic differential treatment)
The results from the real-world scenarios highlight the potential value of MAB algorithms for educational technologies. For almost all scenarios and groups, both types of MAB algorithms chose the optimal condition more often than if students had been assigned uniformly at random, and average rewards were in many cases very close to the optimal expected reward (i.e. if the optimal action had been chosen for all students). The absolute difference in rewards was relatively small between the two bandit types--at most $0.075$--and the contextual bandit achieved at worst 12\% less than the optimal expected reward for any student group. Yet the earlier simulations urge caution for incorporating student characteristics, due to (1) decreases in achieved outcomes when these characteristics are unnecessary and (2) the systematically different treatment of students based on irrelevant characteristics, as illustrated by large difference in condition assignment probability between vectors of student characteristics with identical outcome probabilities. Thus, system designers should weigh the risk of not personalizing when the best policy for the minority differs from the majority with these side effects of personalization and ultimately strive to only include variables that past evidence suggests differentially impact outcomes.
% Since the impact on a minority group of failing to include the possibility of personalization is largest when the best policy for that group differs from the majority group, designers could weigh this risk more heavily, while still including only variables that past evidence suggests are differentially related to outcomes. Additionally, system designers must take into account the greater variability across student experiences that is introduced by contextual MAB algorithms, particularly the ethical issues raised by treating  students systematically differently based on irrelevant characteristics.

% as the rate at which the system chose an optimal action was above 50\% for almost all scenarios and groups

% Our results highlight the difference between optimal choices by a system and the stochastic student outcomes that arise from those choices. When differences between versions of a technology are small, even the best choices may not lead to large observed differences in student outcomes. This is clear in some of the real-world experiments we examined, where the vast majority of students completed their homework regardless of their condition assignment. 
% % Both contextual and non-contextual MAB algorithms performed well on the real-world data, and for only one experiment and one quartile of students was expected performance below that 
% In general, it is difficult to design multiple versions of an educational technology that are all \textit{a priori} reasonable and that lead to large differences in outcomes. 
% Yet, educational technology improvement is precisely the goal of many experiments in fields like artificial intelligence and education, and even incremental differences may in aggregate lead to better experiences for students.
% Thus, even though the average differences in outcomes across bandit types are relatively small, they should be a consideration for system designers. Since the impact on a minority group of failing to include the possibility of personalization is largest when the best policy for that group differs from the majority group, designers could weigh this risk more heavily, while still including only variables that past evidence suggests are differentially related to outcomes. Additionally, system designers must take into account the greater variability across student experiences that is introduced by contextual MAB algorithms, particularly the ethical issues raised by treating  students systematically differently based on irrelevant characteristics.

One could make a number of extensions of this work for using MAB algorithms to improve and personalize educational technologies. First, contextual MAB algorithms might mitigate issues of biases when different types of students interact with an educational technology and while all are most helped by the same version of the technology, their outcomes have different distributions. For example, struggling students may complete homework later, leading the MAB algorithm's early estimates to be non-representative of the broader population. Prior work has shown that this bias significantly worsens inference about the effectiveness of the technology as well as expected student outcomes~\cite{rafferty2019statistical}: the use of a contextual MAB algorithm could allow the system to adapt to such differences across students. Second, if the technology is used by a large number of students, the set of variables used by the contextual algorithm could be increased as more data are collected. Such a system might improve consistency across student outcomes, while still personalizing based on truly relevant features that are justified the sufficient information collected. The work in this paper both provides a starting point for considering what scenarios, algorithms, and metrics should be explored in future work, as well as guidance for system designers who would like to deploy MAB algorithms within their own technologies but are uncertain about which student characteristics, if any, to include for personalization. While including all possible characteristics might capture the desire for maximum peronalization, this works points to the potential costs of such personalization and suggests that consideration of how likely it is that such characteristics will matter is important both for performance and equitability across students.


% % Include something in here about possibility of partially sharing information across actions [ANR: What did I mean by that?]
% Topics to discuss:
% \begin{itemize}
%     \item Binary versus real-valued (Would this make a difference for the group-dependent reward scenarios?)
%     \item Rewards versus optimal actions - real-world impacts
%     \item Knowing about student characteristics could help mitivate differences in when different types of students participate (e.g., if students who are struggling tend to complete homework later).
% \end{itemize}
