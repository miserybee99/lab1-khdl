% SECTION Background and Related Work %
\section{Background and Related Work} \label{Related work}
To provide a comprehensive survey related to our proposed model, we review three emerging techniques:  BERTology models, Graph Convolutional Networks, and fusion BERTology models and Graph Convolutional Networks, respectively.

\textbf{BERTology (BERT and its variants):} In natural language processing (NLP), the main models for sequence transduction rely on complex recurrent neural networks \cite{LSTM} or convolutional neural networks \cite{Empirical-Evaluation-of-Gated-Recurrent-Neural-Networks-on-Sequence-Modeling}. However, the sequential nature of these models makes it infeasible to parallelize them when processing longer sequences, which poses a significant challenge. This problem is due to memory constraints that limit batching across examples. As a solution, \citet{AttentionIsAllYouNeed} proposed a new architecture called Transformer that uses attention mechanisms exclusively, eliminating the need for recurrence and convolutions. Inspired and inherited by Transformer, \citet{devlin-etal-2019-bert} proposed the idea of Bidirectional Encoder Representations from Transformers (BERT). This pre-trained model utilizes masked language models to create deep bidirectional representations. BERT obtains new state-of-the-art results on eleven NLP tasks upon launch, including nine GLUE tasks\footnote{\url{https://huggingface.co/datasets/glue}}, SQuAD\footnote{\url{https://huggingface.co/datasets/squad}} v1.0 and 2.0, and SWAG\footnote{\url{https://huggingface.co/datasets/swag}}. Following the success of BERT, several variants of it have been proposed and achieved improved results compared to BERT. Shortly after the release of BERT, mBERT was published by \citet{devlin-etal-2019-bert}. mBERT provides sentence representations for 104 languages. BERT and its variant were called as BERTology \cite{rogers2021primer}. RoBERTa \cite{DBLP:journals/corr/abs-1907-11692}, an optimized version of BERT, which is trained on a larger dataset, was proposed. Aside from multilingual versions or BERT-based models, researchers from different countries are promoted to build and improve monolingual models based on available BERT architectures for their languages: MacBERT for Chinese \cite{Chinese-BERT}, CamemBERT for France \cite{CamemBERT}. PhoBERT \cite{nguyen-tuan-nguyen-2020-phobert} was the initial public large-scale monolingual language model pre-trained for Vietnamese. PhoBERT provides a consistent improvement over the best pre-trained multilingual model, demonstrating superior performance in several Vietnamese-specific NLP tasks and advancing the state-of-the-art in this field.    

\textbf{Graph Neural Networks:} Recently, interest in Graph Neural Networks (GNNs) has grown \cite{wu2020comprehensive}. Representative examples of GNNs proposed by the present include Graph Convolutional Networks (GCN) and its variants, which is one of the most prominent graph deep learning models \cite{Graph-convolutional-networks}. \citet{semi-supervised} presented an algorithm with a GCN for semi-supervised node classification and achieved state-of-the-art classification results on several network datasets. Since then, GCN has been utilized in various applications, to name a few, prediction tasks \cite{prediction3, prediction4, prediction5}, recommendation tasks \cite{recommender1, recommender2}, and classification tasks \cite{class3, class4, class5}. Especially in the field of NLP, GCN has successfully explored NLP tasks such as semantic role labeling (\cite{Encoding-Sentences-with-Graph-Convolutional-Networks-for-Semantic-Role-Labeling, Adaptive-Convolution-for-Semantic-Role-Labeling}, machine translation \cite{Graph-Convolutional-Encoders-for-Syntax-aware-Neural-Machine-Translation, Exploiting-Semantics-in-Neural-Machine-Translation-with-Graph-Convolutional-Networks}, information extraction \cite{GraphIE, Cardinal-Graph-Convolution-Framework-for-Document-Information-Extraction}, relation extraction \cite{Graph-Convolution-over-Pruned-Dependency-Trees-Improves-Relation-Extraction, Attention-Guided-Graph-Convolutional-Networks-for-Relation-Extraction, Dual-Graph-Convolutional-Networks-for-Graph-Based-Semi-Supervised-Classification}, and text classification also. In the context of text classification, several GCN models have proposed, to name a few: An extension of the GCN framework to the inductive setting called GraphSAGE that allows embeddings to be efficiently generated for unseen nodes in \cite{Inductive-Representation-Learning-on-Large-Graphs}. \citet{FastGCN} presented FastGCN, a fast improvement of the GCN model for learning graph embeddings. FastGCN achieves significant speedups compared to traditional GCN training methods while maintaining similar or even better performance on several benchmark datasets. GCN recursively aggregates neighbor node representations, causing each layer's receptive field to grow exponentially. To address this drawback, \citet{Stochastic-Training-of-Graph-Convolutional-Networks-with-Variance-Reduction} introduced algorithms based on control variates to decrease the size of the receptive field. In \cite{Dual-Graph-Convolutional-Networks-for-Graph-Based-Semi-Supervised-Classification}, the authors proposed Dual Graph Convolutional Networks (DGCN) as a simple and scalable method for semi-supervised learning on graph-structured data which can be applied when only a tiny portion of the training data is labeled. \citet{Learning-Graph-Pooling-and-Hybrid-Convolutional-Operations-for-Text-Representations} proposed a graph pooling layer and the hybrid convolutional (hConv) layer that integrates GCN and regular convolutional operations to incorporate node ordering information, achieving a better performance over the conventional CNN-based and GCN-based methods. Recognizing the inherent complexity and redundant computations in GCN, \citet{Simplifying-Graph-Convolutional-Networks} proposed Simplifying Graph Convolutional Networks (SGCN) as a solution to address these issues. In a separate research endeavor, \citet{Graph-Convolutional-Networks-for-Text-Classification} focused on constructing a heterogeneous graph using a corpus and introduced a model known as Text Graph Convolutional Networks (TextGCN). This model employs graph neural networks to concurrently learn word and document embeddings, and it significantly outperformed existing state-of-the-art methods across multiple benchmark datasets.

% Since GCN remains unnecessary complexity and redundant computation, \citet{Simplifying-Graph-Convolutional-Networks} introduced Simplifying Graph Convolutional Networks (SGCN) to address the mentioned problem. \citet{Graph-Convolutional-Networks-for-Text-Classification} firstly built a corpus as a heterogeneous graph and proposed Text Graph Convolutional Networks (TextGCN). This model can learn word and document embeddings with graph neural networks jointly. TextGCN outperformed state-of-the-art comparison methods on several benchmark datasets. 

\textbf{Graph Neural Networks Integrated with BERT:} Following the success of BERT, Graph Neural Networks, and their variants, researchers proposed new models by applying them together. \citet{Graph-Bert} proposed GRAPH-BERT (Graph-Based BERT), which relies entirely on the attention mechanism for graph representation learning without any graph convolution or aggregation operators. \citet{VGCN-BERT} presented the VGCN-BERT model, which integrates the power of BERT with a Vocabulary Graph Convolutional Network (VGCN) to capture global information about the language vocabulary. \citet{Bert-Enhanced} proposed the Bert-Enhanced text Graph Neural Network model (BEGNN), in which text features are extracted using GNN, while semantic features are extracted using BERT. \citet{BertGCN} introduced Bert-GCN, which merges transductive learning with extensive pre-training to accomplish text classification.
