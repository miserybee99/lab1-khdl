% SECTION Conclusion and Future work %
\section{Conclusion and Future Work} 
\label{Conclusion}

In this study, we proposed a novel approach combining ViCGCN to take advantage of the powerful contextualized word representations learned by PhoBERT and leverage the graph structure of Graph Convolutional Networks (GCN). Moreover, we verified the impact of GCN on BERTology. The experimental results indicate that ViCGCN outperformed 13 powerful baseline models, including BERTology models, fused BERTology-GCN models, and state-of-the-art methods, across three benchmark social media datasets. Our proposed approach, ViCGCN, significantly improves up to 6.21\%, 4.61\%, and 2.63\% compared to the best Contextual Language Model and 2.38\%, 1.50\%, and 1.86\% compared to the best baseline model and previous studies on UIT-VSMEC, UIT-ViCTSD, and UIT-VSFC datasets, respectively. Additionally, our proposed ViCGCN model successfully addresses the challenge of imbalanced and noisy data in benchmark social media datasets. Finally, incorporating GCN as the final layer in BERTology models leads to a substantial improvement in performance, which claims the huge impact of GCN on Contextualized Language Models on text classification tasks.

Despite the outstanding results of our proposed approach, a practical text classification system that can be applied in real-world scenarios is necessary. Moreover, to improve the accuracy and robustness of our system, automatic pre-processing techniques for text normalization are required. This could include converting slang or informal language to the standard text, detecting and resolving spelling errors, or identifying and removing redundant information. Furthermore, we aim to experiment with other graph neural network models to compare their performance with the ViCGCN model, such as Graph Attention Networks (GATs) \cite{velickovic2018graph} and other graph-based models \cite{wu2020comprehensive}.
