{
  "title": "On Reducing Undesirable Behavior in Deep Reinforcement Learning Models",
  "authors": [
    "Ophir M. Carmel",
    "Guy Katz"
  ],
  "submission_date": "2023-09-06T09:47:36+00:00",
  "revised_dates": [
    "2023-09-11T14:09:36+00:00"
  ],
  "publication_venue": null,
  "abstract": "Deep reinforcement learning (DRL) has proven extremely useful in a large\nvariety of application domains. However, even successful DRL-based software can\nexhibit highly undesirable behavior. This is due to DRL training being based on\nmaximizing a reward function, which typically captures general trends but\ncannot precisely capture, or rule out, certain behaviors of the system. In this\npaper, we propose a novel framework aimed at drastically reducing the\nundesirable behavior of DRL-based software, while maintaining its excellent\nperformance. In addition, our framework can assist in providing engineers with\na comprehensible characterization of such undesirable behavior. Under the hood,\nour approach is based on extracting decision tree classifiers from erroneous\nstate-action pairs, and then integrating these trees into the DRL training\nloop, penalizing the system whenever it performs an error. We provide a\nproof-of-concept implementation of our approach, and use it to evaluate the\ntechnique on three significant case studies. We find that our approach can\nextend existing frameworks in a straightforward manner, and incurs only a\nslight overhead in training time. Further, it incurs only a very slight hit to\nperformance, or even in some cases - improves it, while significantly reducing\nthe frequency of undesirable behavior.",
  "categories": [
    "cs.LG"
  ],
  "arxiv_id": "2309.02869"
}