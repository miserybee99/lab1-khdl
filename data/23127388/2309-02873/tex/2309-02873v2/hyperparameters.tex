\section{Experimental details} \label{section:hyperparameters}
In this section, we will specify the configurations for each experiment.
This includes detailed descrpitions of the models as well as hyperparameters and training details. 

\subsection{Models} \label{section:mathematics}
In this section, we will add the necessary background on the models and data that we consider. 

\paragraph{Damped system: }
The data $x(t)$ are generated by adding a sine wave and a second sine oscillations that is exponentially damped over time. 
This yields the system
\begin{equation}
x(t)= \sin(0.1 t)+2\exp(-0.01 t)\sin(t).
\end{equation}
For the simulator, a simple sine wave with slight mismatch in the amplitude is chosen.
This yields
\begin{equation}
s(t)=0.7 \sin(0.1 t).
\end{equation}
The signals are obtained, by evaluating at $t=0, \dots 400$ with a discretization $\Delta t = 0.1$. Further the observation data are corrupted with white noise with variance $0.1$.  
\paragraph{Double torsion pendulum: }
For system ii), we consider the data from \citet{lisowski}.
We artificially add a transient component again $\Delta x$ via
\begin{equation}
\Delta x(t)=\exp(-0.5 t)\sin(50 t),
\end{equation}
evaluated at the interval $t = 0,1,2, \dots$. 
\paragraph{Van der Pol oscillator: }
For experiment iv), we consider a Van-der-Pol oscillator with additional control input. 
The differential equation for the Van-der-Pol oscillator with external force is given as
\begin{equation}
\begin{pmatrix}
\dot{x}(t) \\
\dot{y}(t) \\
\dot{u}(t) \\
\dot{v}(t)
\end{pmatrix}
=
\begin{pmatrix}
y \\
-x+a(1-x^2)y+bu \\
v \\
-\omega^2 u
\end{pmatrix},
\end{equation}
where $a,b$ and $\omega$ are system parameters. 
We choose $a=5, b=80, \omega=7.0$ and initial conditions 
\begin{equation}
\begin{pmatrix}
\dot{x}_0 \\
\dot{y}_0 \\
\dot{u}_0 \\
\dot{v}_0
\end{pmatrix}
=
\begin{pmatrix}
-2 \\
1 \\
0.31 \\
1
\end{pmatrix}.
\end{equation}
The system is evaluated at $t=5,\dots,100$ with step size $\Delta t=0.1$. 



\subsection{Architecture details}\label{sec:arch}
Here, we will add details on the chosen architectures. 
This includes details on the networks and training details such as hyperparameter choice. 
\subsubsection{KKL model}\label{sec:KKL}
We will first describe the details of the KKL architecture.
This includes the options for the nonlinear transformation, the controllable pair and modeling variants for the non-OVS residuum. 
\paragraph{Nonlinear transformation: }
As proposed in \citet{9683277}, we consider an MLP with three layers and ReLU activation functions in between for the default scenario.
The number of neurons for each experiment will be documented in Sec \ref{sec:hyperparameters}. 
In the default scenario, we jointly propagate $z$ and $u$ through time and do not explicitly model $f_u$ but directly the observer. 
Due to the observer property, $u_{n+1}$ converges to $f_u(u_n)$. 
One advantage of this choice is that the whole rollout $z_0,\dots,z_N$ can be precomputed and the transformation can be applied in a batched manner making it efficient. 

However, sometimes direct access to $f_u$ might be required, e.g. in order to analyze fixed points or include symmetries. 
In this case, we provide the option to train $T^{\star}_{\theta}$ with an invertible neural network. 
The network is then built by stacking affine coupling layers \citep{DBLP:conf/iclr/DinhSB17}.
In particular, we stack 2 coupling layers, where the inner neural networks are modeled via 2 linear layers with ReLU activation functions. 
In contrast to the default setting, the transformation $T_{\theta}$ and the inverse $T_{\theta}^{\star}$ have to be computed in each time step, making it computationally more expensive. 
Furthermore, the invertible NN approach is less flexible since the number of neurons per layer has to match the latent dimensionality.  
Thus, if no direct access to the dynamis $f_u$ is required, the default setting is favorable. 
\paragraph{Controllable pair: }
The matrix $D_{\theta}$ is chosen as trainable diagonal matrix with sigmoid activation functions on the entries.
This ensures that $\lambda \in (0,1)$ for the eigenvalues $\lambda$.
However, of course this restricts the eigenvalues to positive ones.
Still, it worked well for our experiments.
Other activation functions such as tanh etc. can be easily incorporated.
It would also be possible to train the matrix freely as proposed in \citet{9683277}.
However, the pair $(D,F)$ would not meet the required properties in Thm \ref{Thm1} by design anymore.  
\subsubsection{Remaining components}
Here, we will describe the setting for the remaining components, in particular the non-OVS residuum.
\paragraph{Non-OVS residuum: } 
As stated in the method section, we model the non-OVS part via
\begin{equation}
v_{n+1}=f_{\theta}^v(u_n,v_n,y_n),
\end{equation}
where $f_{\theta}^v$ is modeled as GRU architecture. 
For efficiency reasons, we omit to provide $u_n$ as an input. 
Note that this is possible without loss of generality. 
However, the implementation of the method additionally contains the option to provide the simulator as a control input to $f_{\theta}^v$. 
\paragraph{Exponentially damped non-OVS residuum: }
In order to obtain a non-OVS component that vanishes over time, we propose to exponentially damp the observations. 
This is achieved by constructing a non-linear observation model $g$.
The latent states evolve according to the GRU transitions in Eq.~\eqref{eq:GRUequation}.
However, the linear observation function $g(x)=U_o x$ (cf. Eq.~\eqref{eq:obs}) is replaced by a non-linear observation model.
In particular, we consider 
\begin{equation}
g_{\textrm{damp}}(x)=a \exp(-\textrm{softplus}(b)t_n)\tanh(U_0 x),
\end{equation}
with trainable parameters $a$, $b$, a trainable linear model $U_0$ and the time interval $t=t_0, \dots, t_n$. 


\subsection{GRU architecture}
We consider GRUs for our non-OVS residuum as well as for the baselines. 
We consider GRU dynamics $f$ as presented in Eq. \eqref{eq:GRUequation} with output feedback $y$ and control input $i$. 
The hidden GRU states are mapped to the observations via the observation model $g$ (cf. Eq. \eqref{eq:obs}).


\paragraph{Warmup: }
The hidden states of recurrent architectures such as GRUs are usually obtained with a short warmup phase of length $Rn$, where the architecture receives the observations as an input as described in Eq. \eqref{eq:warmup}.
The outputs are initialized with the observations on that horizon and therefore do not contribute to training. 
For the KKL observer, such a warmup phase is not required since it constantly receives the simulator outputs as an input.
However, the non-OVS residuum is initialized with standard GRU initialization via
\begin{equation}
y^v_{0:R}=\hat{y}_{0:R}-\left(g_{\theta}(u_n)\right)_{0:R}.
\end{equation}
This yields automatically that
\begin{equation}
y_{0:R}=\hat{y}_{0:R}. 
\end{equation}
Due to this architecture, the KKL observer contributes to training during the warmup phase but only via $s$, while the GRU does not contribute. 
However, this is balanzed by taking the MSE in the loss functions, which counterbalances different lengths of training trajectories. 
For the ablation study, all GRUs are initialized with exact data and start contributing to training after the warmup phase.  


\paragraph{Hybrid GRU: }
For the hybrid GRU, we provide the simulator $s$ as control input $i$ to the GRU dynamics \eqref{eq:GRUequation}.

\paragraph{Residual model: }
For the residual model, we consider the sum of GRU predictions $r$ and simulations $s$.
In particular, we consider the transitions 
\begin{equation}
\begin{aligned}
x_{n+1}&=f(x_n,\tilde r_n) \\
\end{aligned}
\end{equation}
with corresponding observation model 
\begin{equation}
\begin{aligned}
r_n &=U_ox_n \\
y_n &=r_n+s_n.
\end{aligned}
\end{equation}
The warmup is obtained by feeding the residuum of data for $R$ steps.
This yields
\begin{equation}\label{eq:warmup}
\begin{aligned}
\tilde r_n = 
\begin{cases}
\hat y_n-s_n & \textrm{ for } n \leq R, \\
r_n=U_o x_n & \textrm{ for } n > R. 
\end{cases}
\end{aligned}
\end{equation}
The hyperparameters are optimized by minimizing $\Vert \hat y_n-y_n \Vert$. 




\subsubsection{Filter design}\label{filter}
For the hybrid experiments, we reimplement a method similar to the hybrid model proposed in \citet{ensinger2023combining}.
It is implemented as follows:
\begin{itemize}
	\item Based on the properties of the simulator, the cutoff frequency is obtained, such that $L(s) \approx L(y)$.  
	\item The simulator signal is low-pass filtered with low-pass filter $L$, which yields $\tilde s = L(s)$.
	 This signal is stored.  
	\item During training and predictions the computation $\tilde s+H(y)$ is performed.
\end{itemize}
In contrast to their implementation, we apply forward-backward filtering provided by torch.
Further, the initialization slightly changes, since we filter the signals separately and add them.
Further, we precompute the low-pass filtered simulator signal and store it.  
The filter parameters are obtained with the help of \texttt{scipy.signal.iirfilter}. 
For the experiments here, we consider butterworth filters of order one. 
Thus, for a lowpass filter with cutoff frequency $\omega$, we call \texttt{iirfilter(N=1,Wn=$\omega$,rp=None,rs=None,btype="lowpass",analog=False,\\
ftype="butter",output="ba",fs=10)}.
Appropriate cutoff frequencies are obtained by analyzing the systems.
For training and predictions, we use the filters provided by \texttt{torchaudio}.
We use their implementation of forward-backward filtering \texttt{torchaudio.functional.filtfilt}, which first filters a signal and then filters it backwards.
 
 
\subsection{Hyperparameters}\label{sec:hyperparameters}
Here, we report the number of training steps, learning rates, latent dimensions, length of the warmup phase and additional parameters as cutoff frequency of the filters, damping factor etc. 
All models are trained with Adam optimizer. 
For all experiments, we train on subtrajectories of the full trajectory and consider batch sizes of length 50. 
To make a fair comparison, we choose an identical total amount of latent states for each experiment. 
\paragraph{i) Damped system: }
All models are trained with the learning rate $10^{-3}$. 
All models are trained for 300 steps. 
All models are trained on subtrajectories of length 100, the GRUs model receive 50 steps as a warmup phase. 
For our hybrid KKL-RNN, we consider a 32-dimensional latent space and an MLP with 100 neurons. 
The non-OVS residuum has a 32-dimensional latent space.
For the other models, we consider a GRU with 64 hidden states.
For the Filter, we consider the cutoff frequency 0.05.
%TO ADD: INITIAL DAMPING
\paragraph{ii): Double-torsion pendulum: }
All models are trained with the learning rate $10^{-3}$. 
The GRU is trained with an additional scheduler that multiplies the learning rate with 0.05 after 800 steps.  
All models are trained for 250 steps, the GRU is trained for 1000 steps. 
All models are trained on subtrajectories of length 50, the GRUs receive 20 steps for warmup. 
For our hybrid KKL-RNN, we consider a 64-dimensional latent space and an MLP with 100 neurons. 
The non-OVS residuum is regularized with weight 0.5. 
The non-OVS residuum has a 32-dimensional space. 
All other GRUs have 96-dimensional latent states. 
For the Filter, we consider the cutoff frequency 0.1.

\paragraph{iii) Drill-string system: }
All models are trained with the learning rate $10^{-3}$. 
All models are trained for 250 steps, the GRU is trained for 300 steps. 
All models are trained on subtrajectories of length 500, the GRUs receive 50 steps for the warmup phase.
For our hybrid KKL-RNN, we consider a 64-dimensional latent space and an MLP with 100 neurons. 
The non-OVS residuum is regularized with weight 0.5. 
The non-OVS residuum has a 32-dimensional space. 
For all other models, we consider GRUs with 96-dimensional hidden states. 
For the Filter, we consider the cutoff frequency 0.05.

\paragraph{iv) Van-der-Pol oscillator: } 
All models are trained with the learning rate $10^{-3}$.  
All models are trained for 500 steps, the GRU is trained for 800 steps. 
All models are trained on subtrajectories of length 200, the GRUs receive 20 steps for the warmup phase.
For our KKL-RNN, we consider a 32-dimensional latent space and an MLP with 100 neurons. 
The non-OVS part is regularized with weight 0.5. 
The non-observable GRU residuum has a 32-dimensional space. 
For all other models, we consider GRUs with 64-dimensional hidden states. 

\paragraph{v) Double torsion system: }
For the Filter baseline, we report the results provided in \citep{ensinger2023combining}.
For system v), we first pretrain the simulate substitute. 
To this, end the training signal is downsampled with a rate of 2 and low-pass filtered. 
On this signal, a GRU with 32 hidden dimensions is trained for 1000 training steps with a learning rate of $10^{-3}$. 
The trajectory is split into subtrajectories of length 125, 50 steps are used for warmup.
After training, the signal is upsampled with a rate of 2 again via the torch method \texttt{torch.nn.Upsample(scale\_factor = 2, mode="linear", align\_corners=False)}. 
The pretrained signal is then used as an input for the Residual model, the KKL-RNN and the Obs-GRU, while the GRU is trained in the standard setting on the whole input. 
All models are trained with a learning rate of $10^{-3}$.
KKL-RNN and Obs-GRU are trained on 150 steps, the Residual model is trained on 500 steps, the GRU is trained on 1151 steps. 
For all models, we consider subtrajectories of length 100 and for all GRUs, we consider 50 steps for the warmup phase. 
For our KKL-RNN, we consider a 32-dimensional latent space and an MLP with 100 neurons. 
The non-observable GRU residuum contains 32 hidden states. 
The residual model consists of a GRU with 64 hidden states. 
The Obs-GRU consists of a GRU with 64 latent states, the standard GRU has 96 latent states. 





