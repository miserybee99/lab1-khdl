\section{Limitations and Open Questions}
\label{sec:limitations}
Though we have proposed two effective non-``detect-then-describe'' methods for 3D dense captioning, the captions do not have much diversity because of the limited text annotations, beam search, and self-critical sequence training with the CiDEr reward.
% 
We believe that multi-modal pre-training on 3D vision-language tasks with more training data and the utilization of \textbf{L}arge \textbf{L}anguage \textbf{M}odels(LLM) trained on large corpus would increase the diversity of the generated captions.
% 
Additionally, other reward functions designed for 3D dense captioning will increase the diversity among object descriptions in the same scene.
% 
We will leave these topics for future study.


\section{Conclusions}
\label{sec:conclusion}
%
\whatsnew{
In this work, we decouple the caption generation from caption generation, and propose a set of two transformer-based approaches, namely Vote2Cap-DETR and Vote2Cap-DETR++, for 3D dense captioning.
%
Comparing with the sophisticated and explicit relation modules in conventional ``detect-then-describe'' pipelines, our proposed methods efficiently capture the object-object and object-scene relation through the attention mechanism.
%
The preliminary model, Vote2Cap-DETR, decouples the decoding process to generate captions and box estimations in parallel.
% 
We also propose vote queries for fast convergence, and develop a novel lightweight query-driven caption head for informative caption generation.
% 
In the advanced model, Vote2Cap-DETR++, we further decouple the queries to capture task-specific features for object localization and description generation.
% 
Additionally, we introduce an iterative spatial refinement strategy for vote queries, and insert 3D spatial information for more accurate captions.
%
Extensive experiments on two widely used datasets validate that both the proposed methods surpass prior ``detect-then-describe'' pipelines by a large margin.
}