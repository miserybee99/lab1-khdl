{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75240e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import re\n",
    "import psutil  # For memory tracking\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    START_MONTH = \"2023-09\"\n",
    "    START_ID = \"2848\" # 2848\n",
    "    END_MONTH = \"2023-09\" \n",
    "    END_ID = \"2850\"  # 7847\n",
    "\n",
    "    BASE_DIR = \"./data\"\n",
    "    STUDENT_ID = \"23127388\"\n",
    "    RATE_LIMIT_SEMANTIC_SCHOLAR = 3.0  # seconds between requests (increased from 1.1)\n",
    "    RATE_LIMIT_ARXIV = 3.0  # seconds between arXiv requests\n",
    "    MAX_RETRIES = 3\n",
    "    RETRY_DELAY = 60  # seconds to wait when hitting rate limit (429)\n",
    "\n",
    "\n",
    "def generate_arxiv_ids():\n",
    "    \"\"\"Generate list of arXiv IDs based on the given range\"\"\"\n",
    "    start_num = int(Config.START_ID)\n",
    "    end_num = int(Config.END_ID)\n",
    "    \n",
    "    # Convert \"2023-09\" to \"2309\" format for arXiv API\n",
    "    year_month = Config.START_MONTH.replace(\"-\", \"\")[2:]  # \"2023-09\" -> \"2309\"\n",
    "    \n",
    "    arxiv_ids = []\n",
    "    for i in range(start_num, end_num + 1):\n",
    "        arxiv_id = f\"{year_month}.{i:05d}\"\n",
    "        arxiv_ids.append(arxiv_id)\n",
    "    \n",
    "    logging.info(f\"Generated {len(arxiv_ids)} arXiv IDs from {arxiv_ids[0]} to {arxiv_ids[-1]}\")\n",
    "    return arxiv_ids\n",
    "\n",
    "def format_arxiv_id_for_path(arxiv_id):\n",
    "    \"\"\"Convert arXiv ID to folder name format (yyyymm-id)\"\"\"\n",
    "    \n",
    "    # Handle old format: \"hep-ph/0512066\" -> \"hep-ph-0512066\"\n",
    "    if '/' in arxiv_id:\n",
    "        return arxiv_id.replace('/', '-')\n",
    "    \n",
    "    # Handle new format: \"2309.02848\" -> \"2309-02848\"\n",
    "    if '.' in arxiv_id:\n",
    "        return arxiv_id.replace('.', '-')\n",
    "    \n",
    "    return arxiv_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_versions(arxiv_id):\n",
    "    \"\"\"Get all versions of a paper\"\"\"\n",
    "    try:\n",
    "        # Rate limiting for arXiv API\n",
    "        time.sleep(Config.RATE_LIMIT_ARXIV)\n",
    "        \n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        papers = list(client.results(search))\n",
    "        \n",
    "        if not papers:\n",
    "            return []\n",
    "        \n",
    "        paper = papers[0]\n",
    "        \n",
    "        # Try to get version count from the paper object\n",
    "        # arxiv.py doesn't directly expose version info, so we'll try the entry_id\n",
    "        versions = []\n",
    "        \n",
    "        # Get the latest version number from the entry_id\n",
    "        # Format: http://arxiv.org/abs/2309.02848v2\n",
    "        if hasattr(paper, 'entry_id') and paper.entry_id:\n",
    "            match = re.search(r'v(\\d+)$', paper.entry_id)\n",
    "            if match:\n",
    "                latest_version = int(match.group(1))\n",
    "                for v in range(1, latest_version + 1):\n",
    "                    versions.append(f\"{arxiv_id}v{v}\")\n",
    "            else:\n",
    "                versions.append(f\"{arxiv_id}v1\")\n",
    "        else:\n",
    "            versions.append(f\"{arxiv_id}v1\")\n",
    "        \n",
    "        return versions\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting versions for {arxiv_id}: {str(e)}\")\n",
    "        return [f\"{arxiv_id}v1\"]  # At least try version 1\n",
    "\n",
    "def download_arxiv_paper(arxiv_id_with_version, save_dir):\n",
    "    \"\"\"Download arXiv paper source and metadata for a specific version\"\"\"\n",
    "    try:\n",
    "        # Rate limiting for arXiv API\n",
    "        time.sleep(Config.RATE_LIMIT_ARXIV)\n",
    "        \n",
    "        # Extract base ID without version\n",
    "        base_id = re.sub(r'v\\d+$', '', arxiv_id_with_version)\n",
    "        \n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[base_id])\n",
    "        papers = list(client.results(search))\n",
    "        \n",
    "        if not papers:\n",
    "            logging.warning(f\"Paper not found: {arxiv_id_with_version}\")\n",
    "            return None, None\n",
    "        \n",
    "        paper = papers[0]\n",
    "        \n",
    "        # Download source file\n",
    "        source_filename = f\"{arxiv_id_with_version}.tar.gz\"\n",
    "        source_path = os.path.join(save_dir, source_filename)\n",
    "        \n",
    "        try:\n",
    "            paper.download_source(dirpath=save_dir, filename=source_filename)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not download source for {arxiv_id_with_version}: {str(e)}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Extract metadata (same for all versions)\n",
    "        metadata = {\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": [author.name for author in paper.authors],\n",
    "            \"submission_date\": paper.published.isoformat() if paper.published else None,\n",
    "            \"revised_dates\": [paper.updated.isoformat()] if paper.updated else [],\n",
    "            \"publication_venue\": paper.journal_ref if hasattr(paper, 'journal_ref') else None,\n",
    "            \"abstract\": paper.summary,\n",
    "            \"categories\": paper.categories,\n",
    "            \"arxiv_id\": base_id\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"âœ“ Downloaded {arxiv_id_with_version}: {paper.title[:50]}...\")\n",
    "        return metadata, source_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"âœ— Error downloading {arxiv_id_with_version}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_and_remove_figures(tar_path, extract_dir):\n",
    "    \"\"\"Extract tar.gz/gz, keep only .tex files and empty folders\"\"\"\n",
    "    import gzip\n",
    "    import shutil\n",
    "    \n",
    "    try:\n",
    "        # Create extraction directory\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        \n",
    "        # Try to extract as tar.gz first\n",
    "        try:\n",
    "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "                tar.extractall(path=extract_dir)\n",
    "            logging.info(f\"Extracted tar.gz archive to {extract_dir}\")\n",
    "        except tarfile.ReadError:\n",
    "            # Not a tar archive, might be a single .gz file\n",
    "            logging.warning(f\"{tar_path} is not a tar archive, trying as single .gz file\")\n",
    "            try:\n",
    "                # Extract as single gzip file\n",
    "                output_file = os.path.join(extract_dir, \"main.tex\")\n",
    "                with gzip.open(tar_path, 'rb') as f_in:\n",
    "                    with open(output_file, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                logging.info(f\"Extracted single .gz file to {output_file}\")\n",
    "            except Exception as gz_error:\n",
    "                logging.error(f\"Failed to extract as .gz: {str(gz_error)}\")\n",
    "                # Try copying as plain text (some papers might not have source)\n",
    "                try:\n",
    "                    output_file = os.path.join(extract_dir, \"README.txt\")\n",
    "                    with open(output_file, 'w') as f:\n",
    "                        f.write(\"Source not available or extraction failed.\\n\")\n",
    "                        f.write(f\"Original file: {tar_path}\\n\")\n",
    "                    logging.warning(f\"Could not extract source, created placeholder\")\n",
    "                except:\n",
    "                    return True\n",
    "        \n",
    "        # Remove ALL non-.tex files but KEEP folders (including Figures/, vis/, etc.)\n",
    "        # Only keep .tex files and empty folders\n",
    "        print(f\"ðŸ§¹ Cleaning up non-.tex files in {os.path.basename(extract_dir)}...\")\n",
    "        removed_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        for root, dirs, files in os.walk(extract_dir):\n",
    "            for file in files:\n",
    "                file_ext = os.path.splitext(file)[1].lower()\n",
    "                # Keep ONLY .tex files, remove everything else\n",
    "                if file_ext != '.tex':\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        os.remove(file_path)\n",
    "                        removed_count += 1\n",
    "                    except Exception as e:\n",
    "                        failed_count += 1\n",
    "                        logging.warning(f\"Failed to remove {file_path}: {str(e)}\")\n",
    "        \n",
    "        logging.info(f\"Cleanup: Removed {removed_count} non-.tex files, {failed_count} failed from {extract_dir}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting {tar_path}: {str(e)}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3706a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references_from_semantic_scholar(arxiv_id):\n",
    "    \"\"\"Get references from Semantic Scholar API with rate limiting and retry\"\"\"\n",
    "    \n",
    "    for attempt in range(Config.MAX_RETRIES):\n",
    "        time.sleep(Config.RATE_LIMIT_SEMANTIC_SCHOLAR)  # Rate limiting\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{arxiv_id}\"\n",
    "            params = {\n",
    "                \"fields\": \"references,references.externalIds,references.title,references.authors,references.year,references.publicationVenue\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                references_data = {}\n",
    "                \n",
    "                for ref in data.get(\"references\", []):\n",
    "                    # Safety check: ref might be None\n",
    "                    if not ref or not isinstance(ref, dict):\n",
    "                        continue\n",
    "                    \n",
    "                    external_ids = ref.get(\"externalIds\")\n",
    "                    if not external_ids or not isinstance(external_ids, dict):\n",
    "                        continue\n",
    "                    \n",
    "                    arxiv_id_ref = external_ids.get(\"ArXiv\")\n",
    "                    if not arxiv_id_ref:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Validate arXiv ID format before processing\n",
    "                        # ONLY accept new format: \"2309.02848\" or \"2309.02848v1\"\n",
    "                        # SKIP old format: \"hep-ph/0512066\", \"cs/0001001\"\n",
    "                        \n",
    "                        # Remove version suffix if present\n",
    "                        clean_id = re.sub(r'v\\d+$', '', str(arxiv_id_ref))\n",
    "                        \n",
    "                        # Check if it's NEW format ONLY (must have dot, NOT slash)\n",
    "                        if '.' not in clean_id or '/' in clean_id:\n",
    "                            logging.debug(f\"Skipping non-new-format arXiv ID: {arxiv_id_ref}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate new format pattern: YYMM.NNNNN\n",
    "                        if not re.match(r'^\\d{4}\\.\\d+$', clean_id):\n",
    "                            logging.warning(f\"Skipping invalid new-format arXiv ID: {arxiv_id_ref}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Format arXiv ID for path\n",
    "                        formatted_id = format_arxiv_id_for_path(clean_id)\n",
    "                        \n",
    "                        # Safety check for authors\n",
    "                        authors_list = ref.get(\"authors\", [])\n",
    "                        if authors_list is None:\n",
    "                            authors_list = []\n",
    "                        \n",
    "                        ref_metadata = {\n",
    "                            \"title\": ref.get(\"title\", \"\"),\n",
    "                            \"authors\": [author.get(\"name\", \"\") for author in authors_list if author and isinstance(author, dict)],\n",
    "                            \"submission_date\": f\"{ref.get('year', '')}-01-01\" if ref.get(\"year\") else None,\n",
    "                            \"revised_dates\": [],  # Semantic Scholar doesn't provide revised dates\n",
    "                            \"publication_venue\": ref.get(\"publicationVenue\", \"\"),\n",
    "                            \"year\": ref.get(\"year\")\n",
    "                        }\n",
    "                        \n",
    "                        references_data[formatted_id] = ref_metadata\n",
    "                        \n",
    "                    except Exception as parse_error:\n",
    "                        logging.warning(f\"Error parsing reference {arxiv_id_ref}: {str(parse_error)}\")\n",
    "                        continue\n",
    "                \n",
    "                logging.info(f\"Found {len(references_data)} arXiv references for {arxiv_id}\")\n",
    "                return references_data\n",
    "                \n",
    "            elif response.status_code == 404:\n",
    "                logging.warning(f\"Paper {arxiv_id} not found in Semantic Scholar\")\n",
    "                return {}\n",
    "                \n",
    "            elif response.status_code == 429:\n",
    "                # Rate limit exceeded - wait longer and retry\n",
    "                wait_time = Config.RETRY_DELAY * (attempt + 1)\n",
    "                logging.warning(f\"Rate limit hit for {arxiv_id} (attempt {attempt + 1}/{Config.MAX_RETRIES}). Waiting {wait_time}s...\")\n",
    "                if attempt < Config.MAX_RETRIES - 1:\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(f\"Failed to get references for {arxiv_id} after {Config.MAX_RETRIES} attempts\")\n",
    "                    return {}\n",
    "            else:\n",
    "                logging.warning(f\"Semantic Scholar API error for {arxiv_id}: {response.status_code}\")\n",
    "                return {}\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting references for {arxiv_id}: {str(e)}\")\n",
    "            if attempt < Config.MAX_RETRIES - 1:\n",
    "                time.sleep(Config.RETRY_DELAY)\n",
    "                continue\n",
    "            return {}\n",
    "    \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9647285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_paper(arxiv_id, base_dir):\n",
    "    \"\"\"Process a single arXiv paper - download all versions, extract, get references\"\"\"\n",
    "    try:\n",
    "        # Create paper directory\n",
    "        paper_dir_name = format_arxiv_id_for_path(arxiv_id)\n",
    "        paper_dir = os.path.join(base_dir, Config.STUDENT_ID, paper_dir_name)\n",
    "        tex_dir = os.path.join(paper_dir, \"tex\")\n",
    "        os.makedirs(tex_dir, exist_ok=True)\n",
    "        \n",
    "        # Step 1: Get all versions of the paper\n",
    "        versions = get_paper_versions(arxiv_id)\n",
    "        logging.info(f\"Found {len(versions)} version(s) for {arxiv_id}\")\n",
    "        \n",
    "        if not versions:\n",
    "            logging.warning(f\"No versions found for {arxiv_id}\")\n",
    "            return False\n",
    "        \n",
    "        # Step 2: Download all versions\n",
    "        metadata = None\n",
    "        successful_downloads = 0\n",
    "        \n",
    "        for version in versions:\n",
    "            version_metadata, source_path = download_arxiv_paper(version, paper_dir)\n",
    "            \n",
    "            if version_metadata and source_path:\n",
    "                # Store metadata from first successful download\n",
    "                if metadata is None:\n",
    "                    metadata = version_metadata\n",
    "                \n",
    "                # Version folder name: \"2309-02848v1\" instead of \"v1\"\n",
    "                version_formatted = format_arxiv_id_for_path(version)\n",
    "                \n",
    "                # Extract to version-specific subdirectory in tex/\n",
    "                # Only keep .tex files and empty folders\n",
    "                version_tex_dir = os.path.join(tex_dir, version_formatted)\n",
    "                if extract_and_remove_figures(source_path, version_tex_dir):\n",
    "                    successful_downloads += 1\n",
    "                \n",
    "                # Clean up: remove original tar.gz file\n",
    "                if os.path.exists(source_path):\n",
    "                    os.remove(source_path)\n",
    "        \n",
    "        if successful_downloads == 0:\n",
    "            logging.error(f\"Failed to download any version for {arxiv_id}\")\n",
    "            return False\n",
    "        \n",
    "        # Step 3: Get references from Semantic Scholar (only once for the paper)\n",
    "        # Skip references without arXiv ID (already filtered in get_references_from_semantic_scholar)\n",
    "        references = get_references_from_semantic_scholar(arxiv_id)\n",
    "        \n",
    "        # Step 4: Save all data\n",
    "        # Save metadata\n",
    "        with open(os.path.join(paper_dir, \"metadata.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save references (only those with arXiv IDs)\n",
    "        with open(os.path.join(paper_dir, \"references.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(references, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logging.info(f\"âœ“ Successfully processed {arxiv_id} ({successful_downloads}/{len(versions)} versions)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"âœ— Failed to process {arxiv_id}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def scrape_arxiv_papers():\n",
    "    \"\"\"Main function to scrape all arXiv papers in the given range\"\"\"\n",
    "    # Generate arXiv IDs\n",
    "    arxiv_ids = generate_arxiv_ids()\n",
    "    \n",
    "    # Create base directory\n",
    "    base_dir = Path(Config.BASE_DIR)\n",
    "    base_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'total': len(arxiv_ids),\n",
    "        'success': 0,\n",
    "        'failed': 0,\n",
    "        'failed_ids': [],\n",
    "        'start_time': datetime.now()\n",
    "    }\n",
    "    \n",
    "    logging.info(f\"Starting to scrape {stats['total']} papers...\")\n",
    "    \n",
    "    # Process each paper\n",
    "    for i, arxiv_id in enumerate(tqdm(arxiv_ids, desc=\"Scraping papers\")):\n",
    "        logging.info(f\"Processing {arxiv_id} ({i+1}/{stats['total']})\")\n",
    "        \n",
    "        success = process_single_paper(arxiv_id, base_dir)\n",
    "        \n",
    "        if success:\n",
    "            stats['success'] += 1\n",
    "        else:\n",
    "            stats['failed'] += 1\n",
    "            stats['failed_ids'].append(arxiv_id)\n",
    "        \n",
    "        # Progress update every 10 papers\n",
    "        if (i + 1) % 10 == 0:\n",
    "            logging.info(f\"Progress: {i+1}/{stats['total']} - Success: {stats['success']}, Failed: {stats['failed']}\")\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    stats['end_time'] = datetime.now()\n",
    "    stats['duration'] = stats['end_time'] - stats['start_time']\n",
    "    stats['success_rate'] = (stats['success'] / stats['total']) * 100\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f2500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6:\n",
    "\n",
    "def calculate_statistics():\n",
    "    \"\"\"Calculate statistics about scraped data\"\"\"\n",
    "    data_dir = Path(Config.BASE_DIR) / Config.STUDENT_ID\n",
    "    \n",
    "    if not data_dir.exists():\n",
    "        return {\"error\": \"Data directory not found\"}\n",
    "    \n",
    "    stats = {\n",
    "        'total_papers': 0,\n",
    "        'paper_sizes': [],\n",
    "        'reference_counts': [],\n",
    "        'successful_references': 0,\n",
    "        'total_references': 0\n",
    "    }\n",
    "    \n",
    "    for paper_dir in data_dir.iterdir():\n",
    "        if paper_dir.is_dir():\n",
    "            stats['total_papers'] += 1\n",
    "            \n",
    "            # Calculate paper size\n",
    "            tex_dir = paper_dir / \"tex\"\n",
    "            if tex_dir.exists():\n",
    "                paper_size = sum(f.stat().st_size for f in tex_dir.rglob('*') if f.is_file())\n",
    "                stats['paper_sizes'].append(paper_size)\n",
    "            \n",
    "            # Count references\n",
    "            references_file = paper_dir / \"references.json\"\n",
    "            if references_file.exists():\n",
    "                with open(references_file, 'r') as f:\n",
    "                    references = json.load(f)\n",
    "                    ref_count = len(references)\n",
    "                    stats['reference_counts'].append(ref_count)\n",
    "                    stats['total_references'] += ref_count\n",
    "                    stats['successful_references'] += sum(1 for ref in references.values() if ref.get('title'))\n",
    "    \n",
    "    # Calculate averages\n",
    "    if stats['paper_sizes']:\n",
    "        stats['avg_paper_size_kb'] = sum(stats['paper_sizes']) / len(stats['paper_sizes']) / 1024\n",
    "        stats['max_paper_size_kb'] = max(stats['paper_sizes']) / 1024 if stats['paper_sizes'] else 0\n",
    "        stats['min_paper_size_kb'] = min(stats['paper_sizes']) / 1024 if stats['paper_sizes'] else 0\n",
    "    \n",
    "    if stats['reference_counts']:\n",
    "        stats['avg_references_per_paper'] = sum(stats['reference_counts']) / len(stats['reference_counts'])\n",
    "        stats['reference_success_rate'] = (stats['successful_references'] / stats['total_references'] * 100) if stats['total_references'] > 0 else 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def generate_report(stats, scraping_stats):\n",
    "    \"\"\"Generate comprehensive report (in-memory only, not saved to file)\"\"\"\n",
    "    report = {\n",
    "        \"scraping_summary\": {\n",
    "            \"student_id\": Config.STUDENT_ID,\n",
    "            \"arxiv_id_range\": f\"{Config.START_MONTH}.{Config.START_ID} to {Config.END_MONTH}.{Config.END_ID}\",\n",
    "            \"total_papers_attempted\": scraping_stats['total'],\n",
    "            \"successfully_scraped\": scraping_stats['success'],\n",
    "            \"failed\": scraping_stats['failed'],\n",
    "            \"success_rate\": f\"{scraping_stats['success_rate']:.2f}%\",\n",
    "            \"duration\": str(scraping_stats['duration'])\n",
    "        },\n",
    "        \"data_statistics\": stats,\n",
    "        \"performance_metrics\": {\n",
    "            \"tools_used\": [\"arxiv.py\", \"requests\", \"Semantic Scholar API\"],\n",
    "            \"rate_limiting\": f\"{Config.RATE_LIMIT_SEMANTIC_SCHOLAR}s between Semantic Scholar requests\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # No longer save report to file\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e93a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     separator = \"-------\" * 10\n",
    "    \n",
    "#     print(separator)\n",
    "#     print(\"ðŸš€ Starting arXiv Paper Scraper\")\n",
    "#     print(f\"ðŸ“Š Range: {Config.START_MONTH}.{Config.START_ID} to {Config.END_MONTH}.{Config.END_ID}\")\n",
    "#     print(f\"ðŸŽ¯ Student ID: {Config.STUDENT_ID}\")\n",
    "#     print(separator)\n",
    "    \n",
    "#     # Start scraping\n",
    "#     scraping_stats = scrape_arxiv_papers()\n",
    "    \n",
    "#     print(\"\\n\" + separator)\n",
    "#     print(\"ðŸ“ˆ Scraping Completed! Calculating statistics...\")\n",
    "#     print(separator)\n",
    "    \n",
    "#     # Calculate statistics\n",
    "#     stats = calculate_statistics()\n",
    "    \n",
    "#     # Generate report\n",
    "#     report = generate_report(stats, scraping_stats)\n",
    "    \n",
    "#     # Print summary\n",
    "#     print(\"\\n\" + separator)\n",
    "#     print(\"ðŸŽ‰ FINAL SUMMARY:\")\n",
    "#     print(separator)\n",
    "#     print(f\"Total papers attempted: {report['scraping_summary']['total_papers_attempted']}\")\n",
    "#     print(f\"Successfully scraped: {report['scraping_summary']['successfully_scraped']}\")\n",
    "#     print(f\"Failed: {report['scraping_summary']['failed']}\")\n",
    "#     print(f\"Success rate: {report['scraping_summary']['success_rate']}\")\n",
    "#     print(f\"Duration: {report['scraping_summary']['duration']}\")\n",
    "#     print(separator)\n",
    "    \n",
    "#     if 'avg_paper_size_kb' in stats:\n",
    "#         print(f\"Average paper size: {stats['avg_paper_size_kb']:.2f} KB\")\n",
    "#         print(f\"Max paper size: {stats['max_paper_size_kb']:.2f} KB\")\n",
    "#         print(f\"Min paper size: {stats['min_paper_size_kb']:.2f} KB\")\n",
    "#     if 'avg_references_per_paper' in stats:\n",
    "#         print(f\"Average references per paper: {stats['avg_references_per_paper']:.2f}\")\n",
    "#         print(f\"Reference scraping success rate: {stats.get('reference_success_rate', 0):.2f}%\")\n",
    "    \n",
    "#     print(separator)\n",
    "#     print(f\"ðŸ“ Data saved in: {Config.BASE_DIR}/{Config.STUDENT_ID}/\")\n",
    "#     print(separator)\n",
    "    \n",
    "#     # Print failed IDs if any\n",
    "#     if scraping_stats.get('failed_ids'):\n",
    "#         print(\"\\nâš ï¸  Failed paper IDs:\")\n",
    "#         for failed_id in scraping_stats['failed_ids']:\n",
    "#             print(f\"  - {failed_id}\")\n",
    "#         print(separator)\n",
    "\n",
    "# # main function to run program\n",
    "# if __name__ == \"__main__\": \n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26083c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED METRICS COLLECTION\n",
    "\n",
    "def get_directory_size(path):\n",
    "    \"\"\"Calculate total size of directory in bytes\"\"\"\n",
    "    total = 0\n",
    "    try:\n",
    "        for entry in os.scandir(path):\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_directory_size(entry.path)\n",
    "    except:\n",
    "        pass\n",
    "    return total\n",
    "\n",
    "def process_single_paper_with_metrics(arxiv_id, base_dir, global_stats):\n",
    "    \"\"\"Process a single paper with detailed metrics tracking\"\"\"\n",
    "    paper_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create paper directory\n",
    "        paper_dir_name = format_arxiv_id_for_path(arxiv_id)\n",
    "        paper_dir = os.path.join(base_dir, Config.STUDENT_ID, paper_dir_name)\n",
    "        tex_dir = os.path.join(paper_dir, \"tex\")\n",
    "        os.makedirs(tex_dir, exist_ok=True)\n",
    "        \n",
    "        # Step 1: Get all versions\n",
    "        versions = get_paper_versions(arxiv_id)\n",
    "        if not versions:\n",
    "            return False, 0, 0, 0, 0\n",
    "        \n",
    "        # Step 2: Download and extract all versions\n",
    "        metadata = None\n",
    "        successful_downloads = 0\n",
    "        total_size_before = 0\n",
    "        total_size_after = 0\n",
    "        \n",
    "        for version in versions:\n",
    "            version_metadata, source_path = download_arxiv_paper(version, paper_dir)\n",
    "            \n",
    "            if version_metadata and source_path:\n",
    "                if metadata is None:\n",
    "                    metadata = version_metadata\n",
    "                \n",
    "                # Measure size before extraction\n",
    "                if os.path.exists(source_path):\n",
    "                    size_before_extract = os.path.getsize(source_path)\n",
    "                else:\n",
    "                    size_before_extract = 0\n",
    "                \n",
    "                # Extract\n",
    "                version_formatted = format_arxiv_id_for_path(version)\n",
    "                version_tex_dir = os.path.join(tex_dir, version_formatted)\n",
    "                \n",
    "                # Calculate size before cleanup\n",
    "                if extract_and_remove_figures(source_path, version_tex_dir):\n",
    "                    # Size after cleanup\n",
    "                    size_after = get_directory_size(version_tex_dir)\n",
    "                    total_size_before += size_before_extract\n",
    "                    total_size_after += size_after\n",
    "                    successful_downloads += 1\n",
    "                \n",
    "                # Clean up tar.gz\n",
    "                if os.path.exists(source_path):\n",
    "                    os.remove(source_path)\n",
    "        \n",
    "        if successful_downloads == 0:\n",
    "            return False, 0, 0, 0, 0\n",
    "        \n",
    "        # Step 3: Get references\n",
    "        references = get_references_from_semantic_scholar(arxiv_id)\n",
    "        \n",
    "        # Step 4: Save data\n",
    "        with open(os.path.join(paper_dir, \"metadata.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        with open(os.path.join(paper_dir, \"references.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(references, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        paper_time = time.time() - paper_start_time\n",
    "        \n",
    "        return True, paper_time, total_size_before, total_size_after, len(references)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {arxiv_id}: {str(e)}\")\n",
    "        return False, 0, 0, 0, 0\n",
    "\n",
    "def scrape_with_enhanced_metrics():\n",
    "    \"\"\"Scrape with comprehensive metrics collection\"\"\"\n",
    "    # Start metrics\n",
    "    overall_start_time = time.time()\n",
    "    process = psutil.Process()\n",
    "    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Entry discovery timing\n",
    "    entry_start = time.time()\n",
    "    arxiv_ids = generate_arxiv_ids()\n",
    "    entry_discovery_time = time.time() - entry_start\n",
    "    \n",
    "    # Create base directory\n",
    "    base_dir = Path(Config.BASE_DIR)\n",
    "    base_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'total': len(arxiv_ids),\n",
    "        'success': 0,\n",
    "        'failed': 0,\n",
    "        'failed_ids': [],\n",
    "        'paper_times': [],\n",
    "        'sizes_before': [],\n",
    "        'sizes_after': [],\n",
    "        'reference_counts': [],\n",
    "        'memory_samples': [],\n",
    "        'disk_usage_samples': []\n",
    "    }\n",
    "    \n",
    "    # Initial disk usage\n",
    "    try:\n",
    "        disk_usage = psutil.disk_usage(str(base_dir))\n",
    "        stats['initial_disk_free'] = disk_usage.free / 1024 / 1024 / 1024  # GB\n",
    "    except:\n",
    "        stats['initial_disk_free'] = 0\n",
    "    \n",
    "    logging.info(f\"Starting to scrape {stats['total']} papers...\")\n",
    "    print(f\"Entry discovery took {entry_discovery_time:.2f} seconds\")\n",
    "    \n",
    "    # Process each paper\n",
    "    for i, arxiv_id in enumerate(tqdm(arxiv_ids, desc=\"Scraping papers\")):\n",
    "        # Memory sampling (every 10 papers)\n",
    "        if i % 10 == 0:\n",
    "            current_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            stats['memory_samples'].append(current_memory)\n",
    "            \n",
    "            # Disk usage\n",
    "            try:\n",
    "                data_dir = base_dir / Config.STUDENT_ID\n",
    "                if data_dir.exists():\n",
    "                    disk_used = get_directory_size(str(data_dir)) / 1024 / 1024  # MB\n",
    "                    stats['disk_usage_samples'].append(disk_used)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        success, paper_time, size_before, size_after, ref_count = process_single_paper_with_metrics(\n",
    "            arxiv_id, base_dir, stats\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            stats['success'] += 1\n",
    "            stats['paper_times'].append(paper_time)\n",
    "            stats['sizes_before'].append(size_before)\n",
    "            stats['sizes_after'].append(size_after)\n",
    "            stats['reference_counts'].append(ref_count)\n",
    "        else:\n",
    "            stats['failed'] += 1\n",
    "            stats['failed_ids'].append(arxiv_id)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            logging.info(f\"Progress: {i+1}/{stats['total']} - Success: {stats['success']}, Failed: {stats['failed']}\")\n",
    "    \n",
    "    # Final metrics\n",
    "    overall_duration = time.time() - overall_start_time\n",
    "    final_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Final disk usage\n",
    "    try:\n",
    "        data_dir = base_dir / Config.STUDENT_ID\n",
    "        if data_dir.exists():\n",
    "            final_disk_used = get_directory_size(str(data_dir)) / 1024 / 1024  # MB\n",
    "        else:\n",
    "            final_disk_used = 0\n",
    "    except:\n",
    "        final_disk_used = 0\n",
    "    \n",
    "    # Calculate statistics\n",
    "    enhanced_stats = {\n",
    "        'scraping_summary': {\n",
    "            'student_id': Config.STUDENT_ID,\n",
    "            'arxiv_id_range': f\"{Config.START_MONTH}.{Config.START_ID} to {Config.END_MONTH}.{Config.END_ID}\",\n",
    "            'total_papers_attempted': stats['total'],\n",
    "            'successfully_scraped': stats['success'],\n",
    "            'failed': stats['failed'],\n",
    "            'success_rate_percent': (stats['success'] / stats['total']) * 100 if stats['total'] > 0 else 0,\n",
    "            'failed_paper_ids': stats['failed_ids']\n",
    "        },\n",
    "        'timing_metrics': {\n",
    "            'total_duration_seconds': overall_duration,\n",
    "            'total_duration_formatted': str(datetime.now() - datetime.fromtimestamp(time.time() - overall_duration)).split('.')[0],\n",
    "            'entry_discovery_time_seconds': entry_discovery_time,\n",
    "            'average_time_per_paper_seconds': sum(stats['paper_times']) / len(stats['paper_times']) if stats['paper_times'] else 0,\n",
    "            'min_time_per_paper_seconds': min(stats['paper_times']) if stats['paper_times'] else 0,\n",
    "            'max_time_per_paper_seconds': max(stats['paper_times']) if stats['paper_times'] else 0,\n",
    "        },\n",
    "        'storage_metrics': {\n",
    "            'average_paper_size_before_cleanup_mb': sum(stats['sizes_before']) / len(stats['sizes_before']) / 1024 / 1024 if stats['sizes_before'] else 0,\n",
    "            'average_paper_size_after_cleanup_mb': sum(stats['sizes_after']) / len(stats['sizes_after']) / 1024 / 1024 if stats['sizes_after'] else 0,\n",
    "            'total_size_before_cleanup_mb': sum(stats['sizes_before']) / 1024 / 1024 if stats['sizes_before'] else 0,\n",
    "            'total_size_after_cleanup_mb': sum(stats['sizes_after']) / 1024 / 1024 if stats['sizes_after'] else 0,\n",
    "            'storage_saved_percent': ((sum(stats['sizes_before']) - sum(stats['sizes_after'])) / sum(stats['sizes_before']) * 100) if sum(stats['sizes_before']) > 0 else 0,\n",
    "            'final_output_size_mb': final_disk_used,\n",
    "            'final_output_size_gb': final_disk_used / 1024,\n",
    "        },\n",
    "        'memory_metrics': {\n",
    "            'initial_memory_mb': initial_memory,\n",
    "            'final_memory_mb': final_memory,\n",
    "            'max_memory_used_mb': max(stats['memory_samples']) if stats['memory_samples'] else final_memory,\n",
    "            'average_memory_mb': sum(stats['memory_samples']) / len(stats['memory_samples']) if stats['memory_samples'] else final_memory,\n",
    "            'memory_increase_mb': final_memory - initial_memory,\n",
    "        },\n",
    "        'reference_metrics': {\n",
    "            'total_references_collected': sum(stats['reference_counts']),\n",
    "            'average_references_per_paper': sum(stats['reference_counts']) / len(stats['reference_counts']) if stats['reference_counts'] else 0,\n",
    "            'min_references_per_paper': min(stats['reference_counts']) if stats['reference_counts'] else 0,\n",
    "            'max_references_per_paper': max(stats['reference_counts']) if stats['reference_counts'] else 0,\n",
    "        },\n",
    "        'tools_used': [\n",
    "            'arxiv.py - For paper download via arXiv API',\n",
    "            'Semantic Scholar API - For reference extraction',\n",
    "            'psutil - For memory and disk usage tracking'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save enhanced report to file\n",
    "    report_file = base_dir / f\"{Config.STUDENT_ID}_detailed_report.json\"\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(enhanced_stats, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    logging.info(f\"Enhanced report saved to {report_file}\")\n",
    "    \n",
    "    return enhanced_stats\n",
    "\n",
    "# Run with enhanced metrics (uncomment to use)\n",
    "enhanced_report = scrape_with_enhanced_metrics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
